{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Production Ingestion Pipeline - Abu Dhabi Government Knowledge Base\n",
    "\n",
    "## Overview\n",
    "This notebook serves as the **source of truth** for all production ingestion and tuning operations for the Abu Dhabi Government Knowledge Base (WoG) system.\n",
    "\n",
    "### Key Features\n",
    "- **ðŸ—„ï¸ Production Database**: Creates and manages `WoG_Prod` database\n",
    "- **ðŸ“Š Contextual RAG**: Advanced section-to-chunk mapping with 105 sections per document\n",
    "- **ðŸŒ Remote Docling**: GPU-accelerated document processing with rich section extraction\n",
    "- **ðŸ” Language Filtering**: Processes only 'en' and 'ar+en' documents\n",
    "- **ðŸ“‹ Hierarchy Management**: Handles null values in government_entity, functional_domain, document_category\n",
    "- **âš¡ Performance Monitoring**: Step-by-step metrics and optimization\n",
    "\n",
    "### Architecture\n",
    "- **Database**: PostgreSQL + PGVector with 3072D embeddings\n",
    "- **Processing**: Remote Docling server at `http://74.162.37.71:5001/v1alpha/convert/file`\n",
    "- **Chunking**: 250-token chunks with 30-token overlap using tiktoken\n",
    "- **Embedding**: text-embedding-3-large with token-aware generation\n",
    "- **Search**: Cosine similarity with metadata filtering\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup and Configuration\n",
    "\n",
    "### Initialize Environment Variables and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Production Ingestion Pipeline Initialized\n",
      "ðŸ“Š Database: WoG_Prod\n",
      "ðŸ“ Source Folder: knowledge_base_prod\n",
      "ðŸ”¤ Allowed Languages: ['en', 'ar+en']\n",
      "ðŸŽ¯ Embedding Model: text-embedding-3-large (3072D)\n",
      "ðŸŒ Remote Docling: http://74.162.37.71:5001/v1alpha/convert/file\n",
      "â° Started: 2025-07-17 00:54:14\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import psycopg2\n",
    "from datetime import datetime\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Add src directory to path\n",
    "sys.path.append(str(Path.cwd() / 'src'))\n",
    "\n",
    "# Configuration\n",
    "PRODUCTION_DATABASE_NAME = \"WoG_Prod\"\n",
    "PRODUCTION_KB_FOLDER = \"knowledge_base_prod\"\n",
    "EMBEDDING_MODEL = \"text-embedding-3-large\"\n",
    "VECTOR_DIMENSION = 3072\n",
    "ALLOWED_LANGUAGES = ['en', 'ar+en']\n",
    "\n",
    "print(\"ðŸ”§ Production Ingestion Pipeline Initialized\")\n",
    "print(f\"ðŸ“Š Database: {PRODUCTION_DATABASE_NAME}\")\n",
    "print(f\"ðŸ“ Source Folder: {PRODUCTION_KB_FOLDER}\")\n",
    "print(f\"ðŸ”¤ Allowed Languages: {ALLOWED_LANGUAGES}\")\n",
    "print(f\"ðŸŽ¯ Embedding Model: {EMBEDDING_MODEL} ({VECTOR_DIMENSION}D)\")\n",
    "print(f\"ðŸŒ Remote Docling: {os.getenv('DOCLING_SERVER_URL')}\")\n",
    "print(f\"â° Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create Production Database\n",
    "\n",
    "### Create WoG_Prod Database with PGVector Extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Failed to create production database: database \"wog_prod\" already exists\n",
      "\n",
      "ðŸ“Š Database Status: âŒ Failed\n"
     ]
    }
   ],
   "source": [
    "def create_production_database():\n",
    "    \"\"\"Create WoG_Prod database with PGVector extension\"\"\"\n",
    "    try:\n",
    "        # Connect to default postgres database to create new database\n",
    "        conn = psycopg2.connect(\n",
    "            host=\"localhost\",\n",
    "            database=\"postgres\",\n",
    "            user=os.getenv('DB_USER', 'mustaqmollah'),\n",
    "            password=os.getenv('DB_PASSWORD', '')\n",
    "        )\n",
    "        conn.autocommit = True\n",
    "        \n",
    "        with conn.cursor() as cur:\n",
    "            # Check if database exists\n",
    "            cur.execute(f\"SELECT 1 FROM pg_catalog.pg_database WHERE datname = '{PRODUCTION_DATABASE_NAME}'\")\n",
    "            exists = cur.fetchone()\n",
    "            \n",
    "            if exists:\n",
    "                print(f\"âš ï¸  Database '{PRODUCTION_DATABASE_NAME}' already exists\")\n",
    "                response = input(\"Do you want to drop and recreate it? (y/N): \")\n",
    "                if response.lower() == 'y':\n",
    "                    cur.execute(f\"DROP DATABASE IF EXISTS {PRODUCTION_DATABASE_NAME}\")\n",
    "                    print(f\"ðŸ—‘ï¸  Dropped existing database '{PRODUCTION_DATABASE_NAME}'\")\n",
    "                else:\n",
    "                    print(f\"âœ… Using existing database '{PRODUCTION_DATABASE_NAME}'\")\n",
    "                    conn.close()\n",
    "                    return True\n",
    "            \n",
    "            # Create new database\n",
    "            cur.execute(f\"CREATE DATABASE {PRODUCTION_DATABASE_NAME}\")\n",
    "            print(f\"ðŸŽ‰ Created database '{PRODUCTION_DATABASE_NAME}'\")\n",
    "        \n",
    "        conn.close()\n",
    "        \n",
    "        # Connect to new database and enable PGVector\n",
    "        prod_conn = psycopg2.connect(\n",
    "            host=\"localhost\",\n",
    "            database=PRODUCTION_DATABASE_NAME,\n",
    "            user=os.getenv('DB_USER', 'mustaqmollah'),\n",
    "            password=os.getenv('DB_PASSWORD', '')\n",
    "        )\n",
    "        prod_conn.autocommit = True\n",
    "        \n",
    "        with prod_conn.cursor() as cur:\n",
    "            # Enable PGVector extension\n",
    "            cur.execute(\"CREATE EXTENSION IF NOT EXISTS vector\")\n",
    "            print(\"âœ… PGVector extension enabled\")\n",
    "            \n",
    "            # Verify vector support\n",
    "            cur.execute(\"SELECT 1\")\n",
    "            print(f\"âœ… Database connection verified: {PRODUCTION_DATABASE_NAME}\")\n",
    "        \n",
    "        prod_conn.close()\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed to create production database: {e}\")\n",
    "        return False\n",
    "\n",
    "# Create production database\n",
    "db_success = create_production_database()\n",
    "print(f\"ðŸ“Š Database Status: {'âœ… Success' if db_success else 'âŒ Failed'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create Database Schema\n",
    "\n",
    "### Create Knowledge Base Tables with Hierarchy Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Knowledge Base PostgreSQL connection pool initialized (max_size=10)\n",
      "âœ… PGVector extension enabled for Knowledge Base\n",
      "ðŸ”„ Creating Knowledge Base hierarchy tables...\n",
      "ðŸ”§ Granting PUBLIC access to all WoG tables...\n",
      "ðŸŒ± Seeding Knowledge Base with Excel hierarchy data...\n",
      "âœ… Knowledge Base database initialized with hierarchy data\n",
      "âœ… Knowledge Base PostgreSQL connection pool initialized (max_size=10)\n",
      "âœ… PGVector extension enabled for Knowledge Base\n",
      "ðŸ”„ Creating Knowledge Base hierarchy tables...\n",
      "ðŸ”§ Granting PUBLIC access to all WoG tables...\n",
      "ðŸŒ± Seeding Knowledge Base with Excel hierarchy data...\n",
      "âœ… Knowledge Base database initialized with hierarchy data\n",
      "ðŸ—ï¸  Creating database schema...\n",
      "âœ… Created table: entities\n",
      "âœ… Created table: functional_domains\n",
      "âœ… Created table: document_categories\n",
      "âœ… Created table: knowledge_base\n",
      "ðŸ” Creating indexes...\n",
      "âœ… Created indexes\n",
      "ðŸ” Granting permissions...\n",
      "âœ… Permissions granted\n",
      "ðŸ—ï¸  Schema Status: âœ… Success\n"
     ]
    }
   ],
   "source": [
    "# Update environment to use production database\n",
    "os.environ['KB_DATABASE_URL'] = f\"postgresql://mustaqmollah@localhost:5432/{PRODUCTION_DATABASE_NAME}\"\n",
    "\n",
    "# Import database models and manager\n",
    "from models import (\n",
    "    get_create_entities_table_sql,\n",
    "    get_create_domains_table_sql,\n",
    "    get_create_categories_table_sql,\n",
    "    get_create_knowledge_base_table_sql,\n",
    "    get_create_kb_indexes_sql\n",
    ")\n",
    "from database import KnowledgeBaseManager\n",
    "\n",
    "def create_database_schema():\n",
    "    \"\"\"Create all database tables and indexes\"\"\"\n",
    "    try:\n",
    "        # Initialize database manager\n",
    "        kb_manager = KnowledgeBaseManager()\n",
    "        \n",
    "        print(\"ðŸ—ï¸  Creating database schema...\")\n",
    "        \n",
    "        # Create tables\n",
    "        tables = [\n",
    "            (\"entities\", get_create_entities_table_sql()),\n",
    "            (\"functional_domains\", get_create_domains_table_sql()),\n",
    "            (\"document_categories\", get_create_categories_table_sql()),\n",
    "            (\"knowledge_base\", get_create_knowledge_base_table_sql(EMBEDDING_MODEL))\n",
    "        ]\n",
    "        \n",
    "        for table_name, sql in tables:\n",
    "            try:\n",
    "                with kb_manager.get_connection() as conn:\n",
    "                    with conn.cursor() as cur:\n",
    "                        cur.execute(sql)\n",
    "                        conn.commit()\n",
    "                print(f\"âœ… Created table: {table_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Failed to create table {table_name}: {e}\")\n",
    "                return False\n",
    "        \n",
    "        # Create indexes\n",
    "        print(\"ðŸ” Creating indexes...\")\n",
    "        try:\n",
    "            with kb_manager.get_connection() as conn:\n",
    "                with conn.cursor() as cur:\n",
    "                    cur.execute(get_create_kb_indexes_sql(EMBEDDING_MODEL))\n",
    "                    conn.commit()\n",
    "            print(\"âœ… Created indexes\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Failed to create indexes: {e}\")\n",
    "            return False\n",
    "        \n",
    "        # Grant permissions\n",
    "        print(\"ðŸ” Granting permissions...\")\n",
    "        try:\n",
    "            with kb_manager.get_connection() as conn:\n",
    "                with conn.cursor() as cur:\n",
    "                    tables = ['entities', 'functional_domains', 'document_categories', 'knowledge_base']\n",
    "                    for table in tables:\n",
    "                        cur.execute(f'GRANT ALL PRIVILEGES ON TABLE {table} TO PUBLIC')\n",
    "                    conn.commit()\n",
    "            print(\"âœ… Permissions granted\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Failed to grant permissions: {e}\")\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed to create database schema: {e}\")\n",
    "        return False\n",
    "\n",
    "# Create schema\n",
    "schema_success = create_database_schema()\n",
    "print(f\"ðŸ—ï¸  Schema Status: {'âœ… Success' if schema_success else 'âŒ Failed'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Generate Production YAML Mapping\n",
    "\n",
    "### Process Excel Hierarchy Data with Language Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‹ Generating production YAML mapping...\n",
      "ðŸ”§ KB Mapping Generator initialized\n",
      "ðŸ“ Knowledge Base dir: /Users/mustaqmollah/Desktop/GovGPT/ContextualRag/govgpt-kb/knowledge_base_prod\n",
      "ðŸ“ Version: v1.2\n",
      "ðŸ“Š Using folder: knowledge_base_prod\n",
      "ðŸ“ Production folder: /Users/mustaqmollah/Desktop/GovGPT/ContextualRag/govgpt-kb/knowledge_base_prod\n",
      "ðŸ“Š Excel file: knowledge_base_hierarchy_data.xlsx\n",
      "ðŸ“ Version: v1.2\n",
      "ðŸ”„ Generating YAML mapping v1.2...\n",
      "ðŸ“– Reading Excel file: knowledge_base_hierarchy_data.xlsx\n",
      "âœ… Read 24 rows from Excel file\n",
      "â­ï¸  Skipping document 'CX Effortless_Guide_AR' - language 'ar' not in allowed list ['en', 'ar+en']\n",
      "â­ï¸  Skipping document 'Abu Dhabi Government Finance Policy Manual_AR' - language 'ar' not in allowed list ['en', 'ar+en']\n",
      "â­ï¸  Skipping document 'HR - Government Employee Guide Version 1 - AR' - language 'ar' not in allowed list ['en', 'ar+en']\n",
      "â­ï¸  Skipping document 'HR - Government Employee Guide Version 2 - AR' - language 'ar' not in allowed list ['en', 'ar+en']\n",
      "â­ï¸  Skipping document 'HR - HR Law- AR' - language 'ar' not in allowed list ['en', 'ar+en']\n",
      "â­ï¸  Skipping document 'HR - Implementation Regulation for HR Law No 6 Year 2016 - AR' - language 'ar' not in allowed list ['en', 'ar+en']\n",
      "â­ï¸  Skipping document 'Proc - Policies and Procedures for Sales Auctions and Warehouses - AR' - language 'ar' not in allowed list ['en', 'ar+en']\n",
      "â­ï¸  Skipping document 'Proc - Procurement Charter - AR' - language 'ar' not in allowed list ['en', 'ar+en']\n",
      "â­ï¸  Skipping document 'Proc - Procurement Manual (Business Process) - AR' - language 'ar' not in allowed list ['en', 'ar+en']\n",
      "â­ï¸  Skipping document 'Proc - Procurement Standard Regulations - AR' - language 'ar' not in allowed list ['en', 'ar+en']\n",
      "ðŸ“‹ Processed 14 documents (10 skipped due to language filtering)\n",
      "ðŸ” Scanning knowledge base directory: /Users/mustaqmollah/Desktop/GovGPT/ContextualRag/govgpt-kb/knowledge_base_prod\n",
      "ðŸ“ Found 24 files in knowledge base\n",
      "âœ… Matched: 'CX Abu Dhabi Government Tone of Voice Document' â†’ CX Abu Dhabi Government Tone of Voice Document.pdf\n",
      "âœ… Matched: 'CX Effortless_Guide_EN_V2' â†’ CX Effortless_Guide_EN_V2.pdf\n",
      "âœ… Matched: 'CX Glossary' â†’ CX Glossary.pdf\n",
      "âœ… Matched: 'CX TOV templates' â†’ CX TOV templates.pdf\n",
      "âœ… Matched: 'Abu Dhabi Government Finance Policy Manual_EN' â†’ Abu Dhabi Government Finance Policy Manual_EN.pdf\n",
      "âœ… Matched: 'HR - HR Law - EN' â†’ HR - HR Law - EN.pdf\n",
      "âœ… Matched: 'HR - Implementation Regulation for HR Law No 6 Year 2016 - EN' â†’ HR - Implementation Regulation for HR Law No 6 Year 2016 - EN.pdf\n",
      "âœ… Matched: 'Proc - DGE Procurement Framework - EN' â†’ Proc - DGE Procurement Framework - EN.DOC\n",
      "âœ… Matched: 'Proc - Policies and Procedures for Sales Auctions and Warehouses - EN' â†’ Proc - Policies and Procedures for Sales Auctions and Warehouses - EN.PDF\n",
      "âœ… Matched: 'Proc - Procurement Charter - EN' â†’ Proc - Procurement Charter - EN.pdf\n",
      "âœ… Matched: 'Proc - Procurement Manual (Ariba Aligned) - EN' â†’ Proc - Procurement Manual (Ariba Aligned) - EN.pdf\n",
      "âœ… Matched: 'Proc - Procurement Manual (Business Process) - EN' â†’ Proc - Procurement Manual (Business Process) - EN.pdf\n",
      "âœ… Matched: 'Proc - Procurement Standard Regulations - EN' â†’ Proc - Procurement Standard Regulations - EN.PDF\n",
      "âœ… Matched: 'Proc - Support Frequently Asked Questions - EN' â†’ Proc - Support Frequently Asked Questions - EN.docx\n",
      "ðŸ“‹ Unmatched files (10):\n",
      "   - Proc - Procurement Standard Regulations - AR.pdf\n",
      "   - Proc - Procurement Manual (Business Process) - AR.pdf\n",
      "   - Proc - Policies and Procedures for Sales Auctions and Warehouses - AR.pdf\n",
      "   - Proc - Procurement Charter - AR.pdf\n",
      "   - HR - Implementation Regulation for HR Law No 6 Year 2016 - AR.pdf\n",
      "   - HR - Government Employee Guide Version 2 - AR.pdf\n",
      "   - HR - Government Employee Guide Version 1 - AR.pdf\n",
      "   - HR - HR Law- AR.pdf\n",
      "   - Abu Dhabi Government Finance Policy Manual_AR.docx\n",
      "   - CX Effortless_Guide_AR.pdf\n",
      "\n",
      "ðŸ”— Mapped 14 available files\n",
      "â“ 0 documents marked as missing\n",
      "âœ… Generated YAML mapping: /Users/mustaqmollah/Desktop/GovGPT/ContextualRag/govgpt-kb/config/mapping_v1.2.yaml\n",
      "ðŸ“Š Summary:\n",
      "   - Total documents: 14\n",
      "   - Available: 14\n",
      "   - Missing: 0\n",
      "   - Entities: 4\n",
      "   - Domains: 4\n",
      "   - Categories: 9\n",
      "âœ… Production YAML mapping generated successfully\n",
      "ðŸ“Š Mapping Statistics:\n",
      "   - Total documents: 14\n",
      "   - Available documents: 14\n",
      "   - Missing documents: 0\n",
      "   - Entities: 4\n",
      "   - Domains: 4\n",
      "   - Categories: 9\n",
      "   - Languages: {'en': 11, 'ar+en': 3}\n",
      "\n",
      "ðŸ“„ Available Documents (14):\n",
      "   - CX Abu Dhabi Government Tone of Voice Document (en)\n",
      "   - CX Effortless_Guide_EN_V2 (en)\n",
      "   - CX Glossary (ar+en)\n",
      "   - CX TOV templates (ar+en)\n",
      "   - Abu Dhabi Government Finance Policy Manual_EN (en)\n",
      "   - HR - HR Law - EN (en)\n",
      "   - HR - Implementation Regulation for HR Law No 6 Year 2016 - EN (en)\n",
      "   - Proc - DGE Procurement Framework - EN (en)\n",
      "   - Proc - Policies and Procedures for Sales Auctions and Warehouses - EN (en)\n",
      "   - Proc - Procurement Charter - EN (en)\n",
      "   ... and 4 more\n",
      "\n",
      "â“ Missing Documents (0):\n",
      "ðŸ“‹ Mapping Status: âœ… Success\n"
     ]
    }
   ],
   "source": [
    "# Import mapping generator\n",
    "sys.path.append(str(Path.cwd() / 'scripts'))\n",
    "from generate_mapping import KBMappingGenerator\n",
    "\n",
    "def generate_production_mapping():\n",
    "    \"\"\"Generate YAML mapping for production data\"\"\"\n",
    "    try:\n",
    "        print(\"ðŸ“‹ Generating production YAML mapping...\")\n",
    "        \n",
    "        # Initialize mapping generator with production folder\n",
    "        excel_file = \"knowledge_base_hierarchy_data.xlsx\"\n",
    "        version = \"1.2\"\n",
    "        \n",
    "        generator = KBMappingGenerator(\n",
    "            excel_file=excel_file, \n",
    "            version=version,\n",
    "            knowledge_base_folder=PRODUCTION_KB_FOLDER\n",
    "        )\n",
    "        \n",
    "        print(f\"ðŸ“ Production folder: {generator.knowledge_base_dir}\")\n",
    "        print(f\"ðŸ“Š Excel file: {excel_file}\")\n",
    "        print(f\"ðŸ“ Version: v{version}\")\n",
    "        \n",
    "        # Check if production folder exists\n",
    "        if not generator.knowledge_base_dir.exists():\n",
    "            print(f\"âŒ Production folder does not exist: {generator.knowledge_base_dir}\")\n",
    "            print(\"Please create the knowledge_base_prod folder and add your production documents\")\n",
    "            return False\n",
    "        \n",
    "        # Generate mapping\n",
    "        success = generator.generate_yaml_mapping()\n",
    "        \n",
    "        if success:\n",
    "            print(\"âœ… Production YAML mapping generated successfully\")\n",
    "            \n",
    "            # Show mapping statistics\n",
    "            mapping_file = generator.config_dir / f\"mapping_v{version}.yaml\"\n",
    "            if mapping_file.exists():\n",
    "                import yaml\n",
    "                with open(mapping_file, 'r') as f:\n",
    "                    mapping_data = yaml.safe_load(f)\n",
    "                \n",
    "                print(f\"ðŸ“Š Mapping Statistics:\")\n",
    "                print(f\"   - Total documents: {mapping_data.get('total_documents', 0)}\")\n",
    "                print(f\"   - Available documents: {mapping_data.get('available_documents', 0)}\")\n",
    "                print(f\"   - Missing documents: {mapping_data.get('missing_documents', 0)}\")\n",
    "                print(f\"   - Entities: {len(mapping_data.get('entities', []))}\")\n",
    "                print(f\"   - Domains: {len(mapping_data.get('functional_domains', []))}\")\n",
    "                print(f\"   - Categories: {len(mapping_data.get('document_categories', []))}\")\n",
    "                \n",
    "                # Show language distribution\n",
    "                languages = {}\n",
    "                for doc in mapping_data.get('document_mappings', []):\n",
    "                    lang = doc.get('language', 'unknown')\n",
    "                    languages[lang] = languages.get(lang, 0) + 1\n",
    "                \n",
    "                print(f\"   - Languages: {languages}\")\n",
    "                \n",
    "                # Show available vs missing breakdown\n",
    "                available_docs = [d for d in mapping_data.get('document_mappings', []) if d.get('status') == 'available']\n",
    "                missing_docs = [d for d in mapping_data.get('document_mappings', []) if d.get('status') == 'missing']\n",
    "                \n",
    "                print(f\"\\nðŸ“„ Available Documents ({len(available_docs)}):\")\n",
    "                for doc in available_docs[:10]:  # Show first 10\n",
    "                    print(f\"   - {doc['excel_title']} ({doc['language']})\")\n",
    "                if len(available_docs) > 10:\n",
    "                    print(f\"   ... and {len(available_docs) - 10} more\")\n",
    "                \n",
    "                print(f\"\\nâ“ Missing Documents ({len(missing_docs)}):\")\n",
    "                for doc in missing_docs[:5]:  # Show first 5\n",
    "                    print(f\"   - {doc['excel_title']} ({doc['language']})\")\n",
    "                if len(missing_docs) > 5:\n",
    "                    print(f\"   ... and {len(missing_docs) - 5} more\")\n",
    "            \n",
    "            return True\n",
    "        else:\n",
    "            print(\"âŒ Failed to generate production YAML mapping\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error generating production mapping: {e}\")\n",
    "        return False\n",
    "\n",
    "# Generate production mapping\n",
    "mapping_success = generate_production_mapping()\n",
    "print(f\"ðŸ“‹ Mapping Status: {'âœ… Success' if mapping_success else 'âŒ Failed'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Populate Hierarchy Tables\n",
    "\n",
    "### Load Entities, Domains, and Categories from YAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Knowledge Base PostgreSQL connection pool initialized (max_size=10)\n",
      "âœ… PGVector extension enabled for Knowledge Base\n",
      "ðŸ”„ Creating Knowledge Base hierarchy tables...\n",
      "ðŸ”§ Granting PUBLIC access to all WoG tables...\n",
      "ðŸŒ± Seeding Knowledge Base with Excel hierarchy data...\n",
      "âœ… Knowledge Base database initialized with hierarchy data\n",
      "ðŸ“Š Populating hierarchy tables...\n",
      "ðŸ”§ DocumentProcessor initialized\n",
      "ðŸ“ Cache directory: cache\n",
      "ðŸš€ Processing method: Docling\n",
      "ðŸŒ Docling server URL: 'http://74.162.37.71:5001/v1alpha/convert/file'\n",
      "ðŸ’¾ Cache enabled: True\n",
      "ðŸ—‚ï¸  Use cache: True\n",
      "ðŸ”§ DocumentProcessor initialized\n",
      "ðŸ“ Cache directory: cache\n",
      "ðŸš€ Processing method: Docling\n",
      "ðŸŒ Docling server URL: 'http://74.162.37.71:5001/v1alpha/convert/file'\n",
      "ðŸ’¾ Cache enabled: True\n",
      "ðŸ—‚ï¸  Use cache: True\n",
      "ðŸ§© ChunkProcessor initialized\n",
      "ðŸ§  Embedding model: text-embedding-3-large\n",
      "ðŸ“ Chunk size: 250, overlap: 30\n",
      "âš¡ Max workers: 8\n",
      "ðŸ’¾ Cache directory: ./cache\n",
      "ðŸ“Š ProcessingLogger initialized for version 1.2\n",
      "ðŸ“ Log file: logs/processing_v1.2.log\n",
      "ðŸ”§ Knowledge Base populator initialized\n",
      "ðŸ“ Mapping file: /Users/mustaqmollah/Desktop/GovGPT/ContextualRag/govgpt-kb/config/mapping_v1.2.yaml\n",
      "ðŸ“ Version: v1.2\n",
      "ðŸ§© Using ChunkProcessor for chunking and embeddings\n",
      "ðŸ”„ Populating hierarchy tables...\n",
      "âœ… Entity: Unknown\n",
      "âš ï¸  Entity exists: DGE\n",
      "âš ï¸  Entity exists: Department of Finance\n",
      "âš ï¸  Entity exists: Human Resource Authority\n",
      "âš ï¸  Domain exists: CX\n",
      "âš ï¸  Domain exists: Finance\n",
      "âš ï¸  Domain exists: HR\n",
      "âš ï¸  Domain exists: Procurement\n",
      "âš ï¸  Category exists: Guide\n",
      "âš ï¸  Category exists: Glossary\n",
      "âš ï¸  Category exists: Templates\n",
      "âš ï¸  Category exists: Policy\n",
      "âš ï¸  Category exists: Law\n",
      "âš ï¸  Category exists: Framework\n",
      "âš ï¸  Category exists: Charter\n",
      "âš ï¸  Category exists: Manual\n",
      "âš ï¸  Category exists: FAQ\n",
      "âœ… Hierarchy tables populated successfully\n",
      "âœ… Hierarchy tables populated successfully\n",
      "ðŸ“Š Database Contents:\n",
      "   - Entities: 5\n",
      "   - Domains: 5\n",
      "   - Categories: 9\n",
      "\n",
      "ðŸ›ï¸  Sample Entities:\n",
      "   - General (GEN) - General Government\n",
      "   - DGE (DGE) - Department of Government Enablement\n",
      "   - Department of Finance (DOF) - Department of Finance\n",
      "   - Human Resource Authority (HRA) - Human Resource Authority\n",
      "   - Unknown (U) - Unknown\n",
      "\n",
      "ðŸ”§ Sample Domains:\n",
      "   - CX - Customer Experience\n",
      "   - Finance - Finance\n",
      "   - HR - Human Resources\n",
      "   - Procurement - Procurement\n",
      "   - IT - Information Technology\n",
      "\n",
      "ðŸ“‹ Sample Categories:\n",
      "   - Guide - Instructional guides and manuals\n",
      "   - Glossary - Glossaries and definitions\n",
      "   - Templates - Document templates and forms\n",
      "   - Policy - Government policies and guidelines\n",
      "   - Law - Legal documents and regulations\n",
      "ðŸ“Š Hierarchy Status: âœ… Success\n"
     ]
    }
   ],
   "source": [
    "# Import database populator\n",
    "from populate_database import KBDatabasePopulator\n",
    "\n",
    "def populate_hierarchy_tables():\n",
    "    \"\"\"Populate hierarchy tables from YAML mapping\"\"\"\n",
    "    try:\n",
    "        print(\"ðŸ“Š Populating hierarchy tables...\")\n",
    "        \n",
    "        # Initialize database populator\n",
    "        populator = KBDatabasePopulator(version=\"1.2\")\n",
    "        \n",
    "        # Load production mapping\n",
    "        mapping_file = Path.cwd() / \"config\" / \"mapping_v1.2.yaml\"\n",
    "        if not mapping_file.exists():\n",
    "            print(f\"âŒ Production mapping file not found: {mapping_file}\")\n",
    "            return False\n",
    "        \n",
    "        import yaml\n",
    "        with open(mapping_file, 'r') as f:\n",
    "            mapping_data = yaml.safe_load(f)\n",
    "        \n",
    "        # Populate hierarchy tables\n",
    "        success = populator.populate_hierarchy_tables(mapping_data)\n",
    "        \n",
    "        if success:\n",
    "            print(\"âœ… Hierarchy tables populated successfully\")\n",
    "            \n",
    "            # Verify table contents\n",
    "            from database import get_kb_manager\n",
    "            kb_manager = get_kb_manager()\n",
    "            \n",
    "            try:\n",
    "                with kb_manager.get_connection() as conn:\n",
    "                    with conn.cursor() as cur:\n",
    "                        # Count entities\n",
    "                        cur.execute(\"SELECT COUNT(*) FROM entities\")\n",
    "                        entity_count = cur.fetchone()[0]\n",
    "                        \n",
    "                        # Count domains\n",
    "                        cur.execute(\"SELECT COUNT(*) FROM functional_domains\")\n",
    "                        domain_count = cur.fetchone()[0]\n",
    "                        \n",
    "                        # Count categories\n",
    "                        cur.execute(\"SELECT COUNT(*) FROM document_categories\")\n",
    "                        category_count = cur.fetchone()[0]\n",
    "                        \n",
    "                        print(f\"ðŸ“Š Database Contents:\")\n",
    "                        print(f\"   - Entities: {entity_count}\")\n",
    "                        print(f\"   - Domains: {domain_count}\")\n",
    "                        print(f\"   - Categories: {category_count}\")\n",
    "                        \n",
    "                        # Show sample entities\n",
    "                        cur.execute(\"SELECT name, display_name, code FROM entities LIMIT 5\")\n",
    "                        entities = cur.fetchall()\n",
    "                        print(f\"\\nðŸ›ï¸  Sample Entities:\")\n",
    "                        for entity in entities:\n",
    "                            print(f\"   - {entity[0]} ({entity[2]}) - {entity[1]}\")\n",
    "                        \n",
    "                        # Show sample domains\n",
    "                        cur.execute(\"SELECT name, display_name FROM functional_domains LIMIT 5\")\n",
    "                        domains = cur.fetchall()\n",
    "                        print(f\"\\nðŸ”§ Sample Domains:\")\n",
    "                        for domain in domains:\n",
    "                            print(f\"   - {domain[0]} - {domain[1]}\")\n",
    "                        \n",
    "                        # Show sample categories\n",
    "                        cur.execute(\"SELECT name, description FROM document_categories LIMIT 5\")\n",
    "                        categories = cur.fetchall()\n",
    "                        print(f\"\\nðŸ“‹ Sample Categories:\")\n",
    "                        for category in categories:\n",
    "                            print(f\"   - {category[0]} - {category[1]}\")\n",
    "                            \n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Error verifying hierarchy tables: {e}\")\n",
    "                return False\n",
    "            \n",
    "            return True\n",
    "        else:\n",
    "            print(\"âŒ Failed to populate hierarchy tables\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error populating hierarchy tables: {e}\")\n",
    "        return False\n",
    "\n",
    "# Populate hierarchy tables\n",
    "hierarchy_success = populate_hierarchy_tables()\n",
    "print(f\"ðŸ“Š Hierarchy Status: {'âœ… Success' if hierarchy_success else 'âŒ Failed'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Clear Cache for Fresh Processing\n",
    "\n",
    "### Clear Document Processing Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ—‘ï¸  Clearing 0 cached files...\n",
      "âœ… Cache cleared: 0 files removed\n",
      "ðŸ—‘ï¸  Cache Status: âœ… Cleared\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "def clear_processing_cache():\n",
    "    \"\"\"Clear document processing cache for fresh processing\"\"\"\n",
    "    try:\n",
    "        cache_dir = Path.cwd() / \"cache\"\n",
    "        \n",
    "        if cache_dir.exists():\n",
    "            # Count files before clearing\n",
    "            cache_files = list(cache_dir.glob(\"*.json\"))\n",
    "            file_count = len(cache_files)\n",
    "            \n",
    "            print(f\"ðŸ—‘ï¸  Clearing {file_count} cached files...\")\n",
    "            \n",
    "            # Remove all cache files\n",
    "            for cache_file in cache_files:\n",
    "                cache_file.unlink()\n",
    "                \n",
    "            print(f\"âœ… Cache cleared: {file_count} files removed\")\n",
    "        else:\n",
    "            print(\"ðŸ“ Cache directory does not exist - creating it\")\n",
    "            cache_dir.mkdir(exist_ok=True)\n",
    "            \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error clearing cache: {e}\")\n",
    "        return False\n",
    "\n",
    "# Clear cache\n",
    "cache_success = clear_processing_cache()\n",
    "print(f\"ðŸ—‘ï¸  Cache Status: {'âœ… Cleared' if cache_success else 'âŒ Failed'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Process Production Documents\n",
    "\n",
    "### Process Documents with Remote Docling and Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Processing production documents...\n",
      "ðŸ”§ DocumentProcessor initialized\n",
      "ðŸ“ Cache directory: cache\n",
      "ðŸš€ Processing method: Docling\n",
      "ðŸŒ Docling server URL: 'http://74.162.37.71:5001/v1alpha/convert/file'\n",
      "ðŸ’¾ Cache enabled: True\n",
      "ðŸ—‚ï¸  Use cache: True\n",
      "ðŸ”§ DocumentProcessor initialized\n",
      "ðŸ“ Cache directory: cache\n",
      "ðŸš€ Processing method: Docling\n",
      "ðŸŒ Docling server URL: 'http://74.162.37.71:5001/v1alpha/convert/file'\n",
      "ðŸ’¾ Cache enabled: True\n",
      "ðŸ—‚ï¸  Use cache: True\n",
      "ðŸ§© ChunkProcessor initialized\n",
      "ðŸ§  Embedding model: text-embedding-3-large\n",
      "ðŸ“ Chunk size: 250, overlap: 30\n",
      "âš¡ Max workers: 8\n",
      "ðŸ’¾ Cache directory: ./cache\n",
      "ðŸ“Š ProcessingLogger initialized for version 1.2\n",
      "ðŸ“ Log file: logs/processing_v1.2.log\n",
      "ðŸ”§ Knowledge Base populator initialized\n",
      "ðŸ“ Mapping file: /Users/mustaqmollah/Desktop/GovGPT/ContextualRag/govgpt-kb/config/mapping_v1.2.yaml\n",
      "ðŸ“ Version: v1.2\n",
      "ðŸ§© Using ChunkProcessor for chunking and embeddings\n",
      "ðŸ“„ Found 14 available documents to process\n",
      "\n",
      "ðŸ“„ Processing document 1/14: CX Abu Dhabi Government Tone of Voice Document\n",
      "   Language: en\n",
      "   Entity: Unknown\n",
      "   Domain: CX\n",
      "   Category: Guide\n",
      "\n",
      "ðŸ”„ Processing: CX Abu Dhabi Government Tone of Voice Document\n",
      "ðŸ“ File: knowledge_base_prod/CX/CX Abu Dhabi Government Tone of Voice Document.pdf\n",
      "ðŸ“‹ Mapped to: Unknown > CX > Guide\n",
      "ðŸ”„ Processing: CX Abu Dhabi Government Tone of Voice Document.pdf\n",
      "ðŸŒ Using remote Docling server: http://74.162.37.71:5001/v1alpha/convert/file\n",
      "ðŸ” Processing with Docling: CX Abu Dhabi Government Tone of Voice Document.pdf\n",
      "ðŸ“‹ Available fields in document: ['filename', 'md_content', 'json_content', 'html_content', 'text_content', 'doctags_content']\n",
      "âœ… json_content has content: 13 chars\n",
      "âŒ text_content is None or empty\n",
      "âŒ md_content is None or empty\n",
      "âŒ html_content is None or empty\n",
      "ðŸ” json_content keys: ['schema_name', 'version', 'name', 'origin', 'furniture', 'body', 'groups', 'texts', 'pictures', 'tables', 'key_value_items', 'form_items', 'pages']\n",
      "âœ… Extracted text from json_content.texts: 38826 chars\n",
      "âœ… Successfully deserialized DoclingDocument from remote server\n",
      "ðŸ“‹ Extracted 121 sections from DoclingDocument\n",
      "ðŸ’¾ Cached: CX Abu Dhabi Government Tone of Voice Document.pdf\n",
      "ðŸ§© Processing document to chunks: Abu Dhabi Government Tone of Voice\n",
      " â€¢ Generated 36 chunks using token-based chunking\n",
      " â€¢ Mapped 36 chunks to 121 sections\n",
      "ðŸš€ Enriching 36 chunks with 8 workers...\n",
      "ðŸ’¾ Cached 36 enriched chunks for CX Abu Dhabi Government Tone of Voice Document\n",
      " â€¢ Enhanced 36 chunks with contextualization\n",
      "ðŸ”„ Generating embeddings for 36 chunks...\n",
      "ðŸ” Processing batch 1/2 (32 texts)\n",
      "âš ï¸  Batch exceeds token limit (9700 > 7992), reducing batch size\n",
      "âœ… Successfully processed 32 texts with batch_size=16\n",
      "ðŸ“Š Generated embeddings for batch 1/2\n",
      "ðŸ” Processing batch 2/2 (4 texts)\n",
      "ðŸ“Š Generated embeddings for batch 2/2\n",
      "âœ… Generated 36 embeddings with token-aware processing\n",
      " â€¢ Generated 36 embeddings\n",
      " â€¢ Created 36 KBChunkEmbedding objects\n",
      "âœ… Stored 36 Knowledge Base embeddings\n",
      "âœ… Successfully processed CX Abu Dhabi Government Tone of Voice Document\n",
      "ðŸ“Š Stored 36 chunks in Knowledge Base\n",
      "ðŸ“‹ Cached: CX Abu Dhabi Government Tone of Voice Document.pdf\n",
      "   âœ… Success (48.51s)\n",
      "   Progress: 1/14 (7.1%) - ETA: 10.5min\n",
      "\n",
      "ðŸ“„ Processing document 2/14: CX Effortless_Guide_EN_V2\n",
      "   Language: en\n",
      "   Entity: DGE\n",
      "   Domain: CX\n",
      "   Category: Guide\n",
      "\n",
      "ðŸ”„ Processing: CX Effortless_Guide_EN_V2\n",
      "ðŸ“ File: knowledge_base_prod/CX/CX Effortless_Guide_EN_V2.pdf\n",
      "ðŸ“‹ Mapped to: DGE > CX > Guide\n",
      "ðŸ”„ Processing: CX Effortless_Guide_EN_V2.pdf\n",
      "ðŸŒ Using remote Docling server: http://74.162.37.71:5001/v1alpha/convert/file\n",
      "ðŸ” Processing with Docling: CX Effortless_Guide_EN_V2.pdf\n",
      "ðŸ“‹ Available fields in document: ['filename', 'md_content', 'json_content', 'html_content', 'text_content', 'doctags_content']\n",
      "âœ… json_content has content: 13 chars\n",
      "âŒ text_content is None or empty\n",
      "âŒ md_content is None or empty\n",
      "âŒ html_content is None or empty\n",
      "ðŸ” json_content keys: ['schema_name', 'version', 'name', 'origin', 'furniture', 'body', 'groups', 'texts', 'pictures', 'tables', 'key_value_items', 'form_items', 'pages']\n",
      "âœ… Extracted text from json_content.texts: 172497 chars\n",
      "âœ… Successfully deserialized DoclingDocument from remote server\n",
      "ðŸ“‹ Extracted 308 sections from DoclingDocument\n",
      "ðŸ’¾ Cached: CX Effortless_Guide_EN_V2.pdf\n",
      "ðŸ§© Processing document to chunks: Â© Issued by Department of Government Enablement | 2023 First Edition\n",
      " â€¢ Generated 149 chunks using token-based chunking\n",
      " â€¢ Mapped 149 chunks to 308 sections\n",
      "ðŸš€ Enriching 149 chunks with 8 workers...\n",
      "ðŸ’¾ Cached 149 enriched chunks for CX Effortless_Guide_EN_V2\n",
      " â€¢ Enhanced 149 chunks with contextualization\n",
      "ðŸ”„ Generating embeddings for 149 chunks...\n",
      "ðŸ” Processing batch 1/5 (32 texts)\n",
      "âš ï¸  Batch exceeds token limit (10109 > 7992), reducing batch size\n",
      "âœ… Successfully processed 32 texts with batch_size=16\n",
      "ðŸ“Š Generated embeddings for batch 1/5\n",
      "ðŸ” Processing batch 2/5 (32 texts)\n",
      "âš ï¸  Batch exceeds token limit (10083 > 7992), reducing batch size\n",
      "âœ… Successfully processed 32 texts with batch_size=16\n",
      "ðŸ“Š Generated embeddings for batch 2/5\n",
      "ðŸ” Processing batch 3/5 (32 texts)\n",
      "âš ï¸  Batch exceeds token limit (10053 > 7992), reducing batch size\n",
      "âœ… Successfully processed 32 texts with batch_size=16\n",
      "ðŸ“Š Generated embeddings for batch 3/5\n",
      "ðŸ” Processing batch 4/5 (32 texts)\n",
      "âš ï¸  Batch exceeds token limit (9971 > 7992), reducing batch size\n",
      "âœ… Successfully processed 32 texts with batch_size=16\n",
      "ðŸ“Š Generated embeddings for batch 4/5\n",
      "ðŸ” Processing batch 5/5 (21 texts)\n",
      "ðŸ“Š Generated embeddings for batch 5/5\n",
      "âœ… Generated 149 embeddings with token-aware processing\n",
      " â€¢ Generated 149 embeddings\n",
      " â€¢ Created 149 KBChunkEmbedding objects\n",
      "âœ… Stored 149 Knowledge Base embeddings\n",
      "âœ… Successfully processed CX Effortless_Guide_EN_V2\n",
      "ðŸ“Š Stored 149 chunks in Knowledge Base\n",
      "ðŸ“‹ Cached: CX Effortless_Guide_EN_V2.pdf\n",
      "   âœ… Success (497.55s)\n",
      "   Progress: 2/14 (14.3%) - ETA: 54.6min\n",
      "\n",
      "ðŸ“„ Processing document 3/14: CX Glossary\n",
      "   Language: ar+en\n",
      "   Entity: DGE\n",
      "   Domain: CX\n",
      "   Category: Glossary\n",
      "\n",
      "ðŸ”„ Processing: CX Glossary\n",
      "ðŸ“ File: knowledge_base_prod/CX/CX Glossary.pdf\n",
      "ðŸ“‹ Mapped to: DGE > CX > Glossary\n",
      "ðŸ”„ Processing: CX Glossary.pdf\n",
      "ðŸŒ Using remote Docling server: http://74.162.37.71:5001/v1alpha/convert/file\n",
      "ðŸ” Processing with Docling: CX Glossary.pdf\n",
      "ðŸ“‹ Available fields in document: ['filename', 'md_content', 'json_content', 'html_content', 'text_content', 'doctags_content']\n",
      "âœ… json_content has content: 13 chars\n",
      "âŒ text_content is None or empty\n",
      "âŒ md_content is None or empty\n",
      "âŒ html_content is None or empty\n",
      "ðŸ” json_content keys: ['schema_name', 'version', 'name', 'origin', 'furniture', 'body', 'groups', 'texts', 'pictures', 'tables', 'key_value_items', 'form_items', 'pages']\n",
      "âœ… Extracted text from json_content.texts: 5309 chars\n",
      "âœ… Successfully deserialized DoclingDocument from remote server\n",
      "ðŸ“‹ Extracted 1 sections from DoclingDocument\n",
      "ðŸ’¾ Cached: CX Glossary.pdf\n",
      "ðŸ§© Processing document to chunks: Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…ØµØ·Ù„Ø­Ø§Øª Glossary\n",
      " â€¢ Generated 3 chunks using token-based chunking\n",
      " â€¢ Mapped 3 chunks to 1 sections\n",
      "ðŸš€ Enriching 3 chunks with 8 workers...\n",
      "ðŸ’¾ Cached 3 enriched chunks for CX Glossary\n",
      " â€¢ Enhanced 3 chunks with contextualization\n",
      "ðŸ”„ Generating embeddings for 3 chunks...\n",
      "ðŸ” Processing batch 1/1 (3 texts)\n",
      "ðŸ“Š Generated embeddings for batch 1/1\n",
      "âœ… Generated 3 embeddings with token-aware processing\n",
      " â€¢ Generated 3 embeddings\n",
      " â€¢ Created 3 KBChunkEmbedding objects\n",
      "âœ… Stored 3 Knowledge Base embeddings\n",
      "âœ… Successfully processed CX Glossary\n",
      "ðŸ“Š Stored 3 chunks in Knowledge Base\n",
      "ðŸ“‹ Cached: CX Glossary.pdf\n",
      "   âœ… Success (19.71s)\n",
      "   Progress: 3/14 (21.4%) - ETA: 34.6min\n",
      "\n",
      "ðŸ“„ Processing document 4/14: CX TOV templates\n",
      "   Language: ar+en\n",
      "   Entity: DGE\n",
      "   Domain: CX\n",
      "   Category: Templates\n",
      "\n",
      "ðŸ”„ Processing: CX TOV templates\n",
      "ðŸ“ File: knowledge_base_prod/CX/CX TOV templates.pdf\n",
      "ðŸ“‹ Mapped to: DGE > CX > Templates\n",
      "ðŸ”„ Processing: CX TOV templates.pdf\n",
      "ðŸŒ Using remote Docling server: http://74.162.37.71:5001/v1alpha/convert/file\n",
      "ðŸ” Processing with Docling: CX TOV templates.pdf\n",
      "ðŸ“‹ Available fields in document: ['filename', 'md_content', 'json_content', 'html_content', 'text_content', 'doctags_content']\n",
      "âœ… json_content has content: 13 chars\n",
      "âŒ text_content is None or empty\n",
      "âŒ md_content is None or empty\n",
      "âŒ html_content is None or empty\n",
      "ðŸ” json_content keys: ['schema_name', 'version', 'name', 'origin', 'furniture', 'body', 'groups', 'texts', 'pictures', 'tables', 'key_value_items', 'form_items', 'pages']\n",
      "âœ… Extracted text from json_content.texts: 86302 chars\n",
      "âœ… Successfully deserialized DoclingDocument from remote server\n",
      "ðŸ“‹ Extracted 203 sections from DoclingDocument\n",
      "ðŸ’¾ Cached: CX TOV templates.pdf\n",
      "ðŸ§© Processing document to chunks: Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ùˆ Ø§Ù„Ø£Ù…Ø«Ù„Ø© Templates and Examples\n",
      " â€¢ Generated 173 chunks using token-based chunking\n",
      " â€¢ Mapped 173 chunks to 203 sections\n",
      "ðŸš€ Enriching 173 chunks with 8 workers...\n",
      "ðŸ’¾ Cached 173 enriched chunks for CX TOV templates\n",
      " â€¢ Enhanced 173 chunks with contextualization\n",
      "ðŸ”„ Generating embeddings for 173 chunks...\n",
      "ðŸ” Processing batch 1/6 (32 texts)\n",
      "âš ï¸  Batch exceeds token limit (9529 > 7992), reducing batch size\n",
      "âœ… Successfully processed 32 texts with batch_size=16\n",
      "ðŸ“Š Generated embeddings for batch 1/6\n",
      "ðŸ” Processing batch 2/6 (32 texts)\n",
      "âš ï¸  Batch exceeds token limit (9396 > 7992), reducing batch size\n",
      "âœ… Successfully processed 32 texts with batch_size=16\n",
      "ðŸ“Š Generated embeddings for batch 2/6\n",
      "ðŸ” Processing batch 3/6 (32 texts)\n",
      "âš ï¸  Batch exceeds token limit (9557 > 7992), reducing batch size\n",
      "âœ… Successfully processed 32 texts with batch_size=16\n",
      "ðŸ“Š Generated embeddings for batch 3/6\n",
      "ðŸ” Processing batch 4/6 (32 texts)\n",
      "âš ï¸  Batch exceeds token limit (9426 > 7992), reducing batch size\n",
      "âœ… Successfully processed 32 texts with batch_size=16\n",
      "ðŸ“Š Generated embeddings for batch 4/6\n",
      "ðŸ” Processing batch 5/6 (32 texts)\n",
      "âš ï¸  Batch exceeds token limit (9425 > 7992), reducing batch size\n",
      "âœ… Successfully processed 32 texts with batch_size=16\n",
      "ðŸ“Š Generated embeddings for batch 5/6\n",
      "ðŸ” Processing batch 6/6 (13 texts)\n",
      "ðŸ“Š Generated embeddings for batch 6/6\n",
      "âœ… Generated 173 embeddings with token-aware processing\n",
      " â€¢ Generated 173 embeddings\n",
      " â€¢ Created 173 KBChunkEmbedding objects\n",
      "âœ… Stored 173 Knowledge Base embeddings\n",
      "âœ… Successfully processed CX TOV templates\n",
      "ðŸ“Š Stored 173 chunks in Knowledge Base\n",
      "ðŸ“‹ Cached: CX TOV templates.pdf\n",
      "   âœ… Success (850.86s)\n",
      "   Progress: 4/14 (28.6%) - ETA: 59.0min\n",
      "\n",
      "ðŸ“„ Processing document 5/14: Abu Dhabi Government Finance Policy Manual_EN\n",
      "   Language: en\n",
      "   Entity: Department of Finance\n",
      "   Domain: Finance\n",
      "   Category: Policy\n",
      "\n",
      "ðŸ”„ Processing: Abu Dhabi Government Finance Policy Manual_EN\n",
      "ðŸ“ File: knowledge_base_prod/Finance/Abu Dhabi Government Finance Policy Manual_EN.pdf\n",
      "ðŸ“‹ Mapped to: Department of Finance > Finance > Policy\n",
      "ðŸ”„ Processing: Abu Dhabi Government Finance Policy Manual_EN.pdf\n",
      "ðŸŒ Using remote Docling server: http://74.162.37.71:5001/v1alpha/convert/file\n",
      "ðŸ” Processing with Docling: Abu Dhabi Government Finance Policy Manual_EN.pdf\n",
      "ðŸ“‹ Available fields in document: ['filename', 'md_content', 'json_content', 'html_content', 'text_content', 'doctags_content']\n",
      "âœ… json_content has content: 13 chars\n",
      "âŒ text_content is None or empty\n",
      "âŒ md_content is None or empty\n",
      "âŒ html_content is None or empty\n",
      "ðŸ” json_content keys: ['schema_name', 'version', 'name', 'origin', 'furniture', 'body', 'groups', 'texts', 'pictures', 'tables', 'key_value_items', 'form_items', 'pages']\n",
      "âœ… Extracted text from json_content.texts: 145675 chars\n",
      "âœ… Successfully deserialized DoclingDocument from remote server\n",
      "ðŸ“‹ Extracted 135 sections from DoclingDocument\n",
      "ðŸ’¾ Cached: Abu Dhabi Government Finance Policy Manual_EN.pdf\n",
      "ðŸ§© Processing document to chunks: Abu Dhabi Government Finance Policy Manual Department of Finance\n",
      " â€¢ Generated 129 chunks using token-based chunking\n",
      " â€¢ Mapped 129 chunks to 135 sections\n",
      "ðŸš€ Enriching 129 chunks with 8 workers...\n",
      "ðŸ’¾ Cached 129 enriched chunks for Abu Dhabi Government Finance Policy Manual_EN\n",
      " â€¢ Enhanced 129 chunks with contextualization\n",
      "ðŸ”„ Generating embeddings for 129 chunks...\n",
      "ðŸ” Processing batch 1/5 (32 texts)\n",
      "âš ï¸  Batch exceeds token limit (10279 > 7992), reducing batch size\n",
      "âœ… Successfully processed 32 texts with batch_size=16\n",
      "ðŸ“Š Generated embeddings for batch 1/5\n",
      "ðŸ” Processing batch 2/5 (32 texts)\n",
      "âš ï¸  Batch exceeds token limit (10269 > 7992), reducing batch size\n",
      "âœ… Successfully processed 32 texts with batch_size=16\n",
      "ðŸ“Š Generated embeddings for batch 2/5\n",
      "ðŸ” Processing batch 3/5 (32 texts)\n",
      "âš ï¸  Batch exceeds token limit (10333 > 7992), reducing batch size\n",
      "âœ… Successfully processed 32 texts with batch_size=16\n",
      "ðŸ“Š Generated embeddings for batch 3/5\n",
      "ðŸ” Processing batch 4/5 (32 texts)\n",
      "âš ï¸  Batch exceeds token limit (10331 > 7992), reducing batch size\n",
      "âœ… Successfully processed 32 texts with batch_size=16\n",
      "ðŸ“Š Generated embeddings for batch 4/5\n",
      "ðŸ” Processing batch 5/5 (1 texts)\n",
      "ðŸ“Š Generated embeddings for batch 5/5\n",
      "âœ… Generated 129 embeddings with token-aware processing\n",
      " â€¢ Generated 129 embeddings\n",
      " â€¢ Created 129 KBChunkEmbedding objects\n",
      "âœ… Stored 129 Knowledge Base embeddings\n",
      "âœ… Successfully processed Abu Dhabi Government Finance Policy Manual_EN\n",
      "ðŸ“Š Stored 129 chunks in Knowledge Base\n",
      "ðŸ“‹ Cached: Abu Dhabi Government Finance Policy Manual_EN.pdf\n",
      "   âœ… Success (163.15s)\n",
      "   Progress: 5/14 (35.7%) - ETA: 47.4min\n",
      "\n",
      "ðŸ“„ Processing document 6/14: HR - HR Law - EN\n",
      "   Language: en\n",
      "   Entity: Human Resource Authority\n",
      "   Domain: HR\n",
      "   Category: Law\n",
      "\n",
      "ðŸ”„ Processing: HR - HR Law - EN\n",
      "ðŸ“ File: knowledge_base_prod/HR/HR - HR Law - EN.pdf\n",
      "ðŸ“‹ Mapped to: Human Resource Authority > HR > Law\n",
      "ðŸ”„ Processing: HR - HR Law - EN.pdf\n",
      "ðŸŒ Using remote Docling server: http://74.162.37.71:5001/v1alpha/convert/file\n",
      "ðŸ” Processing with Docling: HR - HR Law - EN.pdf\n",
      "ðŸ“‹ Available fields in document: ['filename', 'md_content', 'json_content', 'html_content', 'text_content', 'doctags_content']\n",
      "âœ… json_content has content: 13 chars\n",
      "âŒ text_content is None or empty\n",
      "âŒ md_content is None or empty\n",
      "âŒ html_content is None or empty\n",
      "ðŸ” json_content keys: ['schema_name', 'version', 'name', 'origin', 'furniture', 'body', 'groups', 'texts', 'pictures', 'tables', 'key_value_items', 'form_items', 'pages']\n",
      "âœ… Extracted text from json_content.texts: 53606 chars\n",
      "âœ… Successfully deserialized DoclingDocument from remote server\n",
      "ðŸ“‹ Extracted 105 sections from DoclingDocument\n",
      "ðŸ’¾ Cached: HR - HR Law - EN.pdf\n",
      "ðŸ§© Processing document to chunks: HR law\n",
      " â€¢ Generated 46 chunks using token-based chunking\n",
      " â€¢ Mapped 46 chunks to 105 sections\n",
      "ðŸš€ Enriching 46 chunks with 8 workers...\n",
      "ðŸ’¾ Cached 46 enriched chunks for HR - HR Law - EN\n",
      " â€¢ Enhanced 46 chunks with contextualization\n",
      "ðŸ”„ Generating embeddings for 46 chunks...\n",
      "ðŸ” Processing batch 1/2 (32 texts)\n",
      "âš ï¸  Batch exceeds token limit (10124 > 7992), reducing batch size\n",
      "âœ… Successfully processed 32 texts with batch_size=16\n",
      "ðŸ“Š Generated embeddings for batch 1/2\n",
      "ðŸ” Processing batch 2/2 (14 texts)\n",
      "ðŸ“Š Generated embeddings for batch 2/2\n",
      "âœ… Generated 46 embeddings with token-aware processing\n",
      " â€¢ Generated 46 embeddings\n",
      " â€¢ Created 46 KBChunkEmbedding objects\n",
      "âœ… Stored 46 Knowledge Base embeddings\n",
      "âœ… Successfully processed HR - HR Law - EN\n",
      "ðŸ“Š Stored 46 chunks in Knowledge Base\n",
      "ðŸ“‹ Cached: HR - HR Law - EN.pdf\n",
      "   âœ… Success (54.33s)\n",
      "   Progress: 6/14 (42.9%) - ETA: 36.3min\n",
      "\n",
      "ðŸ“„ Processing document 7/14: HR - Implementation Regulation for HR Law No 6 Year 2016 - EN\n",
      "   Language: en\n",
      "   Entity: Human Resource Authority\n",
      "   Domain: HR\n",
      "   Category: Law\n",
      "\n",
      "ðŸ”„ Processing: HR - Implementation Regulation for HR Law No 6 Year 2016 - EN\n",
      "ðŸ“ File: knowledge_base_prod/HR/HR - Implementation Regulation for HR Law No 6 Year 2016 - EN.pdf\n",
      "ðŸ“‹ Mapped to: Human Resource Authority > HR > Law\n",
      "ðŸ”„ Processing: HR - Implementation Regulation for HR Law No 6 Year 2016 - EN.pdf\n",
      "ðŸŒ Using remote Docling server: http://74.162.37.71:5001/v1alpha/convert/file\n",
      "ðŸ” Processing with Docling: HR - Implementation Regulation for HR Law No 6 Year 2016 - EN.pdf\n",
      "ðŸ“‹ Available fields in document: ['filename', 'md_content', 'json_content', 'html_content', 'text_content', 'doctags_content']\n",
      "âœ… json_content has content: 13 chars\n",
      "âŒ text_content is None or empty\n",
      "âŒ md_content is None or empty\n",
      "âŒ html_content is None or empty\n",
      "ðŸ” json_content keys: ['schema_name', 'version', 'name', 'origin', 'furniture', 'body', 'groups', 'texts', 'pictures', 'tables', 'key_value_items', 'form_items', 'pages']\n",
      "âœ… Extracted text from json_content.texts: 179619 chars\n",
      "âœ… Successfully deserialized DoclingDocument from remote server\n",
      "ðŸ“‹ Extracted 198 sections from DoclingDocument\n",
      "ðŸ’¾ Cached: HR - Implementation Regulation for HR Law No 6 Year 2016 - EN.pdf\n",
      "ðŸ§© Processing document to chunks: Decision No . (     10      ) of 2020\n",
      " â€¢ Generated 165 chunks using token-based chunking\n",
      " â€¢ Mapped 165 chunks to 198 sections\n",
      "ðŸš€ Enriching 165 chunks with 8 workers...\n",
      "ðŸ’¾ Cached 165 enriched chunks for HR - Implementation Regulation for HR Law No 6 Year 2016 - EN\n",
      " â€¢ Enhanced 165 chunks with contextualization\n",
      "ðŸ”„ Generating embeddings for 165 chunks...\n",
      "ðŸ” Processing batch 1/6 (32 texts)\n",
      "âš ï¸  Batch exceeds token limit (10301 > 7992), reducing batch size\n",
      "âœ… Successfully processed 32 texts with batch_size=16\n",
      "ðŸ“Š Generated embeddings for batch 1/6\n",
      "ðŸ” Processing batch 2/6 (32 texts)\n",
      "âš ï¸  Batch exceeds token limit (10211 > 7992), reducing batch size\n",
      "âœ… Successfully processed 32 texts with batch_size=16\n",
      "ðŸ“Š Generated embeddings for batch 2/6\n",
      "ðŸ” Processing batch 3/6 (32 texts)\n",
      "âš ï¸  Batch exceeds token limit (10351 > 7992), reducing batch size\n",
      "âœ… Successfully processed 32 texts with batch_size=16\n",
      "ðŸ“Š Generated embeddings for batch 3/6\n",
      "ðŸ” Processing batch 4/6 (32 texts)\n",
      "âš ï¸  Batch exceeds token limit (10399 > 7992), reducing batch size\n",
      "âœ… Successfully processed 32 texts with batch_size=16\n",
      "ðŸ“Š Generated embeddings for batch 4/6\n",
      "ðŸ” Processing batch 5/6 (32 texts)\n",
      "âš ï¸  Batch exceeds token limit (10148 > 7992), reducing batch size\n",
      "âœ… Successfully processed 32 texts with batch_size=16\n",
      "ðŸ“Š Generated embeddings for batch 5/6\n",
      "ðŸ” Processing batch 6/6 (5 texts)\n",
      "ðŸ“Š Generated embeddings for batch 6/6\n",
      "âœ… Generated 165 embeddings with token-aware processing\n",
      " â€¢ Generated 165 embeddings\n",
      " â€¢ Created 165 KBChunkEmbedding objects\n",
      "âœ… Stored 165 Knowledge Base embeddings\n",
      "âœ… Successfully processed HR - Implementation Regulation for HR Law No 6 Year 2016 - EN\n",
      "ðŸ“Š Stored 165 chunks in Knowledge Base\n",
      "ðŸ“‹ Cached: HR - Implementation Regulation for HR Law No 6 Year 2016 - EN.pdf\n",
      "   âœ… Success (217.31s)\n",
      "   Progress: 7/14 (50.0%) - ETA: 30.9min\n",
      "\n",
      "ðŸ“„ Processing document 8/14: Proc - DGE Procurement Framework - EN\n",
      "   Language: en\n",
      "   Entity: DGE\n",
      "   Domain: Procurement\n",
      "   Category: Framework\n",
      "\n",
      "ðŸ”„ Processing: Proc - DGE Procurement Framework - EN\n",
      "ðŸ“ File: knowledge_base_prod/Procurement/Proc - DGE Procurement Framework - EN.DOC\n",
      "ðŸ“‹ Mapped to: DGE > Procurement > Framework\n",
      "ðŸ”„ Processing: Proc - DGE Procurement Framework - EN.DOC\n",
      "ðŸŒ Using remote Docling server: http://74.162.37.71:5001/v1alpha/convert/file\n",
      "ðŸ” Processing with Docling: Proc - DGE Procurement Framework - EN.DOC\n",
      "ðŸ“‹ Available fields in document: ['filename', 'md_content', 'json_content', 'html_content', 'text_content', 'doctags_content']\n",
      "âœ… json_content has content: 13 chars\n",
      "âŒ text_content is None or empty\n",
      "âŒ md_content is None or empty\n",
      "âŒ html_content is None or empty\n",
      "ðŸ” json_content keys: ['schema_name', 'version', 'name', 'origin', 'furniture', 'body', 'groups', 'texts', 'pictures', 'tables', 'key_value_items', 'form_items', 'pages']\n",
      "âœ… Extracted text from json_content.texts: 221545 chars\n",
      "âœ… Successfully deserialized DoclingDocument from remote server\n",
      "ðŸ“‹ Extracted 244 sections from DoclingDocument\n",
      "ðŸ’¾ Cached: Proc - DGE Procurement Framework - EN.DOC\n",
      "ðŸ§© Processing document to chunks: DGE Revised Procurement Framework\n",
      " â€¢ Generated 185 chunks using token-based chunking\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      " â€¢ Mapped 185 chunks to 244 sections\n",
      "ðŸš€ Enriching 185 chunks with 8 workers...\n",
      "ðŸ’¾ Cached 185 enriched chunks for Proc - DGE Procurement Framework - EN.DOC\n",
      " â€¢ Enhanced 185 chunks with contextualization\n",
      "ðŸ”„ Generating embeddings for 185 chunks...\n",
      "ðŸ” Processing batch 1/6 (32 texts)\n",
      "âš ï¸  Batch exceeds token limit (10177 > 7992), reducing batch size\n",
      "âœ… Successfully processed 32 texts with batch_size=16\n",
      "ðŸ“Š Generated embeddings for batch 1/6\n",
      "ðŸ” Processing batch 2/6 (32 texts)\n",
      "âš ï¸  Batch exceeds token limit (10334 > 7992), reducing batch size\n",
      "âœ… Successfully processed 32 texts with batch_size=16\n",
      "ðŸ“Š Generated embeddings for batch 2/6\n",
      "ðŸ” Processing batch 3/6 (32 texts)\n",
      "âš ï¸  Batch exceeds token limit (10332 > 7992), reducing batch size\n",
      "âœ… Successfully processed 32 texts with batch_size=16\n",
      "ðŸ“Š Generated embeddings for batch 3/6\n",
      "ðŸ” Processing batch 4/6 (32 texts)\n",
      "âš ï¸  Batch exceeds token limit (10413 > 7992), reducing batch size\n",
      "âœ… Successfully processed 32 texts with batch_size=16\n",
      "ðŸ“Š Generated embeddings for batch 4/6\n",
      "ðŸ” Processing batch 5/6 (32 texts)\n",
      "âš ï¸  Batch exceeds token limit (10489 > 7992), reducing batch size\n",
      "âœ… Successfully processed 32 texts with batch_size=16\n",
      "ðŸ“Š Generated embeddings for batch 5/6\n",
      "ðŸ” Processing batch 6/6 (25 texts)\n",
      "ðŸ“Š Generated embeddings for batch 6/6\n",
      "âœ… Generated 185 embeddings with token-aware processing\n",
      " â€¢ Generated 185 embeddings\n",
      " â€¢ Created 185 KBChunkEmbedding objects\n",
      "âœ… Stored 185 Knowledge Base embeddings\n",
      "âœ… Successfully processed Proc - DGE Procurement Framework - EN\n",
      "ðŸ“Š Stored 185 chunks in Knowledge Base\n",
      "ðŸ“‹ Cached: Proc - DGE Procurement Framework - EN.DOC\n",
      "   âœ… Success (62.58s)\n",
      "   Progress: 8/14 (57.1%) - ETA: 23.9min\n",
      "\n",
      "ðŸ“„ Processing document 9/14: Proc - Policies and Procedures for Sales Auctions and Warehouses - EN\n",
      "   Language: en\n",
      "   Entity: DGE\n",
      "   Domain: Procurement\n",
      "   Category: Policy\n",
      "\n",
      "ðŸ”„ Processing: Proc - Policies and Procedures for Sales Auctions and Warehouses - EN\n",
      "ðŸ“ File: knowledge_base_prod/Procurement/Proc - Policies and Procedures for Sales Auctions and Warehouses - EN.PDF\n",
      "ðŸ“‹ Mapped to: DGE > Procurement > Policy\n",
      "ðŸ”„ Processing: Proc - Policies and Procedures for Sales Auctions and Warehouses - EN.PDF\n",
      "ðŸŒ Using remote Docling server: http://74.162.37.71:5001/v1alpha/convert/file\n",
      "ðŸ” Processing with Docling: Proc - Policies and Procedures for Sales Auctions and Warehouses - EN.PDF\n",
      "ðŸ“‹ Available fields in document: ['filename', 'md_content', 'json_content', 'html_content', 'text_content', 'doctags_content']\n",
      "âœ… json_content has content: 13 chars\n",
      "âŒ text_content is None or empty\n",
      "âŒ md_content is None or empty\n",
      "âŒ html_content is None or empty\n",
      "ðŸ” json_content keys: ['schema_name', 'version', 'name', 'origin', 'furniture', 'body', 'groups', 'texts', 'pictures', 'tables', 'key_value_items', 'form_items', 'pages']\n",
      "âœ… Extracted text from json_content.texts: 16207 chars\n",
      "âœ… Successfully deserialized DoclingDocument from remote server\n",
      "ðŸ“‹ Extracted 21 sections from DoclingDocument\n",
      "ðŸ’¾ Cached: Proc - Policies and Procedures for Sales Auctions and Warehouses - EN.PDF\n",
      "ðŸ§© Processing document to chunks: Policies and procedures for Sales Auctions and Warehouses\n",
      " â€¢ Generated 15 chunks using token-based chunking\n",
      " â€¢ Mapped 15 chunks to 21 sections\n",
      "ðŸš€ Enriching 15 chunks with 8 workers...\n",
      "ðŸ’¾ Cached 15 enriched chunks for Proc - Policies and Procedures for Sales Auctions and Warehouses - EN\n",
      " â€¢ Enhanced 15 chunks with contextualization\n",
      "ðŸ”„ Generating embeddings for 15 chunks...\n",
      "ðŸ” Processing batch 1/1 (15 texts)\n",
      "ðŸ“Š Generated embeddings for batch 1/1\n",
      "âœ… Generated 15 embeddings with token-aware processing\n",
      " â€¢ Generated 15 embeddings\n",
      " â€¢ Created 15 KBChunkEmbedding objects\n",
      "âœ… Stored 15 Knowledge Base embeddings\n",
      "âœ… Successfully processed Proc - Policies and Procedures for Sales Auctions and Warehouses - EN\n",
      "ðŸ“Š Stored 15 chunks in Knowledge Base\n",
      "ðŸ“‹ Cached: Proc - Policies and Procedures for Sales Auctions and Warehouses - EN.PDF\n",
      "   âœ… Success (10.60s)\n",
      "   Progress: 9/14 (64.3%) - ETA: 17.8min\n",
      "\n",
      "ðŸ“„ Processing document 10/14: Proc - Procurement Charter - EN\n",
      "   Language: en\n",
      "   Entity: DGE\n",
      "   Domain: Procurement\n",
      "   Category: Charter\n",
      "\n",
      "ðŸ”„ Processing: Proc - Procurement Charter - EN\n",
      "ðŸ“ File: knowledge_base_prod/Procurement/Proc - Procurement Charter - EN.pdf\n",
      "ðŸ“‹ Mapped to: DGE > Procurement > Charter\n",
      "ðŸ”„ Processing: Proc - Procurement Charter - EN.pdf\n",
      "ðŸŒ Using remote Docling server: http://74.162.37.71:5001/v1alpha/convert/file\n",
      "ðŸ” Processing with Docling: Proc - Procurement Charter - EN.pdf\n",
      "ðŸ“‹ Available fields in document: ['filename', 'md_content', 'json_content', 'html_content', 'text_content', 'doctags_content']\n",
      "âœ… json_content has content: 13 chars\n",
      "âŒ text_content is None or empty\n",
      "âŒ md_content is None or empty\n",
      "âŒ html_content is None or empty\n",
      "ðŸ” json_content keys: ['schema_name', 'version', 'name', 'origin', 'furniture', 'body', 'groups', 'texts', 'pictures', 'tables', 'key_value_items', 'form_items', 'pages']\n",
      "âœ… Extracted text from json_content.texts: 24552 chars\n",
      "âœ… Successfully deserialized DoclingDocument from remote server\n",
      "ðŸ“‹ Extracted 37 sections from DoclingDocument\n",
      "ðŸ’¾ Cached: Proc - Procurement Charter - EN.pdf\n",
      "ðŸ§© Processing document to chunks: Procurement Charter Issued pursuant to the Abu Dhabi Procurement Standards\n",
      " â€¢ Generated 21 chunks using token-based chunking\n",
      " â€¢ Mapped 21 chunks to 37 sections\n",
      "ðŸš€ Enriching 21 chunks with 8 workers...\n",
      "ðŸ’¾ Cached 21 enriched chunks for Proc - Procurement Charter - EN\n",
      " â€¢ Enhanced 21 chunks with contextualization\n",
      "ðŸ”„ Generating embeddings for 21 chunks...\n",
      "ðŸ” Processing batch 1/1 (21 texts)\n",
      "ðŸ“Š Generated embeddings for batch 1/1\n",
      "âœ… Generated 21 embeddings with token-aware processing\n",
      " â€¢ Generated 21 embeddings\n",
      " â€¢ Created 21 KBChunkEmbedding objects\n",
      "âœ… Stored 21 Knowledge Base embeddings\n",
      "âœ… Successfully processed Proc - Procurement Charter - EN\n",
      "ðŸ“Š Stored 21 chunks in Knowledge Base\n",
      "ðŸ“‹ Cached: Proc - Procurement Charter - EN.pdf\n",
      "   âœ… Success (19.52s)\n",
      "   Progress: 10/14 (71.4%) - ETA: 13.0min\n",
      "\n",
      "ðŸ“„ Processing document 11/14: Proc - Procurement Manual (Ariba Aligned) - EN\n",
      "   Language: en\n",
      "   Entity: DGE\n",
      "   Domain: Procurement\n",
      "   Category: Manual\n",
      "\n",
      "ðŸ”„ Processing: Proc - Procurement Manual (Ariba Aligned) - EN\n",
      "ðŸ“ File: knowledge_base_prod/Procurement/Proc - Procurement Manual (Ariba Aligned) - EN.pdf\n",
      "ðŸ“‹ Mapped to: DGE > Procurement > Manual\n",
      "ðŸ”„ Processing: Proc - Procurement Manual (Ariba Aligned) - EN.pdf\n",
      "ðŸŒ Using remote Docling server: http://74.162.37.71:5001/v1alpha/convert/file\n",
      "ðŸ” Processing with Docling: Proc - Procurement Manual (Ariba Aligned) - EN.pdf\n",
      "âŒ Failed to process Proc - Procurement Manual (Ariba Aligned) - EN.pdf: Docling processing failed: 500 Server Error: Internal Server Error for url: http://74.162.37.71:5001/v1alpha/convert/file\n",
      "âŒ Document processing failed\n",
      "âŒ Failed: Proc - Procurement Manual (Ariba Aligned) - EN.pdf - Document processing failed\n",
      "   âŒ Failed (121.20s)\n",
      "   Progress: 11/14 (78.6%) - ETA: 9.4min\n",
      "\n",
      "ðŸ“„ Processing document 12/14: Proc - Procurement Manual (Business Process) - EN\n",
      "   Language: en\n",
      "   Entity: DGE\n",
      "   Domain: Procurement\n",
      "   Category: Manual\n",
      "\n",
      "ðŸ”„ Processing: Proc - Procurement Manual (Business Process) - EN\n",
      "ðŸ“ File: knowledge_base_prod/Procurement/Proc - Procurement Manual (Business Process) - EN.pdf\n",
      "ðŸ“‹ Mapped to: DGE > Procurement > Manual\n",
      "ðŸ”„ Processing: Proc - Procurement Manual (Business Process) - EN.pdf\n",
      "ðŸŒ Using remote Docling server: http://74.162.37.71:5001/v1alpha/convert/file\n",
      "ðŸ” Processing with Docling: Proc - Procurement Manual (Business Process) - EN.pdf\n",
      "ðŸ“‹ Available fields in document: ['filename', 'md_content', 'json_content', 'html_content', 'text_content', 'doctags_content']\n",
      "âœ… json_content has content: 13 chars\n",
      "âŒ text_content is None or empty\n",
      "âŒ md_content is None or empty\n",
      "âŒ html_content is None or empty\n",
      "ðŸ” json_content keys: ['schema_name', 'version', 'name', 'origin', 'furniture', 'body', 'groups', 'texts', 'pictures', 'tables', 'key_value_items', 'form_items', 'pages']\n",
      "âœ… Extracted text from json_content.texts: 125235 chars\n",
      "âœ… Successfully deserialized DoclingDocument from remote server\n",
      "ðŸ“‹ Extracted 365 sections from DoclingDocument\n",
      "ðŸ’¾ Cached: Proc - Procurement Manual (Business Process) - EN.pdf\n",
      "ðŸ§© Processing document to chunks: Procurement Manual (Business Processes) Issued pursuant to the Abu Dhabi Procurement Standards\n",
      " â€¢ Generated 121 chunks using token-based chunking\n",
      " â€¢ Mapped 121 chunks to 365 sections\n",
      "ðŸš€ Enriching 121 chunks with 8 workers...\n",
      "ðŸ’¾ Cached 121 enriched chunks for Proc - Procurement Manual (Business Process) - EN\n",
      " â€¢ Enhanced 121 chunks with contextualization\n",
      "ðŸ”„ Generating embeddings for 121 chunks...\n",
      "ðŸ” Processing batch 1/4 (32 texts)\n",
      "âš ï¸  Batch exceeds token limit (10145 > 7992), reducing batch size\n",
      "âœ… Successfully processed 32 texts with batch_size=16\n",
      "ðŸ“Š Generated embeddings for batch 1/4\n",
      "ðŸ” Processing batch 2/4 (32 texts)\n",
      "âš ï¸  Batch exceeds token limit (10273 > 7992), reducing batch size\n",
      "âœ… Successfully processed 32 texts with batch_size=16\n",
      "ðŸ“Š Generated embeddings for batch 2/4\n",
      "ðŸ” Processing batch 3/4 (32 texts)\n",
      "âš ï¸  Batch exceeds token limit (10118 > 7992), reducing batch size\n",
      "âœ… Successfully processed 32 texts with batch_size=16\n",
      "ðŸ“Š Generated embeddings for batch 3/4\n",
      "ðŸ” Processing batch 4/4 (25 texts)\n",
      "ðŸ“Š Generated embeddings for batch 4/4\n",
      "âœ… Generated 121 embeddings with token-aware processing\n",
      " â€¢ Generated 121 embeddings\n",
      " â€¢ Created 121 KBChunkEmbedding objects\n",
      "âœ… Stored 121 Knowledge Base embeddings\n",
      "âœ… Successfully processed Proc - Procurement Manual (Business Process) - EN\n",
      "ðŸ“Š Stored 121 chunks in Knowledge Base\n",
      "ðŸ“‹ Cached: Proc - Procurement Manual (Business Process) - EN.pdf\n",
      "   âœ… Success (228.72s)\n",
      "   Progress: 12/14 (85.7%) - ETA: 6.4min\n",
      "\n",
      "ðŸ“„ Processing document 13/14: Proc - Procurement Standard Regulations - EN\n",
      "   Language: en\n",
      "   Entity: DGE\n",
      "   Domain: Procurement\n",
      "   Category: Manual\n",
      "\n",
      "ðŸ”„ Processing: Proc - Procurement Standard Regulations - EN\n",
      "ðŸ“ File: knowledge_base_prod/Procurement/Proc - Procurement Standard Regulations - EN.PDF\n",
      "ðŸ“‹ Mapped to: DGE > Procurement > Manual\n",
      "ðŸ”„ Processing: Proc - Procurement Standard Regulations - EN.PDF\n",
      "ðŸŒ Using remote Docling server: http://74.162.37.71:5001/v1alpha/convert/file\n",
      "ðŸ” Processing with Docling: Proc - Procurement Standard Regulations - EN.PDF\n",
      "ðŸ“‹ Available fields in document: ['filename', 'md_content', 'json_content', 'html_content', 'text_content', 'doctags_content']\n",
      "âœ… json_content has content: 13 chars\n",
      "âŒ text_content is None or empty\n",
      "âŒ md_content is None or empty\n",
      "âŒ html_content is None or empty\n",
      "ðŸ” json_content keys: ['schema_name', 'version', 'name', 'origin', 'furniture', 'body', 'groups', 'texts', 'pictures', 'tables', 'key_value_items', 'form_items', 'pages']\n",
      "âœ… Extracted text from json_content.texts: 115704 chars\n",
      "âœ… Successfully deserialized DoclingDocument from remote server\n",
      "ðŸ“‹ Extracted 148 sections from DoclingDocument\n",
      "ðŸ’¾ Cached: Proc - Procurement Standard Regulations - EN.PDF\n",
      "ðŸ§© Processing document to chunks: Abu Dhabi Procurement Standards\n",
      " â€¢ Generated 105 chunks using token-based chunking\n",
      " â€¢ Mapped 105 chunks to 148 sections\n",
      "ðŸš€ Enriching 105 chunks with 8 workers...\n",
      "ðŸ’¾ Cached 105 enriched chunks for Proc - Procurement Standard Regulations - EN\n",
      " â€¢ Enhanced 105 chunks with contextualization\n",
      "ðŸ”„ Generating embeddings for 105 chunks...\n",
      "ðŸ” Processing batch 1/4 (32 texts)\n",
      "âš ï¸  Batch exceeds token limit (10003 > 7992), reducing batch size\n",
      "âœ… Successfully processed 32 texts with batch_size=16\n",
      "ðŸ“Š Generated embeddings for batch 1/4\n",
      "ðŸ” Processing batch 2/4 (32 texts)\n",
      "âš ï¸  Batch exceeds token limit (10068 > 7992), reducing batch size\n",
      "âœ… Successfully processed 32 texts with batch_size=16\n",
      "ðŸ“Š Generated embeddings for batch 2/4\n",
      "ðŸ” Processing batch 3/4 (32 texts)\n",
      "âš ï¸  Batch exceeds token limit (10271 > 7992), reducing batch size\n",
      "âœ… Successfully processed 32 texts with batch_size=16\n",
      "ðŸ“Š Generated embeddings for batch 3/4\n",
      "ðŸ” Processing batch 4/4 (9 texts)\n",
      "ðŸ“Š Generated embeddings for batch 4/4\n",
      "âœ… Generated 105 embeddings with token-aware processing\n",
      " â€¢ Generated 105 embeddings\n",
      " â€¢ Created 105 KBChunkEmbedding objects\n",
      "âœ… Stored 105 Knowledge Base embeddings\n",
      "âœ… Successfully processed Proc - Procurement Standard Regulations - EN\n",
      "ðŸ“Š Stored 105 chunks in Knowledge Base\n",
      "ðŸ“‹ Cached: Proc - Procurement Standard Regulations - EN.PDF\n",
      "   âœ… Success (123.55s)\n",
      "   Progress: 13/14 (92.9%) - ETA: 3.1min\n",
      "\n",
      "ðŸ“„ Processing document 14/14: Proc - Support Frequently Asked Questions - EN\n",
      "   Language: ar+en\n",
      "   Entity: DGE\n",
      "   Domain: Procurement\n",
      "   Category: FAQ\n",
      "\n",
      "ðŸ”„ Processing: Proc - Support Frequently Asked Questions - EN\n",
      "ðŸ“ File: knowledge_base_prod/Procurement/Proc - Support Frequently Asked Questions - EN.docx\n",
      "ðŸ“‹ Mapped to: DGE > Procurement > FAQ\n",
      "ðŸ”„ Processing: Proc - Support Frequently Asked Questions - EN.docx\n",
      "ðŸŒ Using remote Docling server: http://74.162.37.71:5001/v1alpha/convert/file\n",
      "ðŸ” Processing with Docling: Proc - Support Frequently Asked Questions - EN.docx\n",
      "ðŸ“‹ Available fields in document: ['filename', 'md_content', 'json_content', 'html_content', 'text_content', 'doctags_content']\n",
      "âœ… json_content has content: 13 chars\n",
      "âŒ text_content is None or empty\n",
      "âŒ md_content is None or empty\n",
      "âŒ html_content is None or empty\n",
      "ðŸ” json_content keys: ['schema_name', 'version', 'name', 'origin', 'furniture', 'body', 'groups', 'texts', 'pictures', 'tables', 'key_value_items', 'form_items', 'pages']\n",
      "âœ… Extracted text from json_content.texts: 2634 chars\n",
      "âœ… Successfully deserialized DoclingDocument from remote server\n",
      "ðŸ“‹ Extracted 23 sections from DoclingDocument\n",
      "ðŸ’¾ Cached: Proc - Support Frequently Asked Questions - EN.docx\n",
      "ðŸ§© Processing document to chunks: March 2023 â€“ V1.0\n",
      " â€¢ Generated 6 chunks using token-based chunking\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "âš ï¸  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      " â€¢ Mapped 6 chunks to 23 sections\n",
      "ðŸš€ Enriching 6 chunks with 8 workers...\n",
      "ðŸ’¾ Cached 6 enriched chunks for Proc - Support Frequently Asked Questions - EN.docx\n",
      " â€¢ Enhanced 6 chunks with contextualization\n",
      "ðŸ”„ Generating embeddings for 6 chunks...\n",
      "ðŸ” Processing batch 1/1 (6 texts)\n",
      "ðŸ“Š Generated embeddings for batch 1/1\n",
      "âœ… Generated 6 embeddings with token-aware processing\n",
      " â€¢ Generated 6 embeddings\n",
      " â€¢ Created 6 KBChunkEmbedding objects\n",
      "âœ… Stored 6 Knowledge Base embeddings\n",
      "âœ… Successfully processed Proc - Support Frequently Asked Questions - EN\n",
      "ðŸ“Š Stored 6 chunks in Knowledge Base\n",
      "ðŸ“‹ Cached: Proc - Support Frequently Asked Questions - EN.docx\n",
      "   âœ… Success (8.33s)\n",
      "   Progress: 14/14 (100.0%) - ETA: 0.0min\n",
      "\n",
      "ðŸ“Š Processing Summary:\n",
      "   - Total documents: 14\n",
      "   - Processed successfully: 13\n",
      "   - Failed: 1\n",
      "   - Success rate: 92.9%\n",
      "   - Total time: 40.4 minutes\n",
      "   - Average time per document: 173.3 seconds\n",
      "ðŸ”„ Processing Status: âœ… Success\n"
     ]
    }
   ],
   "source": [
    "def process_production_documents():\n",
    "    \"\"\"Process production documents with remote Docling\"\"\"\n",
    "    try:\n",
    "        print(\"ðŸ”„ Processing production documents...\")\n",
    "        \n",
    "        # Initialize database populator\n",
    "        populator = KBDatabasePopulator(version=\"1.2\")\n",
    "        \n",
    "        # Load production mapping\n",
    "        mapping_file = Path.cwd() / \"config\" / \"mapping_v1.2.yaml\"\n",
    "        if not mapping_file.exists():\n",
    "            print(f\"âŒ Production mapping file not found: {mapping_file}\")\n",
    "            return False\n",
    "        \n",
    "        import yaml\n",
    "        with open(mapping_file, 'r') as f:\n",
    "            mapping_data = yaml.safe_load(f)\n",
    "        \n",
    "        # Get available documents\n",
    "        available_docs = [d for d in mapping_data.get('document_mappings', []) if d.get('status') == 'available']\n",
    "        \n",
    "        if not available_docs:\n",
    "            print(\"âš ï¸  No available documents found for processing\")\n",
    "            return False\n",
    "        \n",
    "        print(f\"ðŸ“„ Found {len(available_docs)} available documents to process\")\n",
    "        \n",
    "        # Process each document\n",
    "        processed_count = 0\n",
    "        failed_count = 0\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for i, doc_data in enumerate(available_docs, 1):\n",
    "            print(f\"\\nðŸ“„ Processing document {i}/{len(available_docs)}: {doc_data['excel_title']}\")\n",
    "            print(f\"   Language: {doc_data['language']}\")\n",
    "            print(f\"   Entity: {doc_data['entity']}\")\n",
    "            print(f\"   Domain: {doc_data['domain']}\")\n",
    "            print(f\"   Category: {doc_data['category']}\")\n",
    "            \n",
    "            # Create document entry\n",
    "            from populate_database import DocumentEntry\n",
    "            \n",
    "            doc_entry = DocumentEntry(\n",
    "                excel_title=doc_data['excel_title'],\n",
    "                entity=doc_data['entity'],\n",
    "                domain=doc_data['domain'],\n",
    "                category=doc_data['category'],\n",
    "                language=doc_data['language'],\n",
    "                file_path=doc_data['file_path'],\n",
    "                filename=doc_data['filename'],\n",
    "                file_size=doc_data.get('file_size', 0),\n",
    "                status='available'\n",
    "            )\n",
    "            \n",
    "            # Process document\n",
    "            doc_start_time = time.time()\n",
    "            success = populator.populate_document(doc_entry)\n",
    "            doc_end_time = time.time()\n",
    "            \n",
    "            if success:\n",
    "                processed_count += 1\n",
    "                print(f\"   âœ… Success ({doc_end_time - doc_start_time:.2f}s)\")\n",
    "            else:\n",
    "                failed_count += 1\n",
    "                print(f\"   âŒ Failed ({doc_end_time - doc_start_time:.2f}s)\")\n",
    "            \n",
    "            # Show progress\n",
    "            elapsed = time.time() - start_time\n",
    "            avg_time = elapsed / i\n",
    "            eta = avg_time * (len(available_docs) - i)\n",
    "            \n",
    "            print(f\"   Progress: {i}/{len(available_docs)} ({i/len(available_docs)*100:.1f}%) - ETA: {eta/60:.1f}min\")\n",
    "        \n",
    "        # Final summary\n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"\\nðŸ“Š Processing Summary:\")\n",
    "        print(f\"   - Total documents: {len(available_docs)}\")\n",
    "        print(f\"   - Processed successfully: {processed_count}\")\n",
    "        print(f\"   - Failed: {failed_count}\")\n",
    "        print(f\"   - Success rate: {processed_count/len(available_docs)*100:.1f}%\")\n",
    "        print(f\"   - Total time: {total_time/60:.1f} minutes\")\n",
    "        print(f\"   - Average time per document: {total_time/len(available_docs):.1f} seconds\")\n",
    "        \n",
    "        return processed_count > 0\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error processing production documents: {e}\")\n",
    "        return False\n",
    "\n",
    "# Process production documents\n",
    "processing_success = process_production_documents()\n",
    "print(f\"ðŸ”„ Processing Status: {'âœ… Success' if processing_success else 'âŒ Failed'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Verify Database Population\n",
    "\n",
    "### Check Knowledge Base Table Contents and Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Verifying database population...\n",
      "âœ… Knowledge Base PostgreSQL connection pool initialized (max_size=10)\n",
      "âœ… PGVector extension enabled for Knowledge Base\n",
      "ðŸ”„ Creating Knowledge Base hierarchy tables...\n",
      "ðŸ”§ Granting PUBLIC access to all WoG tables...\n",
      "ðŸŒ± Seeding Knowledge Base with Excel hierarchy data...\n",
      "âœ… Knowledge Base database initialized with hierarchy data\n",
      "ðŸ“Š Database Statistics:\n",
      "   - Total chunks: 1,717\n",
      "   - Unique documents: 15\n",
      "   - Chunks with embeddings: 1,717\n",
      "   - Embedding coverage: 100.0%\n",
      "   - Average chunks per document: 114.5\n",
      "\n",
      "ðŸ›ï¸  Entity Distribution:\n",
      "   - DGE: 1,296 chunks (10 docs)\n",
      "   - Human Resource Authority: 256 chunks (3 docs)\n",
      "   - Department of Finance: 129 chunks (1 docs)\n",
      "   - Unknown: 36 chunks (1 docs)\n",
      "\n",
      "ðŸ”§ Domain Distribution:\n",
      "   - IT: 518 chunks (1 docs)\n",
      "   - Procurement: 453 chunks (6 docs)\n",
      "   - CX: 361 chunks (4 docs)\n",
      "   - HR: 256 chunks (3 docs)\n",
      "   - Finance: 129 chunks (1 docs)\n",
      "\n",
      "ðŸ“‹ Category Distribution:\n",
      "   - Guide: 703 chunks (3 docs)\n",
      "   - Law: 256 chunks (3 docs)\n",
      "   - Manual: 226 chunks (2 docs)\n",
      "   - Framework: 185 chunks (1 docs)\n",
      "   - Templates: 173 chunks (1 docs)\n",
      "   - Policy: 144 chunks (2 docs)\n",
      "   - Charter: 21 chunks (1 docs)\n",
      "   - FAQ: 6 chunks (1 docs)\n",
      "   - Glossary: 3 chunks (1 docs)\n",
      "\n",
      "ðŸ“„ Sample Documents:\n",
      "   - Doc 2 - Information Assurance Standards - EN (518 chunks)\n",
      "     Title: NESA UAE INFORMATION ASSURANCE STANDARDS\n",
      "     Hierarchy: DGE > IT > Guide\n",
      "   - Proc - DGE Procurement Framework - EN.DOC (185 chunks)\n",
      "     Title: DGE Revised Procurement Framework\n",
      "     Hierarchy: DGE > Procurement > Framework\n",
      "   - CX TOV templates (173 chunks)\n",
      "     Title: Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ùˆ Ø§Ù„Ø£Ù…Ø«Ù„Ø© Templates and Examples\n",
      "     Hierarchy: DGE > CX > Templates\n",
      "   - HR - Implementation Regulation for HR Law No 6 Year 2016 - EN (165 chunks)\n",
      "     Title: Decision No . (     10      ) of 2020\n",
      "     Hierarchy: Human Resource Authority > HR > Law\n",
      "   - CX Effortless_Guide_EN_V2 (149 chunks)\n",
      "     Title: Â© Issued by Department of Government Enablement | 2023 First Edition\n",
      "     Hierarchy: DGE > CX > Guide\n",
      "   - Abu Dhabi Government Finance Policy Manual_EN (129 chunks)\n",
      "     Title: Abu Dhabi Government Finance Policy Manual Department of Finance\n",
      "     Hierarchy: Department of Finance > Finance > Policy\n",
      "   - Proc - Procurement Manual (Business Process) - EN (121 chunks)\n",
      "     Title: Procurement Manual (Business Processes) Issued pursuant to the Abu Dhabi Procurement Standards\n",
      "     Hierarchy: DGE > Procurement > Manual\n",
      "   - Proc - Procurement Standard Regulations - EN (105 chunks)\n",
      "     Title: Abu Dhabi Procurement Standards\n",
      "     Hierarchy: DGE > Procurement > Manual\n",
      "   - HR - HR Law - EN (46 chunks)\n",
      "     Title: HR law\n",
      "     Hierarchy: Human Resource Authority > HR > Law\n",
      "   - Doc 3 - Human Resources Law - EN (45 chunks)\n",
      "     Title: HR law\n",
      "     Hierarchy: Human Resource Authority > HR > Law\n",
      "\n",
      "ðŸ” Testing Vector Search Performance...\n",
      "   - Search time: 144.2ms\n",
      "   - Results found: 10\n",
      "   - Performance: âœ… Good\n",
      "ðŸ“Š Verification Status: âœ… Success\n"
     ]
    }
   ],
   "source": [
    "def verify_database_population():\n",
    "    \"\"\"Verify database population and show statistics\"\"\"\n",
    "    try:\n",
    "        print(\"ðŸ“Š Verifying database population...\")\n",
    "        \n",
    "        # Initialize database manager\n",
    "        kb_manager = KnowledgeBaseManager()\n",
    "        \n",
    "        with kb_manager.get_connection() as conn:\n",
    "            with conn.cursor() as cur:\n",
    "                # Get total chunk count\n",
    "                cur.execute(\"SELECT COUNT(*) FROM knowledge_base\")\n",
    "                total_chunks = cur.fetchone()[0]\n",
    "                \n",
    "                # Get unique document count\n",
    "                cur.execute(\"SELECT COUNT(DISTINCT document_name) FROM knowledge_base\")\n",
    "                unique_docs = cur.fetchone()[0]\n",
    "                \n",
    "                # Get embedding count\n",
    "                cur.execute(\"SELECT COUNT(*) FROM knowledge_base WHERE embedding IS NOT NULL\")\n",
    "                embedding_count = cur.fetchone()[0]\n",
    "                \n",
    "                # Get language distribution\n",
    "                cur.execute(\"\"\"\n",
    "                    SELECT \n",
    "                        COALESCE(entity, 'Unknown') as entity,\n",
    "                        COUNT(*) as chunk_count,\n",
    "                        COUNT(DISTINCT document_name) as doc_count\n",
    "                    FROM knowledge_base \n",
    "                    GROUP BY entity \n",
    "                    ORDER BY chunk_count DESC\n",
    "                \"\"\")\n",
    "                entity_stats = cur.fetchall()\n",
    "                \n",
    "                # Get domain distribution\n",
    "                cur.execute(\"\"\"\n",
    "                    SELECT \n",
    "                        COALESCE(domain, 'Unknown') as domain,\n",
    "                        COUNT(*) as chunk_count,\n",
    "                        COUNT(DISTINCT document_name) as doc_count\n",
    "                    FROM knowledge_base \n",
    "                    GROUP BY domain \n",
    "                    ORDER BY chunk_count DESC\n",
    "                \"\"\")\n",
    "                domain_stats = cur.fetchall()\n",
    "                \n",
    "                # Get category distribution\n",
    "                cur.execute(\"\"\"\n",
    "                    SELECT \n",
    "                        COALESCE(category, 'Unknown') as category,\n",
    "                        COUNT(*) as chunk_count,\n",
    "                        COUNT(DISTINCT document_name) as doc_count\n",
    "                    FROM knowledge_base \n",
    "                    GROUP BY category \n",
    "                    ORDER BY chunk_count DESC\n",
    "                \"\"\")\n",
    "                category_stats = cur.fetchall()\n",
    "                \n",
    "                # Get sample documents\n",
    "                cur.execute(\"\"\"\n",
    "                    SELECT \n",
    "                        document_name,\n",
    "                        document_title,\n",
    "                        entity,\n",
    "                        domain,\n",
    "                        category,\n",
    "                        COUNT(*) as chunk_count\n",
    "                    FROM knowledge_base \n",
    "                    GROUP BY document_name, document_title, entity, domain, category\n",
    "                    ORDER BY chunk_count DESC\n",
    "                    LIMIT 10\n",
    "                \"\"\")\n",
    "                sample_docs = cur.fetchall()\n",
    "                \n",
    "                # Show statistics\n",
    "                print(f\"ðŸ“Š Database Statistics:\")\n",
    "                print(f\"   - Total chunks: {total_chunks:,}\")\n",
    "                print(f\"   - Unique documents: {unique_docs}\")\n",
    "                print(f\"   - Chunks with embeddings: {embedding_count:,}\")\n",
    "                print(f\"   - Embedding coverage: {embedding_count/total_chunks*100:.1f}%\")\n",
    "                print(f\"   - Average chunks per document: {total_chunks/unique_docs:.1f}\")\n",
    "                \n",
    "                print(f\"\\nðŸ›ï¸  Entity Distribution:\")\n",
    "                for entity, chunk_count, doc_count in entity_stats:\n",
    "                    print(f\"   - {entity}: {chunk_count:,} chunks ({doc_count} docs)\")\n",
    "                \n",
    "                print(f\"\\nðŸ”§ Domain Distribution:\")\n",
    "                for domain, chunk_count, doc_count in domain_stats:\n",
    "                    print(f\"   - {domain}: {chunk_count:,} chunks ({doc_count} docs)\")\n",
    "                \n",
    "                print(f\"\\nðŸ“‹ Category Distribution:\")\n",
    "                for category, chunk_count, doc_count in category_stats:\n",
    "                    print(f\"   - {category}: {chunk_count:,} chunks ({doc_count} docs)\")\n",
    "                \n",
    "                print(f\"\\nðŸ“„ Sample Documents:\")\n",
    "                for doc_name, doc_title, entity, domain, category, chunk_count in sample_docs:\n",
    "                    print(f\"   - {doc_name} ({chunk_count} chunks)\")\n",
    "                    print(f\"     Title: {doc_title}\")\n",
    "                    print(f\"     Hierarchy: {entity} > {domain} > {category}\")\n",
    "                \n",
    "                # Test vector search performance\n",
    "                print(f\"\\nðŸ” Testing Vector Search Performance...\")\n",
    "                \n",
    "                # Generate a test embedding\n",
    "                import numpy as np\n",
    "                test_embedding = np.random.random(VECTOR_DIMENSION).tolist()\n",
    "                \n",
    "                # Test search speed\n",
    "                search_start = time.time()\n",
    "                results = kb_manager.search_kb_similar_vectors(test_embedding, limit=10)\n",
    "                search_time = time.time() - search_start\n",
    "                \n",
    "                print(f\"   - Search time: {search_time*1000:.1f}ms\")\n",
    "                print(f\"   - Results found: {len(results)}\")\n",
    "                print(f\"   - Performance: {'âœ… Good' if search_time < 1.0 else 'âš ï¸ Slow'}\")\n",
    "                \n",
    "                return True\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error verifying database population: {e}\")\n",
    "        return False\n",
    "\n",
    "# Verify database population\n",
    "verification_success = verify_database_population()\n",
    "print(f\"ðŸ“Š Verification Status: {'âœ… Success' if verification_success else 'âŒ Failed'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Performance Tuning and Optimization\n",
    "\n",
    "### Analyze and Optimize Database Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_tuning():\n",
    "    \"\"\"Analyze and optimize database performance\"\"\"\n",
    "    try:\n",
    "        print(\"âš¡ Analyzing database performance...\")\n",
    "        \n",
    "        kb_manager = KnowledgeBaseManager()\n",
    "        \n",
    "        with kb_manager.get_connection() as conn:\n",
    "            with conn.cursor() as cur:\n",
    "                # Check table sizes\n",
    "                cur.execute(\"\"\"\n",
    "                    SELECT \n",
    "                        schemaname,\n",
    "                        tablename,\n",
    "                        pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as size\n",
    "                    FROM pg_tables \n",
    "                    WHERE schemaname = 'public'\n",
    "                    ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC\n",
    "                \"\"\")\n",
    "                table_sizes = cur.fetchall()\n",
    "                \n",
    "                # Check index usage\n",
    "                cur.execute(\"\"\"\n",
    "                    SELECT \n",
    "                        schemaname,\n",
    "                        tablename,\n",
    "                        indexname,\n",
    "                        idx_scan,\n",
    "                        idx_tup_read,\n",
    "                        idx_tup_fetch\n",
    "                    FROM pg_stat_user_indexes \n",
    "                    ORDER BY idx_scan DESC\n",
    "                \"\"\")\n",
    "                index_stats = cur.fetchall()\n",
    "                \n",
    "                # Analyze query performance\n",
    "                print(f\"ðŸ“Š Performance Analysis:\")\n",
    "                \n",
    "                print(f\"\\nðŸ’¾ Table Sizes:\")\n",
    "                for schema, table, size in table_sizes:\n",
    "                    print(f\"   - {table}: {size}\")\n",
    "                \n",
    "                print(f\"\\nðŸ” Index Usage:\")\n",
    "                for schema, table, index, scans, reads, fetches in index_stats[:10]:\n",
    "                    print(f\"   - {index} ({table}): {scans:,} scans, {reads:,} reads\")\n",
    "                \n",
    "                # Test different search scenarios\n",
    "                print(f\"\\nðŸŽ¯ Search Performance Tests:\")\n",
    "                \n",
    "                # Test 1: Basic vector search\n",
    "                test_embedding = np.random.random(VECTOR_DIMENSION).tolist()\n",
    "                start_time = time.time()\n",
    "                results = kb_manager.search_kb_similar_vectors(test_embedding, limit=20)\n",
    "                basic_search_time = time.time() - start_time\n",
    "                print(f\"   - Basic vector search (20 results): {basic_search_time*1000:.1f}ms\")\n",
    "                \n",
    "                # Test 2: Filtered search by entity\n",
    "                start_time = time.time()\n",
    "                results = kb_manager.search_kb_similar_vectors(\n",
    "                    test_embedding, \n",
    "                    limit=20,\n",
    "                    entity_filter=['DGE']\n",
    "                )\n",
    "                entity_search_time = time.time() - start_time\n",
    "                print(f\"   - Entity filtered search: {entity_search_time*1000:.1f}ms\")\n",
    "                \n",
    "                # Test 3: Multi-filter search\n",
    "                start_time = time.time()\n",
    "                results = kb_manager.search_kb_similar_vectors(\n",
    "                    test_embedding, \n",
    "                    limit=20,\n",
    "                    entity_filter=['DGE'],\n",
    "                    domain_filter=['HR'],\n",
    "                    category_filter=['Guide']\n",
    "                )\n",
    "                multi_filter_time = time.time() - start_time\n",
    "                print(f\"   - Multi-filter search: {multi_filter_time*1000:.1f}ms\")\n",
    "                \n",
    "                # Performance recommendations\n",
    "                print(f\"\\nðŸ’¡ Performance Recommendations:\")\n",
    "                \n",
    "                if basic_search_time > 1.0:\n",
    "                    print(f\"   âš ï¸ Basic search is slow (>{basic_search_time:.1f}s) - consider query optimization\")\n",
    "                else:\n",
    "                    print(f\"   âœ… Basic search performance is good ({basic_search_time*1000:.1f}ms)\")\n",
    "                \n",
    "                if entity_search_time > basic_search_time * 1.5:\n",
    "                    print(f\"   âš ï¸ Filtered search is significantly slower - check indexes\")\n",
    "                else:\n",
    "                    print(f\"   âœ… Filtered search performance is acceptable\")\n",
    "                \n",
    "                # Database maintenance recommendations\n",
    "                print(f\"\\nðŸ”§ Maintenance Recommendations:\")\n",
    "                print(f\"   - Run VACUUM ANALYZE regularly for optimal performance\")\n",
    "                print(f\"   - Monitor index usage and remove unused indexes\")\n",
    "                print(f\"   - Consider connection pooling for production workloads\")\n",
    "                print(f\"   - Set up monitoring for query performance\")\n",
    "                \n",
    "                return True\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error during performance tuning: {e}\")\n",
    "        return False\n",
    "\n",
    "# Performance tuning\n",
    "tuning_success = performance_tuning()\n",
    "print(f\"âš¡ Tuning Status: {'âœ… Success' if tuning_success else 'âŒ Failed'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Final Summary and Next Steps\n",
    "\n",
    "### Pipeline Completion Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_summary():\n",
    "    \"\"\"Generate final pipeline summary\"\"\"\n",
    "    try:\n",
    "        print(\"ðŸ“‹ Production Ingestion Pipeline Summary\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Overall status\n",
    "        steps = [\n",
    "            (\"Database Creation\", db_success),\n",
    "            (\"Schema Creation\", schema_success),\n",
    "            (\"YAML Mapping\", mapping_success),\n",
    "            (\"Hierarchy Population\", hierarchy_success),\n",
    "            (\"Cache Clearing\", cache_success),\n",
    "            (\"Document Processing\", processing_success),\n",
    "            (\"Database Verification\", verification_success),\n",
    "            (\"Performance Tuning\", tuning_success)\n",
    "        ]\n",
    "        \n",
    "        successful_steps = sum(1 for _, success in steps if success)\n",
    "        total_steps = len(steps)\n",
    "        \n",
    "        print(f\"ðŸ“Š Pipeline Status: {successful_steps}/{total_steps} steps completed\")\n",
    "        print(f\"âœ… Success Rate: {successful_steps/total_steps*100:.1f}%\")\n",
    "        \n",
    "        print(f\"\\nðŸ“‹ Step Details:\")\n",
    "        for step_name, success in steps:\n",
    "            status = \"âœ… Success\" if success else \"âŒ Failed\"\n",
    "            print(f\"   - {step_name}: {status}\")\n",
    "        \n",
    "        # Database summary\n",
    "        if verification_success:\n",
    "            kb_manager = KnowledgeBaseManager()\n",
    "            with kb_manager.get_connection() as conn:\n",
    "                with conn.cursor() as cur:\n",
    "                    cur.execute(\"SELECT COUNT(*) FROM knowledge_base\")\n",
    "                    total_chunks = cur.fetchone()[0]\n",
    "                    \n",
    "                    cur.execute(\"SELECT COUNT(DISTINCT document_name) FROM knowledge_base\")\n",
    "                    unique_docs = cur.fetchone()[0]\n",
    "                    \n",
    "                    cur.execute(\"SELECT COUNT(*) FROM knowledge_base WHERE embedding IS NOT NULL\")\n",
    "                    embedding_count = cur.fetchone()[0]\n",
    "                    \n",
    "                    print(f\"\\nðŸ“Š Final Database State:\")\n",
    "                    print(f\"   - Database: {PRODUCTION_DATABASE_NAME}\")\n",
    "                    print(f\"   - Total chunks: {total_chunks:,}\")\n",
    "                    print(f\"   - Unique documents: {unique_docs}\")\n",
    "                    print(f\"   - Embeddings: {embedding_count:,}\")\n",
    "                    print(f\"   - Vector dimension: {VECTOR_DIMENSION}\")\n",
    "                    print(f\"   - Embedding model: {EMBEDDING_MODEL}\")\n",
    "        \n",
    "        # Next steps\n",
    "        print(f\"\\nðŸš€ Next Steps:\")\n",
    "        print(f\"   1. Set up search API endpoints\")\n",
    "        print(f\"   2. Implement BM25 hybrid search\")\n",
    "        print(f\"   3. Create user interface for search\")\n",
    "        print(f\"   4. Set up monitoring and alerts\")\n",
    "        print(f\"   5. Deploy to production environment\")\n",
    "        \n",
    "        # Configuration for next steps\n",
    "        print(f\"\\nðŸ”§ Configuration for Next Steps:\")\n",
    "        print(f\"   - Database URL: postgresql://mustaqmollah@localhost:5432/{PRODUCTION_DATABASE_NAME}\")\n",
    "        print(f\"   - Embedding Model: {EMBEDDING_MODEL}\")\n",
    "        print(f\"   - Vector Dimension: {VECTOR_DIMENSION}\")\n",
    "        print(f\"   - Remote Docling: {os.getenv('DOCLING_SERVER_URL')}\")\n",
    "        print(f\"   - Allowed Languages: {ALLOWED_LANGUAGES}\")\n",
    "        \n",
    "        # Final timestamp\n",
    "        print(f\"\\nâ° Completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        \n",
    "        if successful_steps == total_steps:\n",
    "            print(f\"\\nðŸŽ‰ Production ingestion pipeline completed successfully!\")\n",
    "            print(f\"   The Abu Dhabi Government Knowledge Base is ready for production use.\")\n",
    "        else:\n",
    "            print(f\"\\nâš ï¸  Pipeline completed with {total_steps - successful_steps} failed steps.\")\n",
    "            print(f\"   Please review the failed steps and retry if necessary.\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error generating final summary: {e}\")\n",
    "        return False\n",
    "\n",
    "# Generate final summary\n",
    "final_summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
