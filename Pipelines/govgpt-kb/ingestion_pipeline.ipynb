{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Production Ingestion Pipeline - Abu Dhabi Government Knowledge Base\n",
    "\n",
    "## Overview\n",
    "This notebook serves as the **source of truth** for all production ingestion and tuning operations for the Abu Dhabi Government Knowledge Base (WoG) system.\n",
    "\n",
    "### Key Features\n",
    "- **🗄️ Production Database**: Creates and manages `WoG_Prod` database\n",
    "- **📊 Contextual RAG**: Advanced section-to-chunk mapping with 105 sections per document\n",
    "- **🌐 Remote Docling**: GPU-accelerated document processing with rich section extraction\n",
    "- **🔍 Language Filtering**: Processes only 'en' and 'ar+en' documents\n",
    "- **📋 Hierarchy Management**: Handles null values in government_entity, functional_domain, document_category\n",
    "- **⚡ Performance Monitoring**: Step-by-step metrics and optimization\n",
    "\n",
    "### Architecture\n",
    "- **Database**: PostgreSQL + PGVector with 3072D embeddings\n",
    "- **Processing**: Remote Docling server at `http://74.162.37.71:5001/v1alpha/convert/file`\n",
    "- **Chunking**: 250-token chunks with 30-token overlap using tiktoken\n",
    "- **Embedding**: text-embedding-3-large with token-aware generation\n",
    "- **Search**: Cosine similarity with metadata filtering\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup and Configuration\n",
    "\n",
    "### Initialize Environment Variables and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Production Ingestion Pipeline Initialized\n",
      "📊 Database: WoG_Prod\n",
      "📁 Source Folder: knowledge_base_prod\n",
      "🔤 Allowed Languages: ['en', 'ar+en']\n",
      "🎯 Embedding Model: text-embedding-3-large (3072D)\n",
      "🌐 Remote Docling: http://74.162.37.71:5001/v1alpha/convert/file\n",
      "⏰ Started: 2025-07-17 00:54:14\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import psycopg2\n",
    "from datetime import datetime\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Add src directory to path\n",
    "sys.path.append(str(Path.cwd() / 'src'))\n",
    "\n",
    "# Configuration\n",
    "PRODUCTION_DATABASE_NAME = \"WoG_Prod\"\n",
    "PRODUCTION_KB_FOLDER = \"knowledge_base_prod\"\n",
    "EMBEDDING_MODEL = \"text-embedding-3-large\"\n",
    "VECTOR_DIMENSION = 3072\n",
    "ALLOWED_LANGUAGES = ['en', 'ar+en']\n",
    "\n",
    "print(\"🔧 Production Ingestion Pipeline Initialized\")\n",
    "print(f\"📊 Database: {PRODUCTION_DATABASE_NAME}\")\n",
    "print(f\"📁 Source Folder: {PRODUCTION_KB_FOLDER}\")\n",
    "print(f\"🔤 Allowed Languages: {ALLOWED_LANGUAGES}\")\n",
    "print(f\"🎯 Embedding Model: {EMBEDDING_MODEL} ({VECTOR_DIMENSION}D)\")\n",
    "print(f\"🌐 Remote Docling: {os.getenv('DOCLING_SERVER_URL')}\")\n",
    "print(f\"⏰ Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create Production Database\n",
    "\n",
    "### Create WoG_Prod Database with PGVector Extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Failed to create production database: database \"wog_prod\" already exists\n",
      "\n",
      "📊 Database Status: ❌ Failed\n"
     ]
    }
   ],
   "source": [
    "def create_production_database():\n",
    "    \"\"\"Create WoG_Prod database with PGVector extension\"\"\"\n",
    "    try:\n",
    "        # Connect to default postgres database to create new database\n",
    "        conn = psycopg2.connect(\n",
    "            host=\"localhost\",\n",
    "            database=\"postgres\",\n",
    "            user=os.getenv('DB_USER', 'mustaqmollah'),\n",
    "            password=os.getenv('DB_PASSWORD', '')\n",
    "        )\n",
    "        conn.autocommit = True\n",
    "        \n",
    "        with conn.cursor() as cur:\n",
    "            # Check if database exists\n",
    "            cur.execute(f\"SELECT 1 FROM pg_catalog.pg_database WHERE datname = '{PRODUCTION_DATABASE_NAME}'\")\n",
    "            exists = cur.fetchone()\n",
    "            \n",
    "            if exists:\n",
    "                print(f\"⚠️  Database '{PRODUCTION_DATABASE_NAME}' already exists\")\n",
    "                response = input(\"Do you want to drop and recreate it? (y/N): \")\n",
    "                if response.lower() == 'y':\n",
    "                    cur.execute(f\"DROP DATABASE IF EXISTS {PRODUCTION_DATABASE_NAME}\")\n",
    "                    print(f\"🗑️  Dropped existing database '{PRODUCTION_DATABASE_NAME}'\")\n",
    "                else:\n",
    "                    print(f\"✅ Using existing database '{PRODUCTION_DATABASE_NAME}'\")\n",
    "                    conn.close()\n",
    "                    return True\n",
    "            \n",
    "            # Create new database\n",
    "            cur.execute(f\"CREATE DATABASE {PRODUCTION_DATABASE_NAME}\")\n",
    "            print(f\"🎉 Created database '{PRODUCTION_DATABASE_NAME}'\")\n",
    "        \n",
    "        conn.close()\n",
    "        \n",
    "        # Connect to new database and enable PGVector\n",
    "        prod_conn = psycopg2.connect(\n",
    "            host=\"localhost\",\n",
    "            database=PRODUCTION_DATABASE_NAME,\n",
    "            user=os.getenv('DB_USER', 'mustaqmollah'),\n",
    "            password=os.getenv('DB_PASSWORD', '')\n",
    "        )\n",
    "        prod_conn.autocommit = True\n",
    "        \n",
    "        with prod_conn.cursor() as cur:\n",
    "            # Enable PGVector extension\n",
    "            cur.execute(\"CREATE EXTENSION IF NOT EXISTS vector\")\n",
    "            print(\"✅ PGVector extension enabled\")\n",
    "            \n",
    "            # Verify vector support\n",
    "            cur.execute(\"SELECT 1\")\n",
    "            print(f\"✅ Database connection verified: {PRODUCTION_DATABASE_NAME}\")\n",
    "        \n",
    "        prod_conn.close()\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to create production database: {e}\")\n",
    "        return False\n",
    "\n",
    "# Create production database\n",
    "db_success = create_production_database()\n",
    "print(f\"📊 Database Status: {'✅ Success' if db_success else '❌ Failed'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create Database Schema\n",
    "\n",
    "### Create Knowledge Base Tables with Hierarchy Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Knowledge Base PostgreSQL connection pool initialized (max_size=10)\n",
      "✅ PGVector extension enabled for Knowledge Base\n",
      "🔄 Creating Knowledge Base hierarchy tables...\n",
      "🔧 Granting PUBLIC access to all WoG tables...\n",
      "🌱 Seeding Knowledge Base with Excel hierarchy data...\n",
      "✅ Knowledge Base database initialized with hierarchy data\n",
      "✅ Knowledge Base PostgreSQL connection pool initialized (max_size=10)\n",
      "✅ PGVector extension enabled for Knowledge Base\n",
      "🔄 Creating Knowledge Base hierarchy tables...\n",
      "🔧 Granting PUBLIC access to all WoG tables...\n",
      "🌱 Seeding Knowledge Base with Excel hierarchy data...\n",
      "✅ Knowledge Base database initialized with hierarchy data\n",
      "🏗️  Creating database schema...\n",
      "✅ Created table: entities\n",
      "✅ Created table: functional_domains\n",
      "✅ Created table: document_categories\n",
      "✅ Created table: knowledge_base\n",
      "🔍 Creating indexes...\n",
      "✅ Created indexes\n",
      "🔐 Granting permissions...\n",
      "✅ Permissions granted\n",
      "🏗️  Schema Status: ✅ Success\n"
     ]
    }
   ],
   "source": [
    "# Update environment to use production database\n",
    "os.environ['KB_DATABASE_URL'] = f\"postgresql://mustaqmollah@localhost:5432/{PRODUCTION_DATABASE_NAME}\"\n",
    "\n",
    "# Import database models and manager\n",
    "from models import (\n",
    "    get_create_entities_table_sql,\n",
    "    get_create_domains_table_sql,\n",
    "    get_create_categories_table_sql,\n",
    "    get_create_knowledge_base_table_sql,\n",
    "    get_create_kb_indexes_sql\n",
    ")\n",
    "from database import KnowledgeBaseManager\n",
    "\n",
    "def create_database_schema():\n",
    "    \"\"\"Create all database tables and indexes\"\"\"\n",
    "    try:\n",
    "        # Initialize database manager\n",
    "        kb_manager = KnowledgeBaseManager()\n",
    "        \n",
    "        print(\"🏗️  Creating database schema...\")\n",
    "        \n",
    "        # Create tables\n",
    "        tables = [\n",
    "            (\"entities\", get_create_entities_table_sql()),\n",
    "            (\"functional_domains\", get_create_domains_table_sql()),\n",
    "            (\"document_categories\", get_create_categories_table_sql()),\n",
    "            (\"knowledge_base\", get_create_knowledge_base_table_sql(EMBEDDING_MODEL))\n",
    "        ]\n",
    "        \n",
    "        for table_name, sql in tables:\n",
    "            try:\n",
    "                with kb_manager.get_connection() as conn:\n",
    "                    with conn.cursor() as cur:\n",
    "                        cur.execute(sql)\n",
    "                        conn.commit()\n",
    "                print(f\"✅ Created table: {table_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Failed to create table {table_name}: {e}\")\n",
    "                return False\n",
    "        \n",
    "        # Create indexes\n",
    "        print(\"🔍 Creating indexes...\")\n",
    "        try:\n",
    "            with kb_manager.get_connection() as conn:\n",
    "                with conn.cursor() as cur:\n",
    "                    cur.execute(get_create_kb_indexes_sql(EMBEDDING_MODEL))\n",
    "                    conn.commit()\n",
    "            print(\"✅ Created indexes\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to create indexes: {e}\")\n",
    "            return False\n",
    "        \n",
    "        # Grant permissions\n",
    "        print(\"🔐 Granting permissions...\")\n",
    "        try:\n",
    "            with kb_manager.get_connection() as conn:\n",
    "                with conn.cursor() as cur:\n",
    "                    tables = ['entities', 'functional_domains', 'document_categories', 'knowledge_base']\n",
    "                    for table in tables:\n",
    "                        cur.execute(f'GRANT ALL PRIVILEGES ON TABLE {table} TO PUBLIC')\n",
    "                    conn.commit()\n",
    "            print(\"✅ Permissions granted\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to grant permissions: {e}\")\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to create database schema: {e}\")\n",
    "        return False\n",
    "\n",
    "# Create schema\n",
    "schema_success = create_database_schema()\n",
    "print(f\"🏗️  Schema Status: {'✅ Success' if schema_success else '❌ Failed'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Generate Production YAML Mapping\n",
    "\n",
    "### Process Excel Hierarchy Data with Language Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📋 Generating production YAML mapping...\n",
      "🔧 KB Mapping Generator initialized\n",
      "📁 Knowledge Base dir: /Users/mustaqmollah/Desktop/GovGPT/ContextualRag/govgpt-kb/knowledge_base_prod\n",
      "📝 Version: v1.2\n",
      "📊 Using folder: knowledge_base_prod\n",
      "📁 Production folder: /Users/mustaqmollah/Desktop/GovGPT/ContextualRag/govgpt-kb/knowledge_base_prod\n",
      "📊 Excel file: knowledge_base_hierarchy_data.xlsx\n",
      "📝 Version: v1.2\n",
      "🔄 Generating YAML mapping v1.2...\n",
      "📖 Reading Excel file: knowledge_base_hierarchy_data.xlsx\n",
      "✅ Read 24 rows from Excel file\n",
      "⏭️  Skipping document 'CX Effortless_Guide_AR' - language 'ar' not in allowed list ['en', 'ar+en']\n",
      "⏭️  Skipping document 'Abu Dhabi Government Finance Policy Manual_AR' - language 'ar' not in allowed list ['en', 'ar+en']\n",
      "⏭️  Skipping document 'HR - Government Employee Guide Version 1 - AR' - language 'ar' not in allowed list ['en', 'ar+en']\n",
      "⏭️  Skipping document 'HR - Government Employee Guide Version 2 - AR' - language 'ar' not in allowed list ['en', 'ar+en']\n",
      "⏭️  Skipping document 'HR - HR Law- AR' - language 'ar' not in allowed list ['en', 'ar+en']\n",
      "⏭️  Skipping document 'HR - Implementation Regulation for HR Law No 6 Year 2016 - AR' - language 'ar' not in allowed list ['en', 'ar+en']\n",
      "⏭️  Skipping document 'Proc - Policies and Procedures for Sales Auctions and Warehouses - AR' - language 'ar' not in allowed list ['en', 'ar+en']\n",
      "⏭️  Skipping document 'Proc - Procurement Charter - AR' - language 'ar' not in allowed list ['en', 'ar+en']\n",
      "⏭️  Skipping document 'Proc - Procurement Manual (Business Process) - AR' - language 'ar' not in allowed list ['en', 'ar+en']\n",
      "⏭️  Skipping document 'Proc - Procurement Standard Regulations - AR' - language 'ar' not in allowed list ['en', 'ar+en']\n",
      "📋 Processed 14 documents (10 skipped due to language filtering)\n",
      "🔍 Scanning knowledge base directory: /Users/mustaqmollah/Desktop/GovGPT/ContextualRag/govgpt-kb/knowledge_base_prod\n",
      "📁 Found 24 files in knowledge base\n",
      "✅ Matched: 'CX Abu Dhabi Government Tone of Voice Document' → CX Abu Dhabi Government Tone of Voice Document.pdf\n",
      "✅ Matched: 'CX Effortless_Guide_EN_V2' → CX Effortless_Guide_EN_V2.pdf\n",
      "✅ Matched: 'CX Glossary' → CX Glossary.pdf\n",
      "✅ Matched: 'CX TOV templates' → CX TOV templates.pdf\n",
      "✅ Matched: 'Abu Dhabi Government Finance Policy Manual_EN' → Abu Dhabi Government Finance Policy Manual_EN.pdf\n",
      "✅ Matched: 'HR - HR Law - EN' → HR - HR Law - EN.pdf\n",
      "✅ Matched: 'HR - Implementation Regulation for HR Law No 6 Year 2016 - EN' → HR - Implementation Regulation for HR Law No 6 Year 2016 - EN.pdf\n",
      "✅ Matched: 'Proc - DGE Procurement Framework - EN' → Proc - DGE Procurement Framework - EN.DOC\n",
      "✅ Matched: 'Proc - Policies and Procedures for Sales Auctions and Warehouses - EN' → Proc - Policies and Procedures for Sales Auctions and Warehouses - EN.PDF\n",
      "✅ Matched: 'Proc - Procurement Charter - EN' → Proc - Procurement Charter - EN.pdf\n",
      "✅ Matched: 'Proc - Procurement Manual (Ariba Aligned) - EN' → Proc - Procurement Manual (Ariba Aligned) - EN.pdf\n",
      "✅ Matched: 'Proc - Procurement Manual (Business Process) - EN' → Proc - Procurement Manual (Business Process) - EN.pdf\n",
      "✅ Matched: 'Proc - Procurement Standard Regulations - EN' → Proc - Procurement Standard Regulations - EN.PDF\n",
      "✅ Matched: 'Proc - Support Frequently Asked Questions - EN' → Proc - Support Frequently Asked Questions - EN.docx\n",
      "📋 Unmatched files (10):\n",
      "   - Proc - Procurement Standard Regulations - AR.pdf\n",
      "   - Proc - Procurement Manual (Business Process) - AR.pdf\n",
      "   - Proc - Policies and Procedures for Sales Auctions and Warehouses - AR.pdf\n",
      "   - Proc - Procurement Charter - AR.pdf\n",
      "   - HR - Implementation Regulation for HR Law No 6 Year 2016 - AR.pdf\n",
      "   - HR - Government Employee Guide Version 2 - AR.pdf\n",
      "   - HR - Government Employee Guide Version 1 - AR.pdf\n",
      "   - HR - HR Law- AR.pdf\n",
      "   - Abu Dhabi Government Finance Policy Manual_AR.docx\n",
      "   - CX Effortless_Guide_AR.pdf\n",
      "\n",
      "🔗 Mapped 14 available files\n",
      "❓ 0 documents marked as missing\n",
      "✅ Generated YAML mapping: /Users/mustaqmollah/Desktop/GovGPT/ContextualRag/govgpt-kb/config/mapping_v1.2.yaml\n",
      "📊 Summary:\n",
      "   - Total documents: 14\n",
      "   - Available: 14\n",
      "   - Missing: 0\n",
      "   - Entities: 4\n",
      "   - Domains: 4\n",
      "   - Categories: 9\n",
      "✅ Production YAML mapping generated successfully\n",
      "📊 Mapping Statistics:\n",
      "   - Total documents: 14\n",
      "   - Available documents: 14\n",
      "   - Missing documents: 0\n",
      "   - Entities: 4\n",
      "   - Domains: 4\n",
      "   - Categories: 9\n",
      "   - Languages: {'en': 11, 'ar+en': 3}\n",
      "\n",
      "📄 Available Documents (14):\n",
      "   - CX Abu Dhabi Government Tone of Voice Document (en)\n",
      "   - CX Effortless_Guide_EN_V2 (en)\n",
      "   - CX Glossary (ar+en)\n",
      "   - CX TOV templates (ar+en)\n",
      "   - Abu Dhabi Government Finance Policy Manual_EN (en)\n",
      "   - HR - HR Law - EN (en)\n",
      "   - HR - Implementation Regulation for HR Law No 6 Year 2016 - EN (en)\n",
      "   - Proc - DGE Procurement Framework - EN (en)\n",
      "   - Proc - Policies and Procedures for Sales Auctions and Warehouses - EN (en)\n",
      "   - Proc - Procurement Charter - EN (en)\n",
      "   ... and 4 more\n",
      "\n",
      "❓ Missing Documents (0):\n",
      "📋 Mapping Status: ✅ Success\n"
     ]
    }
   ],
   "source": [
    "# Import mapping generator\n",
    "sys.path.append(str(Path.cwd() / 'scripts'))\n",
    "from generate_mapping import KBMappingGenerator\n",
    "\n",
    "def generate_production_mapping():\n",
    "    \"\"\"Generate YAML mapping for production data\"\"\"\n",
    "    try:\n",
    "        print(\"📋 Generating production YAML mapping...\")\n",
    "        \n",
    "        # Initialize mapping generator with production folder\n",
    "        excel_file = \"knowledge_base_hierarchy_data.xlsx\"\n",
    "        version = \"1.2\"\n",
    "        \n",
    "        generator = KBMappingGenerator(\n",
    "            excel_file=excel_file, \n",
    "            version=version,\n",
    "            knowledge_base_folder=PRODUCTION_KB_FOLDER\n",
    "        )\n",
    "        \n",
    "        print(f\"📁 Production folder: {generator.knowledge_base_dir}\")\n",
    "        print(f\"📊 Excel file: {excel_file}\")\n",
    "        print(f\"📝 Version: v{version}\")\n",
    "        \n",
    "        # Check if production folder exists\n",
    "        if not generator.knowledge_base_dir.exists():\n",
    "            print(f\"❌ Production folder does not exist: {generator.knowledge_base_dir}\")\n",
    "            print(\"Please create the knowledge_base_prod folder and add your production documents\")\n",
    "            return False\n",
    "        \n",
    "        # Generate mapping\n",
    "        success = generator.generate_yaml_mapping()\n",
    "        \n",
    "        if success:\n",
    "            print(\"✅ Production YAML mapping generated successfully\")\n",
    "            \n",
    "            # Show mapping statistics\n",
    "            mapping_file = generator.config_dir / f\"mapping_v{version}.yaml\"\n",
    "            if mapping_file.exists():\n",
    "                import yaml\n",
    "                with open(mapping_file, 'r') as f:\n",
    "                    mapping_data = yaml.safe_load(f)\n",
    "                \n",
    "                print(f\"📊 Mapping Statistics:\")\n",
    "                print(f\"   - Total documents: {mapping_data.get('total_documents', 0)}\")\n",
    "                print(f\"   - Available documents: {mapping_data.get('available_documents', 0)}\")\n",
    "                print(f\"   - Missing documents: {mapping_data.get('missing_documents', 0)}\")\n",
    "                print(f\"   - Entities: {len(mapping_data.get('entities', []))}\")\n",
    "                print(f\"   - Domains: {len(mapping_data.get('functional_domains', []))}\")\n",
    "                print(f\"   - Categories: {len(mapping_data.get('document_categories', []))}\")\n",
    "                \n",
    "                # Show language distribution\n",
    "                languages = {}\n",
    "                for doc in mapping_data.get('document_mappings', []):\n",
    "                    lang = doc.get('language', 'unknown')\n",
    "                    languages[lang] = languages.get(lang, 0) + 1\n",
    "                \n",
    "                print(f\"   - Languages: {languages}\")\n",
    "                \n",
    "                # Show available vs missing breakdown\n",
    "                available_docs = [d for d in mapping_data.get('document_mappings', []) if d.get('status') == 'available']\n",
    "                missing_docs = [d for d in mapping_data.get('document_mappings', []) if d.get('status') == 'missing']\n",
    "                \n",
    "                print(f\"\\n📄 Available Documents ({len(available_docs)}):\")\n",
    "                for doc in available_docs[:10]:  # Show first 10\n",
    "                    print(f\"   - {doc['excel_title']} ({doc['language']})\")\n",
    "                if len(available_docs) > 10:\n",
    "                    print(f\"   ... and {len(available_docs) - 10} more\")\n",
    "                \n",
    "                print(f\"\\n❓ Missing Documents ({len(missing_docs)}):\")\n",
    "                for doc in missing_docs[:5]:  # Show first 5\n",
    "                    print(f\"   - {doc['excel_title']} ({doc['language']})\")\n",
    "                if len(missing_docs) > 5:\n",
    "                    print(f\"   ... and {len(missing_docs) - 5} more\")\n",
    "            \n",
    "            return True\n",
    "        else:\n",
    "            print(\"❌ Failed to generate production YAML mapping\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error generating production mapping: {e}\")\n",
    "        return False\n",
    "\n",
    "# Generate production mapping\n",
    "mapping_success = generate_production_mapping()\n",
    "print(f\"📋 Mapping Status: {'✅ Success' if mapping_success else '❌ Failed'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Populate Hierarchy Tables\n",
    "\n",
    "### Load Entities, Domains, and Categories from YAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Knowledge Base PostgreSQL connection pool initialized (max_size=10)\n",
      "✅ PGVector extension enabled for Knowledge Base\n",
      "🔄 Creating Knowledge Base hierarchy tables...\n",
      "🔧 Granting PUBLIC access to all WoG tables...\n",
      "🌱 Seeding Knowledge Base with Excel hierarchy data...\n",
      "✅ Knowledge Base database initialized with hierarchy data\n",
      "📊 Populating hierarchy tables...\n",
      "🔧 DocumentProcessor initialized\n",
      "📁 Cache directory: cache\n",
      "🚀 Processing method: Docling\n",
      "🌐 Docling server URL: 'http://74.162.37.71:5001/v1alpha/convert/file'\n",
      "💾 Cache enabled: True\n",
      "🗂️  Use cache: True\n",
      "🔧 DocumentProcessor initialized\n",
      "📁 Cache directory: cache\n",
      "🚀 Processing method: Docling\n",
      "🌐 Docling server URL: 'http://74.162.37.71:5001/v1alpha/convert/file'\n",
      "💾 Cache enabled: True\n",
      "🗂️  Use cache: True\n",
      "🧩 ChunkProcessor initialized\n",
      "🧠 Embedding model: text-embedding-3-large\n",
      "📐 Chunk size: 250, overlap: 30\n",
      "⚡ Max workers: 8\n",
      "💾 Cache directory: ./cache\n",
      "📊 ProcessingLogger initialized for version 1.2\n",
      "📁 Log file: logs/processing_v1.2.log\n",
      "🔧 Knowledge Base populator initialized\n",
      "📁 Mapping file: /Users/mustaqmollah/Desktop/GovGPT/ContextualRag/govgpt-kb/config/mapping_v1.2.yaml\n",
      "📝 Version: v1.2\n",
      "🧩 Using ChunkProcessor for chunking and embeddings\n",
      "🔄 Populating hierarchy tables...\n",
      "✅ Entity: Unknown\n",
      "⚠️  Entity exists: DGE\n",
      "⚠️  Entity exists: Department of Finance\n",
      "⚠️  Entity exists: Human Resource Authority\n",
      "⚠️  Domain exists: CX\n",
      "⚠️  Domain exists: Finance\n",
      "⚠️  Domain exists: HR\n",
      "⚠️  Domain exists: Procurement\n",
      "⚠️  Category exists: Guide\n",
      "⚠️  Category exists: Glossary\n",
      "⚠️  Category exists: Templates\n",
      "⚠️  Category exists: Policy\n",
      "⚠️  Category exists: Law\n",
      "⚠️  Category exists: Framework\n",
      "⚠️  Category exists: Charter\n",
      "⚠️  Category exists: Manual\n",
      "⚠️  Category exists: FAQ\n",
      "✅ Hierarchy tables populated successfully\n",
      "✅ Hierarchy tables populated successfully\n",
      "📊 Database Contents:\n",
      "   - Entities: 5\n",
      "   - Domains: 5\n",
      "   - Categories: 9\n",
      "\n",
      "🏛️  Sample Entities:\n",
      "   - General (GEN) - General Government\n",
      "   - DGE (DGE) - Department of Government Enablement\n",
      "   - Department of Finance (DOF) - Department of Finance\n",
      "   - Human Resource Authority (HRA) - Human Resource Authority\n",
      "   - Unknown (U) - Unknown\n",
      "\n",
      "🔧 Sample Domains:\n",
      "   - CX - Customer Experience\n",
      "   - Finance - Finance\n",
      "   - HR - Human Resources\n",
      "   - Procurement - Procurement\n",
      "   - IT - Information Technology\n",
      "\n",
      "📋 Sample Categories:\n",
      "   - Guide - Instructional guides and manuals\n",
      "   - Glossary - Glossaries and definitions\n",
      "   - Templates - Document templates and forms\n",
      "   - Policy - Government policies and guidelines\n",
      "   - Law - Legal documents and regulations\n",
      "📊 Hierarchy Status: ✅ Success\n"
     ]
    }
   ],
   "source": [
    "# Import database populator\n",
    "from populate_database import KBDatabasePopulator\n",
    "\n",
    "def populate_hierarchy_tables():\n",
    "    \"\"\"Populate hierarchy tables from YAML mapping\"\"\"\n",
    "    try:\n",
    "        print(\"📊 Populating hierarchy tables...\")\n",
    "        \n",
    "        # Initialize database populator\n",
    "        populator = KBDatabasePopulator(version=\"1.2\")\n",
    "        \n",
    "        # Load production mapping\n",
    "        mapping_file = Path.cwd() / \"config\" / \"mapping_v1.2.yaml\"\n",
    "        if not mapping_file.exists():\n",
    "            print(f\"❌ Production mapping file not found: {mapping_file}\")\n",
    "            return False\n",
    "        \n",
    "        import yaml\n",
    "        with open(mapping_file, 'r') as f:\n",
    "            mapping_data = yaml.safe_load(f)\n",
    "        \n",
    "        # Populate hierarchy tables\n",
    "        success = populator.populate_hierarchy_tables(mapping_data)\n",
    "        \n",
    "        if success:\n",
    "            print(\"✅ Hierarchy tables populated successfully\")\n",
    "            \n",
    "            # Verify table contents\n",
    "            from database import get_kb_manager\n",
    "            kb_manager = get_kb_manager()\n",
    "            \n",
    "            try:\n",
    "                with kb_manager.get_connection() as conn:\n",
    "                    with conn.cursor() as cur:\n",
    "                        # Count entities\n",
    "                        cur.execute(\"SELECT COUNT(*) FROM entities\")\n",
    "                        entity_count = cur.fetchone()[0]\n",
    "                        \n",
    "                        # Count domains\n",
    "                        cur.execute(\"SELECT COUNT(*) FROM functional_domains\")\n",
    "                        domain_count = cur.fetchone()[0]\n",
    "                        \n",
    "                        # Count categories\n",
    "                        cur.execute(\"SELECT COUNT(*) FROM document_categories\")\n",
    "                        category_count = cur.fetchone()[0]\n",
    "                        \n",
    "                        print(f\"📊 Database Contents:\")\n",
    "                        print(f\"   - Entities: {entity_count}\")\n",
    "                        print(f\"   - Domains: {domain_count}\")\n",
    "                        print(f\"   - Categories: {category_count}\")\n",
    "                        \n",
    "                        # Show sample entities\n",
    "                        cur.execute(\"SELECT name, display_name, code FROM entities LIMIT 5\")\n",
    "                        entities = cur.fetchall()\n",
    "                        print(f\"\\n🏛️  Sample Entities:\")\n",
    "                        for entity in entities:\n",
    "                            print(f\"   - {entity[0]} ({entity[2]}) - {entity[1]}\")\n",
    "                        \n",
    "                        # Show sample domains\n",
    "                        cur.execute(\"SELECT name, display_name FROM functional_domains LIMIT 5\")\n",
    "                        domains = cur.fetchall()\n",
    "                        print(f\"\\n🔧 Sample Domains:\")\n",
    "                        for domain in domains:\n",
    "                            print(f\"   - {domain[0]} - {domain[1]}\")\n",
    "                        \n",
    "                        # Show sample categories\n",
    "                        cur.execute(\"SELECT name, description FROM document_categories LIMIT 5\")\n",
    "                        categories = cur.fetchall()\n",
    "                        print(f\"\\n📋 Sample Categories:\")\n",
    "                        for category in categories:\n",
    "                            print(f\"   - {category[0]} - {category[1]}\")\n",
    "                            \n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error verifying hierarchy tables: {e}\")\n",
    "                return False\n",
    "            \n",
    "            return True\n",
    "        else:\n",
    "            print(\"❌ Failed to populate hierarchy tables\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error populating hierarchy tables: {e}\")\n",
    "        return False\n",
    "\n",
    "# Populate hierarchy tables\n",
    "hierarchy_success = populate_hierarchy_tables()\n",
    "print(f\"📊 Hierarchy Status: {'✅ Success' if hierarchy_success else '❌ Failed'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Clear Cache for Fresh Processing\n",
    "\n",
    "### Clear Document Processing Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🗑️  Clearing 0 cached files...\n",
      "✅ Cache cleared: 0 files removed\n",
      "🗑️  Cache Status: ✅ Cleared\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "def clear_processing_cache():\n",
    "    \"\"\"Clear document processing cache for fresh processing\"\"\"\n",
    "    try:\n",
    "        cache_dir = Path.cwd() / \"cache\"\n",
    "        \n",
    "        if cache_dir.exists():\n",
    "            # Count files before clearing\n",
    "            cache_files = list(cache_dir.glob(\"*.json\"))\n",
    "            file_count = len(cache_files)\n",
    "            \n",
    "            print(f\"🗑️  Clearing {file_count} cached files...\")\n",
    "            \n",
    "            # Remove all cache files\n",
    "            for cache_file in cache_files:\n",
    "                cache_file.unlink()\n",
    "                \n",
    "            print(f\"✅ Cache cleared: {file_count} files removed\")\n",
    "        else:\n",
    "            print(\"📁 Cache directory does not exist - creating it\")\n",
    "            cache_dir.mkdir(exist_ok=True)\n",
    "            \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error clearing cache: {e}\")\n",
    "        return False\n",
    "\n",
    "# Clear cache\n",
    "cache_success = clear_processing_cache()\n",
    "print(f\"🗑️  Cache Status: {'✅ Cleared' if cache_success else '❌ Failed'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Process Production Documents\n",
    "\n",
    "### Process Documents with Remote Docling and Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Processing production documents...\n",
      "🔧 DocumentProcessor initialized\n",
      "📁 Cache directory: cache\n",
      "🚀 Processing method: Docling\n",
      "🌐 Docling server URL: 'http://74.162.37.71:5001/v1alpha/convert/file'\n",
      "💾 Cache enabled: True\n",
      "🗂️  Use cache: True\n",
      "🔧 DocumentProcessor initialized\n",
      "📁 Cache directory: cache\n",
      "🚀 Processing method: Docling\n",
      "🌐 Docling server URL: 'http://74.162.37.71:5001/v1alpha/convert/file'\n",
      "💾 Cache enabled: True\n",
      "🗂️  Use cache: True\n",
      "🧩 ChunkProcessor initialized\n",
      "🧠 Embedding model: text-embedding-3-large\n",
      "📐 Chunk size: 250, overlap: 30\n",
      "⚡ Max workers: 8\n",
      "💾 Cache directory: ./cache\n",
      "📊 ProcessingLogger initialized for version 1.2\n",
      "📁 Log file: logs/processing_v1.2.log\n",
      "🔧 Knowledge Base populator initialized\n",
      "📁 Mapping file: /Users/mustaqmollah/Desktop/GovGPT/ContextualRag/govgpt-kb/config/mapping_v1.2.yaml\n",
      "📝 Version: v1.2\n",
      "🧩 Using ChunkProcessor for chunking and embeddings\n",
      "📄 Found 14 available documents to process\n",
      "\n",
      "📄 Processing document 1/14: CX Abu Dhabi Government Tone of Voice Document\n",
      "   Language: en\n",
      "   Entity: Unknown\n",
      "   Domain: CX\n",
      "   Category: Guide\n",
      "\n",
      "🔄 Processing: CX Abu Dhabi Government Tone of Voice Document\n",
      "📁 File: knowledge_base_prod/CX/CX Abu Dhabi Government Tone of Voice Document.pdf\n",
      "📋 Mapped to: Unknown > CX > Guide\n",
      "🔄 Processing: CX Abu Dhabi Government Tone of Voice Document.pdf\n",
      "🌐 Using remote Docling server: http://74.162.37.71:5001/v1alpha/convert/file\n",
      "🔍 Processing with Docling: CX Abu Dhabi Government Tone of Voice Document.pdf\n",
      "📋 Available fields in document: ['filename', 'md_content', 'json_content', 'html_content', 'text_content', 'doctags_content']\n",
      "✅ json_content has content: 13 chars\n",
      "❌ text_content is None or empty\n",
      "❌ md_content is None or empty\n",
      "❌ html_content is None or empty\n",
      "🔍 json_content keys: ['schema_name', 'version', 'name', 'origin', 'furniture', 'body', 'groups', 'texts', 'pictures', 'tables', 'key_value_items', 'form_items', 'pages']\n",
      "✅ Extracted text from json_content.texts: 38826 chars\n",
      "✅ Successfully deserialized DoclingDocument from remote server\n",
      "📋 Extracted 121 sections from DoclingDocument\n",
      "💾 Cached: CX Abu Dhabi Government Tone of Voice Document.pdf\n",
      "🧩 Processing document to chunks: Abu Dhabi Government Tone of Voice\n",
      " • Generated 36 chunks using token-based chunking\n",
      " • Mapped 36 chunks to 121 sections\n",
      "🚀 Enriching 36 chunks with 8 workers...\n",
      "💾 Cached 36 enriched chunks for CX Abu Dhabi Government Tone of Voice Document\n",
      " • Enhanced 36 chunks with contextualization\n",
      "🔄 Generating embeddings for 36 chunks...\n",
      "🔍 Processing batch 1/2 (32 texts)\n",
      "⚠️  Batch exceeds token limit (9700 > 7992), reducing batch size\n",
      "✅ Successfully processed 32 texts with batch_size=16\n",
      "📊 Generated embeddings for batch 1/2\n",
      "🔍 Processing batch 2/2 (4 texts)\n",
      "📊 Generated embeddings for batch 2/2\n",
      "✅ Generated 36 embeddings with token-aware processing\n",
      " • Generated 36 embeddings\n",
      " • Created 36 KBChunkEmbedding objects\n",
      "✅ Stored 36 Knowledge Base embeddings\n",
      "✅ Successfully processed CX Abu Dhabi Government Tone of Voice Document\n",
      "📊 Stored 36 chunks in Knowledge Base\n",
      "📋 Cached: CX Abu Dhabi Government Tone of Voice Document.pdf\n",
      "   ✅ Success (48.51s)\n",
      "   Progress: 1/14 (7.1%) - ETA: 10.5min\n",
      "\n",
      "📄 Processing document 2/14: CX Effortless_Guide_EN_V2\n",
      "   Language: en\n",
      "   Entity: DGE\n",
      "   Domain: CX\n",
      "   Category: Guide\n",
      "\n",
      "🔄 Processing: CX Effortless_Guide_EN_V2\n",
      "📁 File: knowledge_base_prod/CX/CX Effortless_Guide_EN_V2.pdf\n",
      "📋 Mapped to: DGE > CX > Guide\n",
      "🔄 Processing: CX Effortless_Guide_EN_V2.pdf\n",
      "🌐 Using remote Docling server: http://74.162.37.71:5001/v1alpha/convert/file\n",
      "🔍 Processing with Docling: CX Effortless_Guide_EN_V2.pdf\n",
      "📋 Available fields in document: ['filename', 'md_content', 'json_content', 'html_content', 'text_content', 'doctags_content']\n",
      "✅ json_content has content: 13 chars\n",
      "❌ text_content is None or empty\n",
      "❌ md_content is None or empty\n",
      "❌ html_content is None or empty\n",
      "🔍 json_content keys: ['schema_name', 'version', 'name', 'origin', 'furniture', 'body', 'groups', 'texts', 'pictures', 'tables', 'key_value_items', 'form_items', 'pages']\n",
      "✅ Extracted text from json_content.texts: 172497 chars\n",
      "✅ Successfully deserialized DoclingDocument from remote server\n",
      "📋 Extracted 308 sections from DoclingDocument\n",
      "💾 Cached: CX Effortless_Guide_EN_V2.pdf\n",
      "🧩 Processing document to chunks: © Issued by Department of Government Enablement | 2023 First Edition\n",
      " • Generated 149 chunks using token-based chunking\n",
      " • Mapped 149 chunks to 308 sections\n",
      "🚀 Enriching 149 chunks with 8 workers...\n",
      "💾 Cached 149 enriched chunks for CX Effortless_Guide_EN_V2\n",
      " • Enhanced 149 chunks with contextualization\n",
      "🔄 Generating embeddings for 149 chunks...\n",
      "🔍 Processing batch 1/5 (32 texts)\n",
      "⚠️  Batch exceeds token limit (10109 > 7992), reducing batch size\n",
      "✅ Successfully processed 32 texts with batch_size=16\n",
      "📊 Generated embeddings for batch 1/5\n",
      "🔍 Processing batch 2/5 (32 texts)\n",
      "⚠️  Batch exceeds token limit (10083 > 7992), reducing batch size\n",
      "✅ Successfully processed 32 texts with batch_size=16\n",
      "📊 Generated embeddings for batch 2/5\n",
      "🔍 Processing batch 3/5 (32 texts)\n",
      "⚠️  Batch exceeds token limit (10053 > 7992), reducing batch size\n",
      "✅ Successfully processed 32 texts with batch_size=16\n",
      "📊 Generated embeddings for batch 3/5\n",
      "🔍 Processing batch 4/5 (32 texts)\n",
      "⚠️  Batch exceeds token limit (9971 > 7992), reducing batch size\n",
      "✅ Successfully processed 32 texts with batch_size=16\n",
      "📊 Generated embeddings for batch 4/5\n",
      "🔍 Processing batch 5/5 (21 texts)\n",
      "📊 Generated embeddings for batch 5/5\n",
      "✅ Generated 149 embeddings with token-aware processing\n",
      " • Generated 149 embeddings\n",
      " • Created 149 KBChunkEmbedding objects\n",
      "✅ Stored 149 Knowledge Base embeddings\n",
      "✅ Successfully processed CX Effortless_Guide_EN_V2\n",
      "📊 Stored 149 chunks in Knowledge Base\n",
      "📋 Cached: CX Effortless_Guide_EN_V2.pdf\n",
      "   ✅ Success (497.55s)\n",
      "   Progress: 2/14 (14.3%) - ETA: 54.6min\n",
      "\n",
      "📄 Processing document 3/14: CX Glossary\n",
      "   Language: ar+en\n",
      "   Entity: DGE\n",
      "   Domain: CX\n",
      "   Category: Glossary\n",
      "\n",
      "🔄 Processing: CX Glossary\n",
      "📁 File: knowledge_base_prod/CX/CX Glossary.pdf\n",
      "📋 Mapped to: DGE > CX > Glossary\n",
      "🔄 Processing: CX Glossary.pdf\n",
      "🌐 Using remote Docling server: http://74.162.37.71:5001/v1alpha/convert/file\n",
      "🔍 Processing with Docling: CX Glossary.pdf\n",
      "📋 Available fields in document: ['filename', 'md_content', 'json_content', 'html_content', 'text_content', 'doctags_content']\n",
      "✅ json_content has content: 13 chars\n",
      "❌ text_content is None or empty\n",
      "❌ md_content is None or empty\n",
      "❌ html_content is None or empty\n",
      "🔍 json_content keys: ['schema_name', 'version', 'name', 'origin', 'furniture', 'body', 'groups', 'texts', 'pictures', 'tables', 'key_value_items', 'form_items', 'pages']\n",
      "✅ Extracted text from json_content.texts: 5309 chars\n",
      "✅ Successfully deserialized DoclingDocument from remote server\n",
      "📋 Extracted 1 sections from DoclingDocument\n",
      "💾 Cached: CX Glossary.pdf\n",
      "🧩 Processing document to chunks: قائمة المصطلحات Glossary\n",
      " • Generated 3 chunks using token-based chunking\n",
      " • Mapped 3 chunks to 1 sections\n",
      "🚀 Enriching 3 chunks with 8 workers...\n",
      "💾 Cached 3 enriched chunks for CX Glossary\n",
      " • Enhanced 3 chunks with contextualization\n",
      "🔄 Generating embeddings for 3 chunks...\n",
      "🔍 Processing batch 1/1 (3 texts)\n",
      "📊 Generated embeddings for batch 1/1\n",
      "✅ Generated 3 embeddings with token-aware processing\n",
      " • Generated 3 embeddings\n",
      " • Created 3 KBChunkEmbedding objects\n",
      "✅ Stored 3 Knowledge Base embeddings\n",
      "✅ Successfully processed CX Glossary\n",
      "📊 Stored 3 chunks in Knowledge Base\n",
      "📋 Cached: CX Glossary.pdf\n",
      "   ✅ Success (19.71s)\n",
      "   Progress: 3/14 (21.4%) - ETA: 34.6min\n",
      "\n",
      "📄 Processing document 4/14: CX TOV templates\n",
      "   Language: ar+en\n",
      "   Entity: DGE\n",
      "   Domain: CX\n",
      "   Category: Templates\n",
      "\n",
      "🔄 Processing: CX TOV templates\n",
      "📁 File: knowledge_base_prod/CX/CX TOV templates.pdf\n",
      "📋 Mapped to: DGE > CX > Templates\n",
      "🔄 Processing: CX TOV templates.pdf\n",
      "🌐 Using remote Docling server: http://74.162.37.71:5001/v1alpha/convert/file\n",
      "🔍 Processing with Docling: CX TOV templates.pdf\n",
      "📋 Available fields in document: ['filename', 'md_content', 'json_content', 'html_content', 'text_content', 'doctags_content']\n",
      "✅ json_content has content: 13 chars\n",
      "❌ text_content is None or empty\n",
      "❌ md_content is None or empty\n",
      "❌ html_content is None or empty\n",
      "🔍 json_content keys: ['schema_name', 'version', 'name', 'origin', 'furniture', 'body', 'groups', 'texts', 'pictures', 'tables', 'key_value_items', 'form_items', 'pages']\n",
      "✅ Extracted text from json_content.texts: 86302 chars\n",
      "✅ Successfully deserialized DoclingDocument from remote server\n",
      "📋 Extracted 203 sections from DoclingDocument\n",
      "💾 Cached: CX TOV templates.pdf\n",
      "🧩 Processing document to chunks: النماذج و الأمثلة Templates and Examples\n",
      " • Generated 173 chunks using token-based chunking\n",
      " • Mapped 173 chunks to 203 sections\n",
      "🚀 Enriching 173 chunks with 8 workers...\n",
      "💾 Cached 173 enriched chunks for CX TOV templates\n",
      " • Enhanced 173 chunks with contextualization\n",
      "🔄 Generating embeddings for 173 chunks...\n",
      "🔍 Processing batch 1/6 (32 texts)\n",
      "⚠️  Batch exceeds token limit (9529 > 7992), reducing batch size\n",
      "✅ Successfully processed 32 texts with batch_size=16\n",
      "📊 Generated embeddings for batch 1/6\n",
      "🔍 Processing batch 2/6 (32 texts)\n",
      "⚠️  Batch exceeds token limit (9396 > 7992), reducing batch size\n",
      "✅ Successfully processed 32 texts with batch_size=16\n",
      "📊 Generated embeddings for batch 2/6\n",
      "🔍 Processing batch 3/6 (32 texts)\n",
      "⚠️  Batch exceeds token limit (9557 > 7992), reducing batch size\n",
      "✅ Successfully processed 32 texts with batch_size=16\n",
      "📊 Generated embeddings for batch 3/6\n",
      "🔍 Processing batch 4/6 (32 texts)\n",
      "⚠️  Batch exceeds token limit (9426 > 7992), reducing batch size\n",
      "✅ Successfully processed 32 texts with batch_size=16\n",
      "📊 Generated embeddings for batch 4/6\n",
      "🔍 Processing batch 5/6 (32 texts)\n",
      "⚠️  Batch exceeds token limit (9425 > 7992), reducing batch size\n",
      "✅ Successfully processed 32 texts with batch_size=16\n",
      "📊 Generated embeddings for batch 5/6\n",
      "🔍 Processing batch 6/6 (13 texts)\n",
      "📊 Generated embeddings for batch 6/6\n",
      "✅ Generated 173 embeddings with token-aware processing\n",
      " • Generated 173 embeddings\n",
      " • Created 173 KBChunkEmbedding objects\n",
      "✅ Stored 173 Knowledge Base embeddings\n",
      "✅ Successfully processed CX TOV templates\n",
      "📊 Stored 173 chunks in Knowledge Base\n",
      "📋 Cached: CX TOV templates.pdf\n",
      "   ✅ Success (850.86s)\n",
      "   Progress: 4/14 (28.6%) - ETA: 59.0min\n",
      "\n",
      "📄 Processing document 5/14: Abu Dhabi Government Finance Policy Manual_EN\n",
      "   Language: en\n",
      "   Entity: Department of Finance\n",
      "   Domain: Finance\n",
      "   Category: Policy\n",
      "\n",
      "🔄 Processing: Abu Dhabi Government Finance Policy Manual_EN\n",
      "📁 File: knowledge_base_prod/Finance/Abu Dhabi Government Finance Policy Manual_EN.pdf\n",
      "📋 Mapped to: Department of Finance > Finance > Policy\n",
      "🔄 Processing: Abu Dhabi Government Finance Policy Manual_EN.pdf\n",
      "🌐 Using remote Docling server: http://74.162.37.71:5001/v1alpha/convert/file\n",
      "🔍 Processing with Docling: Abu Dhabi Government Finance Policy Manual_EN.pdf\n",
      "📋 Available fields in document: ['filename', 'md_content', 'json_content', 'html_content', 'text_content', 'doctags_content']\n",
      "✅ json_content has content: 13 chars\n",
      "❌ text_content is None or empty\n",
      "❌ md_content is None or empty\n",
      "❌ html_content is None or empty\n",
      "🔍 json_content keys: ['schema_name', 'version', 'name', 'origin', 'furniture', 'body', 'groups', 'texts', 'pictures', 'tables', 'key_value_items', 'form_items', 'pages']\n",
      "✅ Extracted text from json_content.texts: 145675 chars\n",
      "✅ Successfully deserialized DoclingDocument from remote server\n",
      "📋 Extracted 135 sections from DoclingDocument\n",
      "💾 Cached: Abu Dhabi Government Finance Policy Manual_EN.pdf\n",
      "🧩 Processing document to chunks: Abu Dhabi Government Finance Policy Manual Department of Finance\n",
      " • Generated 129 chunks using token-based chunking\n",
      " • Mapped 129 chunks to 135 sections\n",
      "🚀 Enriching 129 chunks with 8 workers...\n",
      "💾 Cached 129 enriched chunks for Abu Dhabi Government Finance Policy Manual_EN\n",
      " • Enhanced 129 chunks with contextualization\n",
      "🔄 Generating embeddings for 129 chunks...\n",
      "🔍 Processing batch 1/5 (32 texts)\n",
      "⚠️  Batch exceeds token limit (10279 > 7992), reducing batch size\n",
      "✅ Successfully processed 32 texts with batch_size=16\n",
      "📊 Generated embeddings for batch 1/5\n",
      "🔍 Processing batch 2/5 (32 texts)\n",
      "⚠️  Batch exceeds token limit (10269 > 7992), reducing batch size\n",
      "✅ Successfully processed 32 texts with batch_size=16\n",
      "📊 Generated embeddings for batch 2/5\n",
      "🔍 Processing batch 3/5 (32 texts)\n",
      "⚠️  Batch exceeds token limit (10333 > 7992), reducing batch size\n",
      "✅ Successfully processed 32 texts with batch_size=16\n",
      "📊 Generated embeddings for batch 3/5\n",
      "🔍 Processing batch 4/5 (32 texts)\n",
      "⚠️  Batch exceeds token limit (10331 > 7992), reducing batch size\n",
      "✅ Successfully processed 32 texts with batch_size=16\n",
      "📊 Generated embeddings for batch 4/5\n",
      "🔍 Processing batch 5/5 (1 texts)\n",
      "📊 Generated embeddings for batch 5/5\n",
      "✅ Generated 129 embeddings with token-aware processing\n",
      " • Generated 129 embeddings\n",
      " • Created 129 KBChunkEmbedding objects\n",
      "✅ Stored 129 Knowledge Base embeddings\n",
      "✅ Successfully processed Abu Dhabi Government Finance Policy Manual_EN\n",
      "📊 Stored 129 chunks in Knowledge Base\n",
      "📋 Cached: Abu Dhabi Government Finance Policy Manual_EN.pdf\n",
      "   ✅ Success (163.15s)\n",
      "   Progress: 5/14 (35.7%) - ETA: 47.4min\n",
      "\n",
      "📄 Processing document 6/14: HR - HR Law - EN\n",
      "   Language: en\n",
      "   Entity: Human Resource Authority\n",
      "   Domain: HR\n",
      "   Category: Law\n",
      "\n",
      "🔄 Processing: HR - HR Law - EN\n",
      "📁 File: knowledge_base_prod/HR/HR - HR Law - EN.pdf\n",
      "📋 Mapped to: Human Resource Authority > HR > Law\n",
      "🔄 Processing: HR - HR Law - EN.pdf\n",
      "🌐 Using remote Docling server: http://74.162.37.71:5001/v1alpha/convert/file\n",
      "🔍 Processing with Docling: HR - HR Law - EN.pdf\n",
      "📋 Available fields in document: ['filename', 'md_content', 'json_content', 'html_content', 'text_content', 'doctags_content']\n",
      "✅ json_content has content: 13 chars\n",
      "❌ text_content is None or empty\n",
      "❌ md_content is None or empty\n",
      "❌ html_content is None or empty\n",
      "🔍 json_content keys: ['schema_name', 'version', 'name', 'origin', 'furniture', 'body', 'groups', 'texts', 'pictures', 'tables', 'key_value_items', 'form_items', 'pages']\n",
      "✅ Extracted text from json_content.texts: 53606 chars\n",
      "✅ Successfully deserialized DoclingDocument from remote server\n",
      "📋 Extracted 105 sections from DoclingDocument\n",
      "💾 Cached: HR - HR Law - EN.pdf\n",
      "🧩 Processing document to chunks: HR law\n",
      " • Generated 46 chunks using token-based chunking\n",
      " • Mapped 46 chunks to 105 sections\n",
      "🚀 Enriching 46 chunks with 8 workers...\n",
      "💾 Cached 46 enriched chunks for HR - HR Law - EN\n",
      " • Enhanced 46 chunks with contextualization\n",
      "🔄 Generating embeddings for 46 chunks...\n",
      "🔍 Processing batch 1/2 (32 texts)\n",
      "⚠️  Batch exceeds token limit (10124 > 7992), reducing batch size\n",
      "✅ Successfully processed 32 texts with batch_size=16\n",
      "📊 Generated embeddings for batch 1/2\n",
      "🔍 Processing batch 2/2 (14 texts)\n",
      "📊 Generated embeddings for batch 2/2\n",
      "✅ Generated 46 embeddings with token-aware processing\n",
      " • Generated 46 embeddings\n",
      " • Created 46 KBChunkEmbedding objects\n",
      "✅ Stored 46 Knowledge Base embeddings\n",
      "✅ Successfully processed HR - HR Law - EN\n",
      "📊 Stored 46 chunks in Knowledge Base\n",
      "📋 Cached: HR - HR Law - EN.pdf\n",
      "   ✅ Success (54.33s)\n",
      "   Progress: 6/14 (42.9%) - ETA: 36.3min\n",
      "\n",
      "📄 Processing document 7/14: HR - Implementation Regulation for HR Law No 6 Year 2016 - EN\n",
      "   Language: en\n",
      "   Entity: Human Resource Authority\n",
      "   Domain: HR\n",
      "   Category: Law\n",
      "\n",
      "🔄 Processing: HR - Implementation Regulation for HR Law No 6 Year 2016 - EN\n",
      "📁 File: knowledge_base_prod/HR/HR - Implementation Regulation for HR Law No 6 Year 2016 - EN.pdf\n",
      "📋 Mapped to: Human Resource Authority > HR > Law\n",
      "🔄 Processing: HR - Implementation Regulation for HR Law No 6 Year 2016 - EN.pdf\n",
      "🌐 Using remote Docling server: http://74.162.37.71:5001/v1alpha/convert/file\n",
      "🔍 Processing with Docling: HR - Implementation Regulation for HR Law No 6 Year 2016 - EN.pdf\n",
      "📋 Available fields in document: ['filename', 'md_content', 'json_content', 'html_content', 'text_content', 'doctags_content']\n",
      "✅ json_content has content: 13 chars\n",
      "❌ text_content is None or empty\n",
      "❌ md_content is None or empty\n",
      "❌ html_content is None or empty\n",
      "🔍 json_content keys: ['schema_name', 'version', 'name', 'origin', 'furniture', 'body', 'groups', 'texts', 'pictures', 'tables', 'key_value_items', 'form_items', 'pages']\n",
      "✅ Extracted text from json_content.texts: 179619 chars\n",
      "✅ Successfully deserialized DoclingDocument from remote server\n",
      "📋 Extracted 198 sections from DoclingDocument\n",
      "💾 Cached: HR - Implementation Regulation for HR Law No 6 Year 2016 - EN.pdf\n",
      "🧩 Processing document to chunks: Decision No . (     10      ) of 2020\n",
      " • Generated 165 chunks using token-based chunking\n",
      " • Mapped 165 chunks to 198 sections\n",
      "🚀 Enriching 165 chunks with 8 workers...\n",
      "💾 Cached 165 enriched chunks for HR - Implementation Regulation for HR Law No 6 Year 2016 - EN\n",
      " • Enhanced 165 chunks with contextualization\n",
      "🔄 Generating embeddings for 165 chunks...\n",
      "🔍 Processing batch 1/6 (32 texts)\n",
      "⚠️  Batch exceeds token limit (10301 > 7992), reducing batch size\n",
      "✅ Successfully processed 32 texts with batch_size=16\n",
      "📊 Generated embeddings for batch 1/6\n",
      "🔍 Processing batch 2/6 (32 texts)\n",
      "⚠️  Batch exceeds token limit (10211 > 7992), reducing batch size\n",
      "✅ Successfully processed 32 texts with batch_size=16\n",
      "📊 Generated embeddings for batch 2/6\n",
      "🔍 Processing batch 3/6 (32 texts)\n",
      "⚠️  Batch exceeds token limit (10351 > 7992), reducing batch size\n",
      "✅ Successfully processed 32 texts with batch_size=16\n",
      "📊 Generated embeddings for batch 3/6\n",
      "🔍 Processing batch 4/6 (32 texts)\n",
      "⚠️  Batch exceeds token limit (10399 > 7992), reducing batch size\n",
      "✅ Successfully processed 32 texts with batch_size=16\n",
      "📊 Generated embeddings for batch 4/6\n",
      "🔍 Processing batch 5/6 (32 texts)\n",
      "⚠️  Batch exceeds token limit (10148 > 7992), reducing batch size\n",
      "✅ Successfully processed 32 texts with batch_size=16\n",
      "📊 Generated embeddings for batch 5/6\n",
      "🔍 Processing batch 6/6 (5 texts)\n",
      "📊 Generated embeddings for batch 6/6\n",
      "✅ Generated 165 embeddings with token-aware processing\n",
      " • Generated 165 embeddings\n",
      " • Created 165 KBChunkEmbedding objects\n",
      "✅ Stored 165 Knowledge Base embeddings\n",
      "✅ Successfully processed HR - Implementation Regulation for HR Law No 6 Year 2016 - EN\n",
      "📊 Stored 165 chunks in Knowledge Base\n",
      "📋 Cached: HR - Implementation Regulation for HR Law No 6 Year 2016 - EN.pdf\n",
      "   ✅ Success (217.31s)\n",
      "   Progress: 7/14 (50.0%) - ETA: 30.9min\n",
      "\n",
      "📄 Processing document 8/14: Proc - DGE Procurement Framework - EN\n",
      "   Language: en\n",
      "   Entity: DGE\n",
      "   Domain: Procurement\n",
      "   Category: Framework\n",
      "\n",
      "🔄 Processing: Proc - DGE Procurement Framework - EN\n",
      "📁 File: knowledge_base_prod/Procurement/Proc - DGE Procurement Framework - EN.DOC\n",
      "📋 Mapped to: DGE > Procurement > Framework\n",
      "🔄 Processing: Proc - DGE Procurement Framework - EN.DOC\n",
      "🌐 Using remote Docling server: http://74.162.37.71:5001/v1alpha/convert/file\n",
      "🔍 Processing with Docling: Proc - DGE Procurement Framework - EN.DOC\n",
      "📋 Available fields in document: ['filename', 'md_content', 'json_content', 'html_content', 'text_content', 'doctags_content']\n",
      "✅ json_content has content: 13 chars\n",
      "❌ text_content is None or empty\n",
      "❌ md_content is None or empty\n",
      "❌ html_content is None or empty\n",
      "🔍 json_content keys: ['schema_name', 'version', 'name', 'origin', 'furniture', 'body', 'groups', 'texts', 'pictures', 'tables', 'key_value_items', 'form_items', 'pages']\n",
      "✅ Extracted text from json_content.texts: 221545 chars\n",
      "✅ Successfully deserialized DoclingDocument from remote server\n",
      "📋 Extracted 244 sections from DoclingDocument\n",
      "💾 Cached: Proc - DGE Procurement Framework - EN.DOC\n",
      "🧩 Processing document to chunks: DGE Revised Procurement Framework\n",
      " • Generated 185 chunks using token-based chunking\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      " • Mapped 185 chunks to 244 sections\n",
      "🚀 Enriching 185 chunks with 8 workers...\n",
      "💾 Cached 185 enriched chunks for Proc - DGE Procurement Framework - EN.DOC\n",
      " • Enhanced 185 chunks with contextualization\n",
      "🔄 Generating embeddings for 185 chunks...\n",
      "🔍 Processing batch 1/6 (32 texts)\n",
      "⚠️  Batch exceeds token limit (10177 > 7992), reducing batch size\n",
      "✅ Successfully processed 32 texts with batch_size=16\n",
      "📊 Generated embeddings for batch 1/6\n",
      "🔍 Processing batch 2/6 (32 texts)\n",
      "⚠️  Batch exceeds token limit (10334 > 7992), reducing batch size\n",
      "✅ Successfully processed 32 texts with batch_size=16\n",
      "📊 Generated embeddings for batch 2/6\n",
      "🔍 Processing batch 3/6 (32 texts)\n",
      "⚠️  Batch exceeds token limit (10332 > 7992), reducing batch size\n",
      "✅ Successfully processed 32 texts with batch_size=16\n",
      "📊 Generated embeddings for batch 3/6\n",
      "🔍 Processing batch 4/6 (32 texts)\n",
      "⚠️  Batch exceeds token limit (10413 > 7992), reducing batch size\n",
      "✅ Successfully processed 32 texts with batch_size=16\n",
      "📊 Generated embeddings for batch 4/6\n",
      "🔍 Processing batch 5/6 (32 texts)\n",
      "⚠️  Batch exceeds token limit (10489 > 7992), reducing batch size\n",
      "✅ Successfully processed 32 texts with batch_size=16\n",
      "📊 Generated embeddings for batch 5/6\n",
      "🔍 Processing batch 6/6 (25 texts)\n",
      "📊 Generated embeddings for batch 6/6\n",
      "✅ Generated 185 embeddings with token-aware processing\n",
      " • Generated 185 embeddings\n",
      " • Created 185 KBChunkEmbedding objects\n",
      "✅ Stored 185 Knowledge Base embeddings\n",
      "✅ Successfully processed Proc - DGE Procurement Framework - EN\n",
      "📊 Stored 185 chunks in Knowledge Base\n",
      "📋 Cached: Proc - DGE Procurement Framework - EN.DOC\n",
      "   ✅ Success (62.58s)\n",
      "   Progress: 8/14 (57.1%) - ETA: 23.9min\n",
      "\n",
      "📄 Processing document 9/14: Proc - Policies and Procedures for Sales Auctions and Warehouses - EN\n",
      "   Language: en\n",
      "   Entity: DGE\n",
      "   Domain: Procurement\n",
      "   Category: Policy\n",
      "\n",
      "🔄 Processing: Proc - Policies and Procedures for Sales Auctions and Warehouses - EN\n",
      "📁 File: knowledge_base_prod/Procurement/Proc - Policies and Procedures for Sales Auctions and Warehouses - EN.PDF\n",
      "📋 Mapped to: DGE > Procurement > Policy\n",
      "🔄 Processing: Proc - Policies and Procedures for Sales Auctions and Warehouses - EN.PDF\n",
      "🌐 Using remote Docling server: http://74.162.37.71:5001/v1alpha/convert/file\n",
      "🔍 Processing with Docling: Proc - Policies and Procedures for Sales Auctions and Warehouses - EN.PDF\n",
      "📋 Available fields in document: ['filename', 'md_content', 'json_content', 'html_content', 'text_content', 'doctags_content']\n",
      "✅ json_content has content: 13 chars\n",
      "❌ text_content is None or empty\n",
      "❌ md_content is None or empty\n",
      "❌ html_content is None or empty\n",
      "🔍 json_content keys: ['schema_name', 'version', 'name', 'origin', 'furniture', 'body', 'groups', 'texts', 'pictures', 'tables', 'key_value_items', 'form_items', 'pages']\n",
      "✅ Extracted text from json_content.texts: 16207 chars\n",
      "✅ Successfully deserialized DoclingDocument from remote server\n",
      "📋 Extracted 21 sections from DoclingDocument\n",
      "💾 Cached: Proc - Policies and Procedures for Sales Auctions and Warehouses - EN.PDF\n",
      "🧩 Processing document to chunks: Policies and procedures for Sales Auctions and Warehouses\n",
      " • Generated 15 chunks using token-based chunking\n",
      " • Mapped 15 chunks to 21 sections\n",
      "🚀 Enriching 15 chunks with 8 workers...\n",
      "💾 Cached 15 enriched chunks for Proc - Policies and Procedures for Sales Auctions and Warehouses - EN\n",
      " • Enhanced 15 chunks with contextualization\n",
      "🔄 Generating embeddings for 15 chunks...\n",
      "🔍 Processing batch 1/1 (15 texts)\n",
      "📊 Generated embeddings for batch 1/1\n",
      "✅ Generated 15 embeddings with token-aware processing\n",
      " • Generated 15 embeddings\n",
      " • Created 15 KBChunkEmbedding objects\n",
      "✅ Stored 15 Knowledge Base embeddings\n",
      "✅ Successfully processed Proc - Policies and Procedures for Sales Auctions and Warehouses - EN\n",
      "📊 Stored 15 chunks in Knowledge Base\n",
      "📋 Cached: Proc - Policies and Procedures for Sales Auctions and Warehouses - EN.PDF\n",
      "   ✅ Success (10.60s)\n",
      "   Progress: 9/14 (64.3%) - ETA: 17.8min\n",
      "\n",
      "📄 Processing document 10/14: Proc - Procurement Charter - EN\n",
      "   Language: en\n",
      "   Entity: DGE\n",
      "   Domain: Procurement\n",
      "   Category: Charter\n",
      "\n",
      "🔄 Processing: Proc - Procurement Charter - EN\n",
      "📁 File: knowledge_base_prod/Procurement/Proc - Procurement Charter - EN.pdf\n",
      "📋 Mapped to: DGE > Procurement > Charter\n",
      "🔄 Processing: Proc - Procurement Charter - EN.pdf\n",
      "🌐 Using remote Docling server: http://74.162.37.71:5001/v1alpha/convert/file\n",
      "🔍 Processing with Docling: Proc - Procurement Charter - EN.pdf\n",
      "📋 Available fields in document: ['filename', 'md_content', 'json_content', 'html_content', 'text_content', 'doctags_content']\n",
      "✅ json_content has content: 13 chars\n",
      "❌ text_content is None or empty\n",
      "❌ md_content is None or empty\n",
      "❌ html_content is None or empty\n",
      "🔍 json_content keys: ['schema_name', 'version', 'name', 'origin', 'furniture', 'body', 'groups', 'texts', 'pictures', 'tables', 'key_value_items', 'form_items', 'pages']\n",
      "✅ Extracted text from json_content.texts: 24552 chars\n",
      "✅ Successfully deserialized DoclingDocument from remote server\n",
      "📋 Extracted 37 sections from DoclingDocument\n",
      "💾 Cached: Proc - Procurement Charter - EN.pdf\n",
      "🧩 Processing document to chunks: Procurement Charter Issued pursuant to the Abu Dhabi Procurement Standards\n",
      " • Generated 21 chunks using token-based chunking\n",
      " • Mapped 21 chunks to 37 sections\n",
      "🚀 Enriching 21 chunks with 8 workers...\n",
      "💾 Cached 21 enriched chunks for Proc - Procurement Charter - EN\n",
      " • Enhanced 21 chunks with contextualization\n",
      "🔄 Generating embeddings for 21 chunks...\n",
      "🔍 Processing batch 1/1 (21 texts)\n",
      "📊 Generated embeddings for batch 1/1\n",
      "✅ Generated 21 embeddings with token-aware processing\n",
      " • Generated 21 embeddings\n",
      " • Created 21 KBChunkEmbedding objects\n",
      "✅ Stored 21 Knowledge Base embeddings\n",
      "✅ Successfully processed Proc - Procurement Charter - EN\n",
      "📊 Stored 21 chunks in Knowledge Base\n",
      "📋 Cached: Proc - Procurement Charter - EN.pdf\n",
      "   ✅ Success (19.52s)\n",
      "   Progress: 10/14 (71.4%) - ETA: 13.0min\n",
      "\n",
      "📄 Processing document 11/14: Proc - Procurement Manual (Ariba Aligned) - EN\n",
      "   Language: en\n",
      "   Entity: DGE\n",
      "   Domain: Procurement\n",
      "   Category: Manual\n",
      "\n",
      "🔄 Processing: Proc - Procurement Manual (Ariba Aligned) - EN\n",
      "📁 File: knowledge_base_prod/Procurement/Proc - Procurement Manual (Ariba Aligned) - EN.pdf\n",
      "📋 Mapped to: DGE > Procurement > Manual\n",
      "🔄 Processing: Proc - Procurement Manual (Ariba Aligned) - EN.pdf\n",
      "🌐 Using remote Docling server: http://74.162.37.71:5001/v1alpha/convert/file\n",
      "🔍 Processing with Docling: Proc - Procurement Manual (Ariba Aligned) - EN.pdf\n",
      "❌ Failed to process Proc - Procurement Manual (Ariba Aligned) - EN.pdf: Docling processing failed: 500 Server Error: Internal Server Error for url: http://74.162.37.71:5001/v1alpha/convert/file\n",
      "❌ Document processing failed\n",
      "❌ Failed: Proc - Procurement Manual (Ariba Aligned) - EN.pdf - Document processing failed\n",
      "   ❌ Failed (121.20s)\n",
      "   Progress: 11/14 (78.6%) - ETA: 9.4min\n",
      "\n",
      "📄 Processing document 12/14: Proc - Procurement Manual (Business Process) - EN\n",
      "   Language: en\n",
      "   Entity: DGE\n",
      "   Domain: Procurement\n",
      "   Category: Manual\n",
      "\n",
      "🔄 Processing: Proc - Procurement Manual (Business Process) - EN\n",
      "📁 File: knowledge_base_prod/Procurement/Proc - Procurement Manual (Business Process) - EN.pdf\n",
      "📋 Mapped to: DGE > Procurement > Manual\n",
      "🔄 Processing: Proc - Procurement Manual (Business Process) - EN.pdf\n",
      "🌐 Using remote Docling server: http://74.162.37.71:5001/v1alpha/convert/file\n",
      "🔍 Processing with Docling: Proc - Procurement Manual (Business Process) - EN.pdf\n",
      "📋 Available fields in document: ['filename', 'md_content', 'json_content', 'html_content', 'text_content', 'doctags_content']\n",
      "✅ json_content has content: 13 chars\n",
      "❌ text_content is None or empty\n",
      "❌ md_content is None or empty\n",
      "❌ html_content is None or empty\n",
      "🔍 json_content keys: ['schema_name', 'version', 'name', 'origin', 'furniture', 'body', 'groups', 'texts', 'pictures', 'tables', 'key_value_items', 'form_items', 'pages']\n",
      "✅ Extracted text from json_content.texts: 125235 chars\n",
      "✅ Successfully deserialized DoclingDocument from remote server\n",
      "📋 Extracted 365 sections from DoclingDocument\n",
      "💾 Cached: Proc - Procurement Manual (Business Process) - EN.pdf\n",
      "🧩 Processing document to chunks: Procurement Manual (Business Processes) Issued pursuant to the Abu Dhabi Procurement Standards\n",
      " • Generated 121 chunks using token-based chunking\n",
      " • Mapped 121 chunks to 365 sections\n",
      "🚀 Enriching 121 chunks with 8 workers...\n",
      "💾 Cached 121 enriched chunks for Proc - Procurement Manual (Business Process) - EN\n",
      " • Enhanced 121 chunks with contextualization\n",
      "🔄 Generating embeddings for 121 chunks...\n",
      "🔍 Processing batch 1/4 (32 texts)\n",
      "⚠️  Batch exceeds token limit (10145 > 7992), reducing batch size\n",
      "✅ Successfully processed 32 texts with batch_size=16\n",
      "📊 Generated embeddings for batch 1/4\n",
      "🔍 Processing batch 2/4 (32 texts)\n",
      "⚠️  Batch exceeds token limit (10273 > 7992), reducing batch size\n",
      "✅ Successfully processed 32 texts with batch_size=16\n",
      "📊 Generated embeddings for batch 2/4\n",
      "🔍 Processing batch 3/4 (32 texts)\n",
      "⚠️  Batch exceeds token limit (10118 > 7992), reducing batch size\n",
      "✅ Successfully processed 32 texts with batch_size=16\n",
      "📊 Generated embeddings for batch 3/4\n",
      "🔍 Processing batch 4/4 (25 texts)\n",
      "📊 Generated embeddings for batch 4/4\n",
      "✅ Generated 121 embeddings with token-aware processing\n",
      " • Generated 121 embeddings\n",
      " • Created 121 KBChunkEmbedding objects\n",
      "✅ Stored 121 Knowledge Base embeddings\n",
      "✅ Successfully processed Proc - Procurement Manual (Business Process) - EN\n",
      "📊 Stored 121 chunks in Knowledge Base\n",
      "📋 Cached: Proc - Procurement Manual (Business Process) - EN.pdf\n",
      "   ✅ Success (228.72s)\n",
      "   Progress: 12/14 (85.7%) - ETA: 6.4min\n",
      "\n",
      "📄 Processing document 13/14: Proc - Procurement Standard Regulations - EN\n",
      "   Language: en\n",
      "   Entity: DGE\n",
      "   Domain: Procurement\n",
      "   Category: Manual\n",
      "\n",
      "🔄 Processing: Proc - Procurement Standard Regulations - EN\n",
      "📁 File: knowledge_base_prod/Procurement/Proc - Procurement Standard Regulations - EN.PDF\n",
      "📋 Mapped to: DGE > Procurement > Manual\n",
      "🔄 Processing: Proc - Procurement Standard Regulations - EN.PDF\n",
      "🌐 Using remote Docling server: http://74.162.37.71:5001/v1alpha/convert/file\n",
      "🔍 Processing with Docling: Proc - Procurement Standard Regulations - EN.PDF\n",
      "📋 Available fields in document: ['filename', 'md_content', 'json_content', 'html_content', 'text_content', 'doctags_content']\n",
      "✅ json_content has content: 13 chars\n",
      "❌ text_content is None or empty\n",
      "❌ md_content is None or empty\n",
      "❌ html_content is None or empty\n",
      "🔍 json_content keys: ['schema_name', 'version', 'name', 'origin', 'furniture', 'body', 'groups', 'texts', 'pictures', 'tables', 'key_value_items', 'form_items', 'pages']\n",
      "✅ Extracted text from json_content.texts: 115704 chars\n",
      "✅ Successfully deserialized DoclingDocument from remote server\n",
      "📋 Extracted 148 sections from DoclingDocument\n",
      "💾 Cached: Proc - Procurement Standard Regulations - EN.PDF\n",
      "🧩 Processing document to chunks: Abu Dhabi Procurement Standards\n",
      " • Generated 105 chunks using token-based chunking\n",
      " • Mapped 105 chunks to 148 sections\n",
      "🚀 Enriching 105 chunks with 8 workers...\n",
      "💾 Cached 105 enriched chunks for Proc - Procurement Standard Regulations - EN\n",
      " • Enhanced 105 chunks with contextualization\n",
      "🔄 Generating embeddings for 105 chunks...\n",
      "🔍 Processing batch 1/4 (32 texts)\n",
      "⚠️  Batch exceeds token limit (10003 > 7992), reducing batch size\n",
      "✅ Successfully processed 32 texts with batch_size=16\n",
      "📊 Generated embeddings for batch 1/4\n",
      "🔍 Processing batch 2/4 (32 texts)\n",
      "⚠️  Batch exceeds token limit (10068 > 7992), reducing batch size\n",
      "✅ Successfully processed 32 texts with batch_size=16\n",
      "📊 Generated embeddings for batch 2/4\n",
      "🔍 Processing batch 3/4 (32 texts)\n",
      "⚠️  Batch exceeds token limit (10271 > 7992), reducing batch size\n",
      "✅ Successfully processed 32 texts with batch_size=16\n",
      "📊 Generated embeddings for batch 3/4\n",
      "🔍 Processing batch 4/4 (9 texts)\n",
      "📊 Generated embeddings for batch 4/4\n",
      "✅ Generated 105 embeddings with token-aware processing\n",
      " • Generated 105 embeddings\n",
      " • Created 105 KBChunkEmbedding objects\n",
      "✅ Stored 105 Knowledge Base embeddings\n",
      "✅ Successfully processed Proc - Procurement Standard Regulations - EN\n",
      "📊 Stored 105 chunks in Knowledge Base\n",
      "📋 Cached: Proc - Procurement Standard Regulations - EN.PDF\n",
      "   ✅ Success (123.55s)\n",
      "   Progress: 13/14 (92.9%) - ETA: 3.1min\n",
      "\n",
      "📄 Processing document 14/14: Proc - Support Frequently Asked Questions - EN\n",
      "   Language: ar+en\n",
      "   Entity: DGE\n",
      "   Domain: Procurement\n",
      "   Category: FAQ\n",
      "\n",
      "🔄 Processing: Proc - Support Frequently Asked Questions - EN\n",
      "📁 File: knowledge_base_prod/Procurement/Proc - Support Frequently Asked Questions - EN.docx\n",
      "📋 Mapped to: DGE > Procurement > FAQ\n",
      "🔄 Processing: Proc - Support Frequently Asked Questions - EN.docx\n",
      "🌐 Using remote Docling server: http://74.162.37.71:5001/v1alpha/convert/file\n",
      "🔍 Processing with Docling: Proc - Support Frequently Asked Questions - EN.docx\n",
      "📋 Available fields in document: ['filename', 'md_content', 'json_content', 'html_content', 'text_content', 'doctags_content']\n",
      "✅ json_content has content: 13 chars\n",
      "❌ text_content is None or empty\n",
      "❌ md_content is None or empty\n",
      "❌ html_content is None or empty\n",
      "🔍 json_content keys: ['schema_name', 'version', 'name', 'origin', 'furniture', 'body', 'groups', 'texts', 'pictures', 'tables', 'key_value_items', 'form_items', 'pages']\n",
      "✅ Extracted text from json_content.texts: 2634 chars\n",
      "✅ Successfully deserialized DoclingDocument from remote server\n",
      "📋 Extracted 23 sections from DoclingDocument\n",
      "💾 Cached: Proc - Support Frequently Asked Questions - EN.docx\n",
      "🧩 Processing document to chunks: March 2023 – V1.0\n",
      " • Generated 6 chunks using token-based chunking\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      "⚠️  Warning: Could not calculate page numbers with PyPDF2: EOF marker not found\n",
      " • Mapped 6 chunks to 23 sections\n",
      "🚀 Enriching 6 chunks with 8 workers...\n",
      "💾 Cached 6 enriched chunks for Proc - Support Frequently Asked Questions - EN.docx\n",
      " • Enhanced 6 chunks with contextualization\n",
      "🔄 Generating embeddings for 6 chunks...\n",
      "🔍 Processing batch 1/1 (6 texts)\n",
      "📊 Generated embeddings for batch 1/1\n",
      "✅ Generated 6 embeddings with token-aware processing\n",
      " • Generated 6 embeddings\n",
      " • Created 6 KBChunkEmbedding objects\n",
      "✅ Stored 6 Knowledge Base embeddings\n",
      "✅ Successfully processed Proc - Support Frequently Asked Questions - EN\n",
      "📊 Stored 6 chunks in Knowledge Base\n",
      "📋 Cached: Proc - Support Frequently Asked Questions - EN.docx\n",
      "   ✅ Success (8.33s)\n",
      "   Progress: 14/14 (100.0%) - ETA: 0.0min\n",
      "\n",
      "📊 Processing Summary:\n",
      "   - Total documents: 14\n",
      "   - Processed successfully: 13\n",
      "   - Failed: 1\n",
      "   - Success rate: 92.9%\n",
      "   - Total time: 40.4 minutes\n",
      "   - Average time per document: 173.3 seconds\n",
      "🔄 Processing Status: ✅ Success\n"
     ]
    }
   ],
   "source": [
    "def process_production_documents():\n",
    "    \"\"\"Process production documents with remote Docling\"\"\"\n",
    "    try:\n",
    "        print(\"🔄 Processing production documents...\")\n",
    "        \n",
    "        # Initialize database populator\n",
    "        populator = KBDatabasePopulator(version=\"1.2\")\n",
    "        \n",
    "        # Load production mapping\n",
    "        mapping_file = Path.cwd() / \"config\" / \"mapping_v1.2.yaml\"\n",
    "        if not mapping_file.exists():\n",
    "            print(f\"❌ Production mapping file not found: {mapping_file}\")\n",
    "            return False\n",
    "        \n",
    "        import yaml\n",
    "        with open(mapping_file, 'r') as f:\n",
    "            mapping_data = yaml.safe_load(f)\n",
    "        \n",
    "        # Get available documents\n",
    "        available_docs = [d for d in mapping_data.get('document_mappings', []) if d.get('status') == 'available']\n",
    "        \n",
    "        if not available_docs:\n",
    "            print(\"⚠️  No available documents found for processing\")\n",
    "            return False\n",
    "        \n",
    "        print(f\"📄 Found {len(available_docs)} available documents to process\")\n",
    "        \n",
    "        # Process each document\n",
    "        processed_count = 0\n",
    "        failed_count = 0\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for i, doc_data in enumerate(available_docs, 1):\n",
    "            print(f\"\\n📄 Processing document {i}/{len(available_docs)}: {doc_data['excel_title']}\")\n",
    "            print(f\"   Language: {doc_data['language']}\")\n",
    "            print(f\"   Entity: {doc_data['entity']}\")\n",
    "            print(f\"   Domain: {doc_data['domain']}\")\n",
    "            print(f\"   Category: {doc_data['category']}\")\n",
    "            \n",
    "            # Create document entry\n",
    "            from populate_database import DocumentEntry\n",
    "            \n",
    "            doc_entry = DocumentEntry(\n",
    "                excel_title=doc_data['excel_title'],\n",
    "                entity=doc_data['entity'],\n",
    "                domain=doc_data['domain'],\n",
    "                category=doc_data['category'],\n",
    "                language=doc_data['language'],\n",
    "                file_path=doc_data['file_path'],\n",
    "                filename=doc_data['filename'],\n",
    "                file_size=doc_data.get('file_size', 0),\n",
    "                status='available'\n",
    "            )\n",
    "            \n",
    "            # Process document\n",
    "            doc_start_time = time.time()\n",
    "            success = populator.populate_document(doc_entry)\n",
    "            doc_end_time = time.time()\n",
    "            \n",
    "            if success:\n",
    "                processed_count += 1\n",
    "                print(f\"   ✅ Success ({doc_end_time - doc_start_time:.2f}s)\")\n",
    "            else:\n",
    "                failed_count += 1\n",
    "                print(f\"   ❌ Failed ({doc_end_time - doc_start_time:.2f}s)\")\n",
    "            \n",
    "            # Show progress\n",
    "            elapsed = time.time() - start_time\n",
    "            avg_time = elapsed / i\n",
    "            eta = avg_time * (len(available_docs) - i)\n",
    "            \n",
    "            print(f\"   Progress: {i}/{len(available_docs)} ({i/len(available_docs)*100:.1f}%) - ETA: {eta/60:.1f}min\")\n",
    "        \n",
    "        # Final summary\n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"\\n📊 Processing Summary:\")\n",
    "        print(f\"   - Total documents: {len(available_docs)}\")\n",
    "        print(f\"   - Processed successfully: {processed_count}\")\n",
    "        print(f\"   - Failed: {failed_count}\")\n",
    "        print(f\"   - Success rate: {processed_count/len(available_docs)*100:.1f}%\")\n",
    "        print(f\"   - Total time: {total_time/60:.1f} minutes\")\n",
    "        print(f\"   - Average time per document: {total_time/len(available_docs):.1f} seconds\")\n",
    "        \n",
    "        return processed_count > 0\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing production documents: {e}\")\n",
    "        return False\n",
    "\n",
    "# Process production documents\n",
    "processing_success = process_production_documents()\n",
    "print(f\"🔄 Processing Status: {'✅ Success' if processing_success else '❌ Failed'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Verify Database Population\n",
    "\n",
    "### Check Knowledge Base Table Contents and Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Verifying database population...\n",
      "✅ Knowledge Base PostgreSQL connection pool initialized (max_size=10)\n",
      "✅ PGVector extension enabled for Knowledge Base\n",
      "🔄 Creating Knowledge Base hierarchy tables...\n",
      "🔧 Granting PUBLIC access to all WoG tables...\n",
      "🌱 Seeding Knowledge Base with Excel hierarchy data...\n",
      "✅ Knowledge Base database initialized with hierarchy data\n",
      "📊 Database Statistics:\n",
      "   - Total chunks: 1,717\n",
      "   - Unique documents: 15\n",
      "   - Chunks with embeddings: 1,717\n",
      "   - Embedding coverage: 100.0%\n",
      "   - Average chunks per document: 114.5\n",
      "\n",
      "🏛️  Entity Distribution:\n",
      "   - DGE: 1,296 chunks (10 docs)\n",
      "   - Human Resource Authority: 256 chunks (3 docs)\n",
      "   - Department of Finance: 129 chunks (1 docs)\n",
      "   - Unknown: 36 chunks (1 docs)\n",
      "\n",
      "🔧 Domain Distribution:\n",
      "   - IT: 518 chunks (1 docs)\n",
      "   - Procurement: 453 chunks (6 docs)\n",
      "   - CX: 361 chunks (4 docs)\n",
      "   - HR: 256 chunks (3 docs)\n",
      "   - Finance: 129 chunks (1 docs)\n",
      "\n",
      "📋 Category Distribution:\n",
      "   - Guide: 703 chunks (3 docs)\n",
      "   - Law: 256 chunks (3 docs)\n",
      "   - Manual: 226 chunks (2 docs)\n",
      "   - Framework: 185 chunks (1 docs)\n",
      "   - Templates: 173 chunks (1 docs)\n",
      "   - Policy: 144 chunks (2 docs)\n",
      "   - Charter: 21 chunks (1 docs)\n",
      "   - FAQ: 6 chunks (1 docs)\n",
      "   - Glossary: 3 chunks (1 docs)\n",
      "\n",
      "📄 Sample Documents:\n",
      "   - Doc 2 - Information Assurance Standards - EN (518 chunks)\n",
      "     Title: NESA UAE INFORMATION ASSURANCE STANDARDS\n",
      "     Hierarchy: DGE > IT > Guide\n",
      "   - Proc - DGE Procurement Framework - EN.DOC (185 chunks)\n",
      "     Title: DGE Revised Procurement Framework\n",
      "     Hierarchy: DGE > Procurement > Framework\n",
      "   - CX TOV templates (173 chunks)\n",
      "     Title: النماذج و الأمثلة Templates and Examples\n",
      "     Hierarchy: DGE > CX > Templates\n",
      "   - HR - Implementation Regulation for HR Law No 6 Year 2016 - EN (165 chunks)\n",
      "     Title: Decision No . (     10      ) of 2020\n",
      "     Hierarchy: Human Resource Authority > HR > Law\n",
      "   - CX Effortless_Guide_EN_V2 (149 chunks)\n",
      "     Title: © Issued by Department of Government Enablement | 2023 First Edition\n",
      "     Hierarchy: DGE > CX > Guide\n",
      "   - Abu Dhabi Government Finance Policy Manual_EN (129 chunks)\n",
      "     Title: Abu Dhabi Government Finance Policy Manual Department of Finance\n",
      "     Hierarchy: Department of Finance > Finance > Policy\n",
      "   - Proc - Procurement Manual (Business Process) - EN (121 chunks)\n",
      "     Title: Procurement Manual (Business Processes) Issued pursuant to the Abu Dhabi Procurement Standards\n",
      "     Hierarchy: DGE > Procurement > Manual\n",
      "   - Proc - Procurement Standard Regulations - EN (105 chunks)\n",
      "     Title: Abu Dhabi Procurement Standards\n",
      "     Hierarchy: DGE > Procurement > Manual\n",
      "   - HR - HR Law - EN (46 chunks)\n",
      "     Title: HR law\n",
      "     Hierarchy: Human Resource Authority > HR > Law\n",
      "   - Doc 3 - Human Resources Law - EN (45 chunks)\n",
      "     Title: HR law\n",
      "     Hierarchy: Human Resource Authority > HR > Law\n",
      "\n",
      "🔍 Testing Vector Search Performance...\n",
      "   - Search time: 144.2ms\n",
      "   - Results found: 10\n",
      "   - Performance: ✅ Good\n",
      "📊 Verification Status: ✅ Success\n"
     ]
    }
   ],
   "source": [
    "def verify_database_population():\n",
    "    \"\"\"Verify database population and show statistics\"\"\"\n",
    "    try:\n",
    "        print(\"📊 Verifying database population...\")\n",
    "        \n",
    "        # Initialize database manager\n",
    "        kb_manager = KnowledgeBaseManager()\n",
    "        \n",
    "        with kb_manager.get_connection() as conn:\n",
    "            with conn.cursor() as cur:\n",
    "                # Get total chunk count\n",
    "                cur.execute(\"SELECT COUNT(*) FROM knowledge_base\")\n",
    "                total_chunks = cur.fetchone()[0]\n",
    "                \n",
    "                # Get unique document count\n",
    "                cur.execute(\"SELECT COUNT(DISTINCT document_name) FROM knowledge_base\")\n",
    "                unique_docs = cur.fetchone()[0]\n",
    "                \n",
    "                # Get embedding count\n",
    "                cur.execute(\"SELECT COUNT(*) FROM knowledge_base WHERE embedding IS NOT NULL\")\n",
    "                embedding_count = cur.fetchone()[0]\n",
    "                \n",
    "                # Get language distribution\n",
    "                cur.execute(\"\"\"\n",
    "                    SELECT \n",
    "                        COALESCE(entity, 'Unknown') as entity,\n",
    "                        COUNT(*) as chunk_count,\n",
    "                        COUNT(DISTINCT document_name) as doc_count\n",
    "                    FROM knowledge_base \n",
    "                    GROUP BY entity \n",
    "                    ORDER BY chunk_count DESC\n",
    "                \"\"\")\n",
    "                entity_stats = cur.fetchall()\n",
    "                \n",
    "                # Get domain distribution\n",
    "                cur.execute(\"\"\"\n",
    "                    SELECT \n",
    "                        COALESCE(domain, 'Unknown') as domain,\n",
    "                        COUNT(*) as chunk_count,\n",
    "                        COUNT(DISTINCT document_name) as doc_count\n",
    "                    FROM knowledge_base \n",
    "                    GROUP BY domain \n",
    "                    ORDER BY chunk_count DESC\n",
    "                \"\"\")\n",
    "                domain_stats = cur.fetchall()\n",
    "                \n",
    "                # Get category distribution\n",
    "                cur.execute(\"\"\"\n",
    "                    SELECT \n",
    "                        COALESCE(category, 'Unknown') as category,\n",
    "                        COUNT(*) as chunk_count,\n",
    "                        COUNT(DISTINCT document_name) as doc_count\n",
    "                    FROM knowledge_base \n",
    "                    GROUP BY category \n",
    "                    ORDER BY chunk_count DESC\n",
    "                \"\"\")\n",
    "                category_stats = cur.fetchall()\n",
    "                \n",
    "                # Get sample documents\n",
    "                cur.execute(\"\"\"\n",
    "                    SELECT \n",
    "                        document_name,\n",
    "                        document_title,\n",
    "                        entity,\n",
    "                        domain,\n",
    "                        category,\n",
    "                        COUNT(*) as chunk_count\n",
    "                    FROM knowledge_base \n",
    "                    GROUP BY document_name, document_title, entity, domain, category\n",
    "                    ORDER BY chunk_count DESC\n",
    "                    LIMIT 10\n",
    "                \"\"\")\n",
    "                sample_docs = cur.fetchall()\n",
    "                \n",
    "                # Show statistics\n",
    "                print(f\"📊 Database Statistics:\")\n",
    "                print(f\"   - Total chunks: {total_chunks:,}\")\n",
    "                print(f\"   - Unique documents: {unique_docs}\")\n",
    "                print(f\"   - Chunks with embeddings: {embedding_count:,}\")\n",
    "                print(f\"   - Embedding coverage: {embedding_count/total_chunks*100:.1f}%\")\n",
    "                print(f\"   - Average chunks per document: {total_chunks/unique_docs:.1f}\")\n",
    "                \n",
    "                print(f\"\\n🏛️  Entity Distribution:\")\n",
    "                for entity, chunk_count, doc_count in entity_stats:\n",
    "                    print(f\"   - {entity}: {chunk_count:,} chunks ({doc_count} docs)\")\n",
    "                \n",
    "                print(f\"\\n🔧 Domain Distribution:\")\n",
    "                for domain, chunk_count, doc_count in domain_stats:\n",
    "                    print(f\"   - {domain}: {chunk_count:,} chunks ({doc_count} docs)\")\n",
    "                \n",
    "                print(f\"\\n📋 Category Distribution:\")\n",
    "                for category, chunk_count, doc_count in category_stats:\n",
    "                    print(f\"   - {category}: {chunk_count:,} chunks ({doc_count} docs)\")\n",
    "                \n",
    "                print(f\"\\n📄 Sample Documents:\")\n",
    "                for doc_name, doc_title, entity, domain, category, chunk_count in sample_docs:\n",
    "                    print(f\"   - {doc_name} ({chunk_count} chunks)\")\n",
    "                    print(f\"     Title: {doc_title}\")\n",
    "                    print(f\"     Hierarchy: {entity} > {domain} > {category}\")\n",
    "                \n",
    "                # Test vector search performance\n",
    "                print(f\"\\n🔍 Testing Vector Search Performance...\")\n",
    "                \n",
    "                # Generate a test embedding\n",
    "                import numpy as np\n",
    "                test_embedding = np.random.random(VECTOR_DIMENSION).tolist()\n",
    "                \n",
    "                # Test search speed\n",
    "                search_start = time.time()\n",
    "                results = kb_manager.search_kb_similar_vectors(test_embedding, limit=10)\n",
    "                search_time = time.time() - search_start\n",
    "                \n",
    "                print(f\"   - Search time: {search_time*1000:.1f}ms\")\n",
    "                print(f\"   - Results found: {len(results)}\")\n",
    "                print(f\"   - Performance: {'✅ Good' if search_time < 1.0 else '⚠️ Slow'}\")\n",
    "                \n",
    "                return True\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error verifying database population: {e}\")\n",
    "        return False\n",
    "\n",
    "# Verify database population\n",
    "verification_success = verify_database_population()\n",
    "print(f\"📊 Verification Status: {'✅ Success' if verification_success else '❌ Failed'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Performance Tuning and Optimization\n",
    "\n",
    "### Analyze and Optimize Database Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_tuning():\n",
    "    \"\"\"Analyze and optimize database performance\"\"\"\n",
    "    try:\n",
    "        print(\"⚡ Analyzing database performance...\")\n",
    "        \n",
    "        kb_manager = KnowledgeBaseManager()\n",
    "        \n",
    "        with kb_manager.get_connection() as conn:\n",
    "            with conn.cursor() as cur:\n",
    "                # Check table sizes\n",
    "                cur.execute(\"\"\"\n",
    "                    SELECT \n",
    "                        schemaname,\n",
    "                        tablename,\n",
    "                        pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as size\n",
    "                    FROM pg_tables \n",
    "                    WHERE schemaname = 'public'\n",
    "                    ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC\n",
    "                \"\"\")\n",
    "                table_sizes = cur.fetchall()\n",
    "                \n",
    "                # Check index usage\n",
    "                cur.execute(\"\"\"\n",
    "                    SELECT \n",
    "                        schemaname,\n",
    "                        tablename,\n",
    "                        indexname,\n",
    "                        idx_scan,\n",
    "                        idx_tup_read,\n",
    "                        idx_tup_fetch\n",
    "                    FROM pg_stat_user_indexes \n",
    "                    ORDER BY idx_scan DESC\n",
    "                \"\"\")\n",
    "                index_stats = cur.fetchall()\n",
    "                \n",
    "                # Analyze query performance\n",
    "                print(f\"📊 Performance Analysis:\")\n",
    "                \n",
    "                print(f\"\\n💾 Table Sizes:\")\n",
    "                for schema, table, size in table_sizes:\n",
    "                    print(f\"   - {table}: {size}\")\n",
    "                \n",
    "                print(f\"\\n🔍 Index Usage:\")\n",
    "                for schema, table, index, scans, reads, fetches in index_stats[:10]:\n",
    "                    print(f\"   - {index} ({table}): {scans:,} scans, {reads:,} reads\")\n",
    "                \n",
    "                # Test different search scenarios\n",
    "                print(f\"\\n🎯 Search Performance Tests:\")\n",
    "                \n",
    "                # Test 1: Basic vector search\n",
    "                test_embedding = np.random.random(VECTOR_DIMENSION).tolist()\n",
    "                start_time = time.time()\n",
    "                results = kb_manager.search_kb_similar_vectors(test_embedding, limit=20)\n",
    "                basic_search_time = time.time() - start_time\n",
    "                print(f\"   - Basic vector search (20 results): {basic_search_time*1000:.1f}ms\")\n",
    "                \n",
    "                # Test 2: Filtered search by entity\n",
    "                start_time = time.time()\n",
    "                results = kb_manager.search_kb_similar_vectors(\n",
    "                    test_embedding, \n",
    "                    limit=20,\n",
    "                    entity_filter=['DGE']\n",
    "                )\n",
    "                entity_search_time = time.time() - start_time\n",
    "                print(f\"   - Entity filtered search: {entity_search_time*1000:.1f}ms\")\n",
    "                \n",
    "                # Test 3: Multi-filter search\n",
    "                start_time = time.time()\n",
    "                results = kb_manager.search_kb_similar_vectors(\n",
    "                    test_embedding, \n",
    "                    limit=20,\n",
    "                    entity_filter=['DGE'],\n",
    "                    domain_filter=['HR'],\n",
    "                    category_filter=['Guide']\n",
    "                )\n",
    "                multi_filter_time = time.time() - start_time\n",
    "                print(f\"   - Multi-filter search: {multi_filter_time*1000:.1f}ms\")\n",
    "                \n",
    "                # Performance recommendations\n",
    "                print(f\"\\n💡 Performance Recommendations:\")\n",
    "                \n",
    "                if basic_search_time > 1.0:\n",
    "                    print(f\"   ⚠️ Basic search is slow (>{basic_search_time:.1f}s) - consider query optimization\")\n",
    "                else:\n",
    "                    print(f\"   ✅ Basic search performance is good ({basic_search_time*1000:.1f}ms)\")\n",
    "                \n",
    "                if entity_search_time > basic_search_time * 1.5:\n",
    "                    print(f\"   ⚠️ Filtered search is significantly slower - check indexes\")\n",
    "                else:\n",
    "                    print(f\"   ✅ Filtered search performance is acceptable\")\n",
    "                \n",
    "                # Database maintenance recommendations\n",
    "                print(f\"\\n🔧 Maintenance Recommendations:\")\n",
    "                print(f\"   - Run VACUUM ANALYZE regularly for optimal performance\")\n",
    "                print(f\"   - Monitor index usage and remove unused indexes\")\n",
    "                print(f\"   - Consider connection pooling for production workloads\")\n",
    "                print(f\"   - Set up monitoring for query performance\")\n",
    "                \n",
    "                return True\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during performance tuning: {e}\")\n",
    "        return False\n",
    "\n",
    "# Performance tuning\n",
    "tuning_success = performance_tuning()\n",
    "print(f\"⚡ Tuning Status: {'✅ Success' if tuning_success else '❌ Failed'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Final Summary and Next Steps\n",
    "\n",
    "### Pipeline Completion Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_summary():\n",
    "    \"\"\"Generate final pipeline summary\"\"\"\n",
    "    try:\n",
    "        print(\"📋 Production Ingestion Pipeline Summary\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Overall status\n",
    "        steps = [\n",
    "            (\"Database Creation\", db_success),\n",
    "            (\"Schema Creation\", schema_success),\n",
    "            (\"YAML Mapping\", mapping_success),\n",
    "            (\"Hierarchy Population\", hierarchy_success),\n",
    "            (\"Cache Clearing\", cache_success),\n",
    "            (\"Document Processing\", processing_success),\n",
    "            (\"Database Verification\", verification_success),\n",
    "            (\"Performance Tuning\", tuning_success)\n",
    "        ]\n",
    "        \n",
    "        successful_steps = sum(1 for _, success in steps if success)\n",
    "        total_steps = len(steps)\n",
    "        \n",
    "        print(f\"📊 Pipeline Status: {successful_steps}/{total_steps} steps completed\")\n",
    "        print(f\"✅ Success Rate: {successful_steps/total_steps*100:.1f}%\")\n",
    "        \n",
    "        print(f\"\\n📋 Step Details:\")\n",
    "        for step_name, success in steps:\n",
    "            status = \"✅ Success\" if success else \"❌ Failed\"\n",
    "            print(f\"   - {step_name}: {status}\")\n",
    "        \n",
    "        # Database summary\n",
    "        if verification_success:\n",
    "            kb_manager = KnowledgeBaseManager()\n",
    "            with kb_manager.get_connection() as conn:\n",
    "                with conn.cursor() as cur:\n",
    "                    cur.execute(\"SELECT COUNT(*) FROM knowledge_base\")\n",
    "                    total_chunks = cur.fetchone()[0]\n",
    "                    \n",
    "                    cur.execute(\"SELECT COUNT(DISTINCT document_name) FROM knowledge_base\")\n",
    "                    unique_docs = cur.fetchone()[0]\n",
    "                    \n",
    "                    cur.execute(\"SELECT COUNT(*) FROM knowledge_base WHERE embedding IS NOT NULL\")\n",
    "                    embedding_count = cur.fetchone()[0]\n",
    "                    \n",
    "                    print(f\"\\n📊 Final Database State:\")\n",
    "                    print(f\"   - Database: {PRODUCTION_DATABASE_NAME}\")\n",
    "                    print(f\"   - Total chunks: {total_chunks:,}\")\n",
    "                    print(f\"   - Unique documents: {unique_docs}\")\n",
    "                    print(f\"   - Embeddings: {embedding_count:,}\")\n",
    "                    print(f\"   - Vector dimension: {VECTOR_DIMENSION}\")\n",
    "                    print(f\"   - Embedding model: {EMBEDDING_MODEL}\")\n",
    "        \n",
    "        # Next steps\n",
    "        print(f\"\\n🚀 Next Steps:\")\n",
    "        print(f\"   1. Set up search API endpoints\")\n",
    "        print(f\"   2. Implement BM25 hybrid search\")\n",
    "        print(f\"   3. Create user interface for search\")\n",
    "        print(f\"   4. Set up monitoring and alerts\")\n",
    "        print(f\"   5. Deploy to production environment\")\n",
    "        \n",
    "        # Configuration for next steps\n",
    "        print(f\"\\n🔧 Configuration for Next Steps:\")\n",
    "        print(f\"   - Database URL: postgresql://mustaqmollah@localhost:5432/{PRODUCTION_DATABASE_NAME}\")\n",
    "        print(f\"   - Embedding Model: {EMBEDDING_MODEL}\")\n",
    "        print(f\"   - Vector Dimension: {VECTOR_DIMENSION}\")\n",
    "        print(f\"   - Remote Docling: {os.getenv('DOCLING_SERVER_URL')}\")\n",
    "        print(f\"   - Allowed Languages: {ALLOWED_LANGUAGES}\")\n",
    "        \n",
    "        # Final timestamp\n",
    "        print(f\"\\n⏰ Completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        \n",
    "        if successful_steps == total_steps:\n",
    "            print(f\"\\n🎉 Production ingestion pipeline completed successfully!\")\n",
    "            print(f\"   The Abu Dhabi Government Knowledge Base is ready for production use.\")\n",
    "        else:\n",
    "            print(f\"\\n⚠️  Pipeline completed with {total_steps - successful_steps} failed steps.\")\n",
    "            print(f\"   Please review the failed steps and retry if necessary.\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error generating final summary: {e}\")\n",
    "        return False\n",
    "\n",
    "# Generate final summary\n",
    "final_summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
