# Environment Configuration Template for GovGPT RAG Pipeline
# Copy this file to .env and fill in your values

# ==============================================
# LLM API Configuration
# ==============================================

# OpenAI API Configuration
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_API_BASE=https://api.openai.com/v1

# Alternative: If using a proxy or custom endpoint
# OPENAI_API_BASE=http://localhost:8080/v1

# ==============================================
# RAG Model Configuration
# ==============================================

# Primary LLM model for response generation
RAG_MODEL=gpt-4.1

# Embedding model for document retrieval
# Options: text-embedding-3-large, text-embedding-3-small, all-MiniLM-L6-v2
EMBED_MODEL=text-embedding-3-large

# ==============================================
# RAG Retrieval Parameters
# ==============================================

# Number of candidates from vector search
VECTOR_K=75

# Number of candidates from BM25 search
BM25_K=75

# Number of chunks after RRF fusion
TOP_K=20

# Final number of chunks for context
TOP_N=5

# RRF parameter for rank fusion
RRF_K=60

# ==============================================
# Server Configuration (for local testing)
# ==============================================

# Local RAG server URL (for Pipeline client)
RAG_SERVER_URL=http://localhost:8100

# Server request timeout in seconds
RAG_SERVER_TIMEOUT=30

# Optional: API key for server authentication
RAG_SERVER_API_KEY=

# Maximum retry attempts for failed requests
RAG_SERVER_MAX_RETRIES=3

# ==============================================
# Pipeline Configuration
# ==============================================

# Enable streaming responses
ENABLE_STREAMING=true

# Enable debug logging
ENABLE_DEBUG=false

# Enable server health checks
ENABLE_SERVER_HEALTH_CHECK=true

# Auto-test connection on startup
AUTO_TEST_CONNECTION=true

# ==============================================
# Docker Configuration
# ==============================================

# Artifact directory path
ARTIFACT_DIR=./artifacts

# Python path
PYTHONPATH=/app

# Unbuffered Python output
PYTHONUNBUFFERED=1

# ==============================================
# Development Settings
# ==============================================

# Log level (INFO, DEBUG, WARNING, ERROR)
LOG_LEVEL=INFO

# Enable development mode
DEV_MODE=false

# ==============================================
# Optional: Additional API Keys
# ==============================================

# Together AI API key (if using Together models)
# TOGETHER_API_KEY=your_together_api_key_here

# LiteLLM proxy settings (if using LiteLLM)
# LITELLM_PROXY_URL=http://localhost:8000
# LITELLM_API_KEY=your_litellm_key_here

# ==============================================
# Notes:
# ==============================================
# 1. Never commit .env files to version control
# 2. Use strong, unique API keys
# 3. Adjust retrieval parameters based on your needs
# 4. Enable debug mode only during development
# 5. For production, use environment variables instead of .env files