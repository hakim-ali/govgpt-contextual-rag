{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Composite Model Evaluation\n",
    "\n",
    "This notebook dynamically consolidates all model evaluation results from the `/evals` folder into a single Excel file with Winner and Runner-up columns.\n",
    "\n",
    "**Features:**\n",
    "- Uses `sample_questions.xlsx` as the master source for Query and GroundTruth columns\n",
    "- Maintains original query order from `sample_questions.xlsx` (133 queries)\n",
    "- Automatically discovers all `*_eval.xlsx` files in `/evals` folder\n",
    "- Dynamically detects `*_Response` and `*_scores` columns\n",
    "- Maps evaluation data by 'Query' column for proper alignment\n",
    "- Determines Winner and Runner-up using winner.py logic\n",
    "- Includes all queries from master file, even if missing from evaluation files\n",
    "- Exports consolidated results to Excel in original query order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import ast\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import winner logic\n",
    "import winner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discover_eval_files(evals_dir: str = \"evals\") -> List[str]:\n",
    "    \"\"\"\n",
    "    Dynamically discover all evaluation Excel files in the evals directory.\n",
    "    \n",
    "    Args:\n",
    "        evals_dir: Directory containing evaluation files\n",
    "        \n",
    "    Returns:\n",
    "        List of file paths matching *_eval.xlsx pattern\n",
    "    \"\"\"\n",
    "    pattern = os.path.join(evals_dir, \"*_eval.xlsx\")\n",
    "    files = glob.glob(pattern)\n",
    "    \n",
    "    print(f\"üìÅ Found {len(files)} evaluation files:\")\n",
    "    for file in files:\n",
    "        print(f\"  - {os.path.basename(file)}\")\n",
    "    \n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_model_name_from_filename(filepath: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract model identifier from filename.\n",
    "    \n",
    "    Args:\n",
    "        filepath: Path to evaluation file\n",
    "        \n",
    "    Returns:\n",
    "        Model identifier (e.g., 'cohere_embed_v_4' from 'cohere_embed_v_4_eval.xlsx')\n",
    "    \"\"\"\n",
    "    basename = os.path.basename(filepath)\n",
    "    # Remove _eval.xlsx suffix\n",
    "    model_name = basename.replace(\"_eval.xlsx\", \"\")\n",
    "    return model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_model_columns(df: pd.DataFrame) -> Tuple[List[str], List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Dynamically detect model response and scores columns.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to analyze\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (response_columns, scores_columns)\n",
    "    \"\"\"\n",
    "    response_columns = [col for col in df.columns if col.endswith('_Response')]\n",
    "    scores_columns = [col for col in df.columns if col.endswith('_scores')]\n",
    "    context_columns = [col for col in df.columns if col.endswith('_Context')]\n",
    "    \n",
    "    return response_columns, scores_columns, context_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_model_names_from_columns(columns: List[str], suffix: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extract model names from column names by removing suffix.\n",
    "    \n",
    "    Args:\n",
    "        columns: List of column names\n",
    "        suffix: Suffix to remove (e.g., '_Response', '_scores')\n",
    "        \n",
    "    Returns:\n",
    "        List of model names\n",
    "    \"\"\"\n",
    "    return [col.replace(suffix, '') for col in columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide_winner_and_runner_up(model_scores: Dict[str, Dict[str, float]]) -> Tuple[str, str, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Modified winner logic to return separate Winner and Runner-up.\n",
    "    \n",
    "    Args:\n",
    "        model_scores: Dictionary of model scores\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (winner, runner_up, aggregate_scores)\n",
    "    \"\"\"\n",
    "    if len(model_scores) == 0:\n",
    "        return \"No_Data\", \"No_Data\", {}\n",
    "    \n",
    "    if len(model_scores) == 1:\n",
    "        single_winner = list(model_scores.keys())[0]\n",
    "        return single_winner, \"No_Runner_Up\", {}\n",
    "    \n",
    "    try:\n",
    "        # Use existing winner logic to get aggregate scores\n",
    "        _, aggregate_scores = winner.decide_winner(model_scores)\n",
    "        \n",
    "        # Sort by aggregate score to get winner and runner-up\n",
    "        sorted_models = sorted(aggregate_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        top_winner = sorted_models[0][0]\n",
    "        runner_up = sorted_models[1][0] if len(sorted_models) > 1 else \"No_Runner_Up\"\n",
    "\n",
    "        # Sort aggregate_scores dictionary in descending order by score\n",
    "        aggregate_scores = dict(sorted(aggregate_scores.items(), key=lambda x: x[1], reverse=True))\n",
    "        aggregate_scores = {k: round(v,2) for k, v in aggregate_scores.items()}\n",
    "\n",
    "        return top_winner, runner_up, aggregate_scores\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in winner selection: {e}\")\n",
    "        return \"Error\", \"Error\", {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_scores_string(scores_str: str) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Parse scores string to dictionary.\n",
    "    \n",
    "    Args:\n",
    "        scores_str: String representation of scores dictionary\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of scores\n",
    "    \"\"\"\n",
    "    if pd.isna(scores_str) or not scores_str.strip():\n",
    "        return {}\n",
    "    \n",
    "    try:\n",
    "        # If it's already a dictionary, return as is\n",
    "        if isinstance(scores_str, dict):\n",
    "            return scores_str\n",
    "        \n",
    "        # Parse string representation\n",
    "        if isinstance(scores_str, str):\n",
    "            scores_str = scores_str.strip()\n",
    "            if scores_str.startswith('{') and scores_str.endswith('}'):\n",
    "                return ast.literal_eval(scores_str)\n",
    "        \n",
    "        return {}\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error parsing scores: {e}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_eval_file(filepath: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load and process a single evaluation file.\n",
    "    \n",
    "    Args:\n",
    "        filepath: Path to evaluation Excel file\n",
    "        \n",
    "    Returns:\n",
    "        Processed DataFrame\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_excel(filepath)\n",
    "        print(f\"üìä Loaded {len(df)} rows from {os.path.basename(filepath)}\")\n",
    "        \n",
    "        # Detect model columns\n",
    "        response_cols, scores_cols, context_cols = detect_model_columns(df)\n",
    "        print(f\"  Response columns: {response_cols}\")\n",
    "        print(f\"  Scores columns: {scores_cols}\")\n",
    "        print(f\"  Context columns: {context_cols}\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading {filepath}: {e}\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consolidate_evaluations(evals_dir: str = \"evals\", questions_file: str = \"sample_questions.xlsx\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Main function to consolidate all evaluation files using sample_questions.xlsx as master.\n",
    "    \n",
    "    Args:\n",
    "        evals_dir: Directory containing evaluation files\n",
    "        questions_file: Path to master questions file with Query and Ground_Truth columns\n",
    "        \n",
    "    Returns:\n",
    "        Consolidated DataFrame with queries in original order\n",
    "    \"\"\"\n",
    "    # Load master questions file to get queries and ground truth in original order\n",
    "    try:\n",
    "        master_df = pd.read_excel(questions_file)\n",
    "        print(f\"üìã Loaded {len(master_df)} queries from master file: {questions_file}\")\n",
    "        print(f\"Columns: {list(master_df.columns)}\")\n",
    "        \n",
    "        if not any(word in col.lower() for col in master_df.columns for word in ['query','queries','question','questions']):\n",
    "            print(\"‚ùå 'Query' column not found in master file\")\n",
    "            return pd.DataFrame()\n",
    "            \n",
    "        # Get queries in original order\n",
    "        master_queries = master_df['Query'].dropna().tolist()\n",
    "        ground_truths = master_df.get('Ground_Truth', pd.Series(['']*len(master_queries))).fillna('').tolist()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading master questions file: {e}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Discover evaluation files\n",
    "    eval_files = discover_eval_files(evals_dir)\n",
    "    \n",
    "    if not eval_files:\n",
    "        print(\"‚ùå No evaluation files found\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Load all eval files\n",
    "    all_dfs = []\n",
    "    for filepath in eval_files:\n",
    "        df = load_and_process_eval_file(filepath)\n",
    "        if not df.empty:\n",
    "            # Add source file info\n",
    "            df['_source_file'] = os.path.basename(filepath)\n",
    "            all_dfs.append(df)\n",
    "    \n",
    "    if not all_dfs:\n",
    "        print(\"‚ùå No valid evaluation files loaded\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    print(f\"\\nüîç Processing {len(master_queries)} queries in original order from master file\")\n",
    "    \n",
    "    # Consolidate data by query in original order\n",
    "    consolidated_results = []\n",
    "    \n",
    "    for i, (query, ground_truth) in enumerate(zip(master_queries, ground_truths)):\n",
    "        if pd.isna(query) or not str(query).strip():\n",
    "            continue\n",
    "            \n",
    "        query_str = str(query).strip()\n",
    "        \n",
    "        # Initialize result row with master data\n",
    "        result_row = {\n",
    "            'Query': query_str,\n",
    "            'GroundTruth': str(ground_truth) if pd.notna(ground_truth) else '',\n",
    "            'RAG': 'Contextual RAG',\n",
    "            'Context': ''\n",
    "        }\n",
    "        \n",
    "        # Collect model data for this query from eval files\n",
    "        model_scores = {}\n",
    "        found_in_evals = False\n",
    "        \n",
    "        for df in all_dfs:\n",
    "            # Find matching query in this dataframe\n",
    "            matching_rows = df[df['Query'] == query_str]\n",
    "            \n",
    "            if matching_rows.empty:\n",
    "                continue\n",
    "                \n",
    "            found_in_evals = True\n",
    "            row = matching_rows.iloc[0]\n",
    "            \n",
    "            # Update context (use first non-empty value from eval files)\n",
    "            if not result_row['Context'] and 'Context' in row and pd.notna(row['Context']):\n",
    "                result_row['Context'] = row['Context']\n",
    "            \n",
    "            # Extract model responses and scores\n",
    "            response_cols, scores_cols, context_cols = detect_model_columns(df)\n",
    "            \n",
    "            for response_col in response_cols:\n",
    "                if response_col in row and pd.notna(row[response_col]):\n",
    "                    result_row[response_col] = row[response_col]\n",
    "\n",
    "            for context_col in context_cols:\n",
    "                if context_col in row and pd.notna(row[context_col]):\n",
    "                    result_row[context_col] = row[context_col]\n",
    "            \n",
    "            for scores_col in scores_cols:\n",
    "                if scores_col in row and pd.notna(row[scores_col]):\n",
    "                    result_row[scores_col] = row[scores_col]\n",
    "                    \n",
    "                    # Parse scores for winner calculation\n",
    "                    model_name = scores_col.replace('_scores', '')\n",
    "                    parsed_scores = parse_scores_string(row[scores_col])\n",
    "                    \n",
    "                    if parsed_scores:\n",
    "                        model_scores[model_name] = parsed_scores\n",
    "        \n",
    "        # Determine winner and runner-up (only if we have evaluation data)\n",
    "        if model_scores:\n",
    "            winner, runner_up, aggregate_scores = decide_winner_and_runner_up(model_scores)\n",
    "            result_row['Winner'] = winner\n",
    "            result_row['Runner_Up'] = runner_up\n",
    "            result_row['Scores'] = aggregate_scores\n",
    "        else:\n",
    "            # Query exists in master but not in evaluation files\n",
    "            result_row['Winner'] = 'No_Evaluation_Data'\n",
    "            result_row['Runner_Up'] = 'No_Evaluation_Data'\n",
    "            result_row['Scores'] = {}\n",
    "        \n",
    "        consolidated_results.append(result_row)\n",
    "        \n",
    "        # Progress indicator for every 20 queries\n",
    "        if (i + 1) % 20 == 0:\n",
    "            print(f\"  Processed {i + 1}/{len(master_queries)} queries...\")\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    consolidated_df = pd.DataFrame(consolidated_results)\n",
    "    \n",
    "    # Count queries with evaluation data\n",
    "    evaluated_count = sum(1 for row in consolidated_results if row['Winner'] != 'No_Evaluation_Data')\n",
    "    \n",
    "    print(f\"\\n‚úÖ Consolidated {len(consolidated_df)} queries in original order\")\n",
    "    print(f\"üìä {evaluated_count} queries have evaluation data, {len(consolidated_df) - evaluated_count} missing from eval files\")\n",
    "    \n",
    "    return consolidated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_consolidated_results(df: pd.DataFrame, output_file: str = \"composite_model_evaluation_results.xlsx\"):\n",
    "    \"\"\"\n",
    "    Save consolidated results to Excel file.\n",
    "    \n",
    "    Args:\n",
    "        df: Consolidated DataFrame\n",
    "        output_file: Output file path\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        print(\"‚ùå No data to save\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        df.to_excel(output_file, index=False)\n",
    "        print(f\"üíæ Consolidated results saved to: {output_file}\")\n",
    "        print(f\"üìä Total rows: {len(df)}\")\n",
    "        \n",
    "        # Show column summary\n",
    "        print(f\"üìã Columns: {len(df.columns)}\")\n",
    "        print(f\"  Core columns: Query, GroundTruth, RAG, Context, Winner, Runner_Up, Scores\")\n",
    "        \n",
    "        # Show dynamic columns\n",
    "        response_cols = [col for col in df.columns if col.endswith('_Response')]\n",
    "        scores_cols = [col for col in df.columns if col.endswith('_scores')]\n",
    "        context_cols = [col for col in df.columns if col.endswith('_Context')]\n",
    "        \n",
    "        print(f\"  Response columns: {response_cols}\")\n",
    "        print(f\"  Scores columns: {scores_cols}\")\n",
    "        print(f\"  Scores columns: {context_cols}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error saving results: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_winner_summary(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Display winner and runner-up summary statistics.\n",
    "    \n",
    "    Args:\n",
    "        df: Consolidated DataFrame\n",
    "    \"\"\"\n",
    "    if df.empty or 'Winner' not in df.columns:\n",
    "        print(\"‚ùå No winner data to summarize\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\nüèÜ Winner Distribution:\")\n",
    "    winner_counts = df['Winner'].value_counts()\n",
    "    for model, count in winner_counts.items():\n",
    "        print(f\"  {model}: {count} wins\")\n",
    "    \n",
    "    if 'Runner_Up' in df.columns:\n",
    "        print(\"\\nü•à Runner-Up Distribution:\")\n",
    "        runner_up_counts = df['Runner_Up'].value_counts()\n",
    "        for model, count in runner_up_counts.items():\n",
    "            print(f\"  {model}: {count} runner-ups\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute Consolidation\n",
    "\n",
    "Run the consolidation process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting composite model evaluation consolidation...\n",
      "üìã Loaded 133 queries from master file: sample_questions.xlsx\n",
      "Columns: ['Query', 'Ground_Truth']\n",
      "üìÅ Found 3 evaluation files:\n",
      "  - gpt41_openai_embed_eval.xlsx\n",
      "  - deepseek_openai_embed_eval.xlsx\n",
      "  - cohere_embed_v_4_eval.xlsx\n",
      "üìä Loaded 133 rows from gpt41_openai_embed_eval.xlsx\n",
      "  Response columns: ['gpt-4.1_Response']\n",
      "  Scores columns: ['gpt-4.1_scores']\n",
      "  Context columns: ['gpt-4.1_Context']\n",
      "üìä Loaded 133 rows from deepseek_openai_embed_eval.xlsx\n",
      "  Response columns: ['deepseek_Response']\n",
      "  Scores columns: ['deepseek_scores']\n",
      "  Context columns: ['deepseek_Context']\n",
      "üìä Loaded 133 rows from cohere_embed_v_4_eval.xlsx\n",
      "  Response columns: ['cohere_Response']\n",
      "  Scores columns: ['cohere_scores']\n",
      "  Context columns: ['cohere_Context']\n",
      "\n",
      "üîç Processing 133 queries in original order from master file\n",
      "  Processed 20/133 queries...\n",
      "  Processed 40/133 queries...\n",
      "  Processed 60/133 queries...\n",
      "  Processed 80/133 queries...\n",
      "  Processed 100/133 queries...\n",
      "  Processed 120/133 queries...\n",
      "\n",
      "‚úÖ Consolidated 133 queries in original order\n",
      "üìä 133 queries have evaluation data, 0 missing from eval files\n",
      "‚ùå Error saving results: [Errno 13] Permission denied: 'composite_model_evaluation_results.xlsx'\n",
      "\n",
      "üèÜ Winner Distribution:\n",
      "  gpt-4.1: 96 wins\n",
      "  deepseek: 20 wins\n",
      "  cohere: 17 wins\n",
      "\n",
      "ü•à Runner-Up Distribution:\n",
      "  deepseek: 77 runner-ups\n",
      "  cohere: 32 runner-ups\n",
      "  gpt-4.1: 20 runner-ups\n",
      "  No_Runner_Up: 4 runner-ups\n",
      "\n",
      "üìã Sample Results:\n",
      "                                               Query    Winner Runner_Up\n",
      "0          What are the exemptions from this policy?   gpt-4.1  deepseek\n",
      "1  What are the main components of the Procuremen...   gpt-4.1  deepseek\n",
      "2  What are the criteria of common and entity-spe...    cohere  deepseek\n",
      "3  Can you give me the details of the UNSPSC cate...   gpt-4.1    cohere\n",
      "4  What are the types of procurement capabilities...   gpt-4.1    cohere\n",
      "5               What does it mean by baseline spend?   gpt-4.1    cohere\n",
      "6  How many types are under the Procurement Benef...   gpt-4.1  deepseek\n",
      "7  What are the calculation methodologies I can u...    cohere   gpt-4.1\n",
      "8  How many types of cost-saving scenarios are th...  deepseek    cohere\n",
      "9  Can you compare the annual Revenue in AED for ...   gpt-4.1  deepseek\n"
     ]
    }
   ],
   "source": [
    "# Run the consolidation\n",
    "print(\"üöÄ Starting composite model evaluation consolidation...\")\n",
    "consolidated_df = consolidate_evaluations()\n",
    "\n",
    "# Save results\n",
    "if not consolidated_df.empty:\n",
    "    save_consolidated_results(consolidated_df)\n",
    "    \n",
    "    # Show summary\n",
    "    show_winner_summary(consolidated_df)\n",
    "    \n",
    "    # Display sample results\n",
    "    print(\"\\nüìã Sample Results:\")\n",
    "    display_cols = ['Query', 'Winner', 'Runner_Up']\n",
    "    available_cols = [col for col in display_cols if col in consolidated_df.columns]\n",
    "    print(consolidated_df[available_cols].head(10))\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No consolidated results generated\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
